so welcome back for the third lecture of this week ah in the last lecture we had discussed top down and bottom up parsing approach and then finally we came up with the dynamic programming approach of ah using c k y algorithm and we converted a grammar to chomsky normal form now how once my grammar is converted to chomsky normal form how do i c k y algorithm work so this is the idea so i will take a sentence i have n words in my sentence so you will think about n plus one lines that are separating them is starting from zero to one ok and so once you have done that any x i j will denote words between line i ( and j and so ( you you will build up a table such that any x i j will contain all the possible non terminal that can derive words between the lines i ( and j ok and you do that bottom up ( so so what i am saying you have ( word one ( word two up to word n in your input stream so you assume some lines like zero ( one two ( n minus one n ok so for example x zero two is words ( w one w two and ( so on so now what ( you will ( you do you build up a ( table ok and table will be some sort of triangular table so this can be zero one two so this element will denote x zero one what are all the non terminals that derive this word between zero and one similarly here you will write x one two and so on suppose they want a three words so this is x two three ok so we will write down all the non terminals that derive ( this you first fill this then you will go next step ( x zero two x one three ok and once ( you fill this you will use x zero three the final one ok now what is the where are we using the fact that ah this grammar is in c n f chomsky normal form i will make sure fact that at any point this can come from only two non terminals or a single terminal so at this point when i am seeing at ( a as a individual word so zero one will always be a individual word similarly for one and two so this will come by ( a rule capital ( a goes to small ( a a rule of this kind ok so i will find out all the non terminals that derives this terminal thats hoW will fill the diagonal ( elements the first diagonal elements next time this cannot come from a single terminal because they are two words ok so this has to come from two non terminals so i will find out if there is a rule that gives me these two non terminals and thats where i am using chomsky normal form ok so finally when ( i am here i will see if ( the sentence as generate the whole sentence so i am catching all the possible intermediate steps here so that ( i denoted i have to ah do certain in ( again and again ok so this is the simple home exercise so in the { in the } ) ( the in the last lecture we had given you a grammar in chomsky normal form now take ( the sentence book the flight through houston and use the c k y algorithm to find a parse tree for that ok ah so but what i will { do i will } ) ( do i will do an example in todays class also so that it becomes become ( more much more comfortable with using c k y algorithm so lets take this example a pilot likes flying planes and the grammar is given to you ok so let do that in the way i had explained so zero one two three four ( five there are five words ok and this is the line ok and now you should understand how do we denote these elements so this is x zero one this is denote only the first word this is ( x one two { x two } ) two x { two three x } ) two three x { three four x } ) three ( four x four five this will be x ( zero two words between ( zero to two x zero { three x zero } ) three x zero { four x zero } ) four ( x zero five x one { three x one } ) three x one { four x one } ) four ( x one five x two { four x two } ) four ( x two ( five and x three five noW have to fill here what are different non terminals that filled each of the individual elements x zero one what is x zero one my word is ( a so a pilot likes flying planes so is there a non terminal that derives the word a and if you see the grammar dt derives a ok so you can fill in ( dt here dt derives a pilot { n n } derives pilot n derives pilot likes v b z derives likes flying so you see there are two non terminals vbg and ( j j they derive this and planes ( n n s ok so filling the diagonal element is very easy the first diagonal elements then you go the next step x zero ( two now x zero two can come from ( x zero ( one and x one two as a break up of these two points so is there an non terminal where or is there a rule in my grammar when the right hand side i have dt followed by ( n n and if you see your grammar yes n p gives me dt followed by ( n n so i can fill in n p here x one three comes from ( x one ( two and ( x two ( three yes x one three pilot likes so it comes from pilot and likes so any non terminal that gives me ( n n followed by v b z and if you see there is no non terminal so i will fill in empty here similarly and if v b z followed by vbg no anything from v b z followed by ( j j x three five v b g followed by ( n n s yes v p and ( j j followed by n ( n s n p ok so there are two possibilities so fine ( this roWs ( done this diagonal is done now we go to next step now how do i derive x zero three a pilot likes now thats why i am using the chomsky normal form i cannot derive it at the sequence of three non terminals because each individual non terminal can give me at most { not at most } ) not at most exactly two non terminals ok so what are the two places from which it can come so one possibilities i can break x ( zero three as ( x zero ( one and x ( one three one word into two words or ( x zero two { x two } ) ( two x ( two three there are two possibilities ok so i have to check individually each of these possibilities ( x zero one { x one } ) one x { one three x } ) ( one three x zero one is dt followed by null so this is already gone ok this is no non terminal x zero two is n p followed by x two three is v b z so n p followed by v b z is there a non terminal when the right hand side i have np followed by v b z ok and if you see your grammar there ( is nothing so this is also null so here it ( is null there is no non terminal let me derive a pilot ( likes ok now pilot likes flying one to four so again one to four will be one ( two two four or one ( three three four so one { two two } four one ( two four one ( two is n { n } two ) ( n n two four is null so this part is null one { three three } four one ( three four one three is ( null so this becomes null also ( two five will be two ( three three five and two ( four four five so two three here is v b z and three five ( is v p is there something that ( v b z and v p { yes v p } ) yes ( v p this means ( v b z and v p two { four four } five two ( four five two four is null ok so this is my v p similarly now you will go to x zero four now what are difference which you can break x zero four { now zero four } ) now zero four again you to break it in a sequence of two non terminals so it can be zero ( one followed by one four or zero ( two followed by two four or zero ( three followed by ( three four there are three ways so let us see ( one by ( one zero ( one is d t one four is null so this part ( is gone zero ( two is empty and two four ( is null this part is also gone zero three is null already so this is null ok one five { one five } ) five ( one five can be one { two two } five one two five one three { three } five ) ( three three five one four { four } five ) ( four four five one ( two and two five yes ( n n followed by a v p so is there something in my grammar ( n n followed by v p no so this is gone similarly one ( three followed by ( three five one three is null ok and one three ( is null so this is gone one ( four followed by ( four five one four ( is null so this is also gone so this is also null now the only thing remain is x zero five now how can i fill x zero five what are different ways so let me write down here ( zero five can be zero ( one followed by one five zero ( two followed by two five zero ( three followed by three five zero ( four followed by four five ok zero ( one followed by ( one five zero one ( is there one five ( is null this is gone zero ( two followed by ( two five zero ( two is there and two five ( is also there this is n ( p followed by v p and this is a sentence in my grammar so s gives me n ( p v p so this is one possibility already so that means this sentence is grammatical at least there is one s that derives this but are there any further ah s so for that we have to look at other possibilities zero ( three three five ok this is null and zero ( four four five is null ok ( so fine so if you see there i think we made one mistake here there should have been another v p in this case ok so there is v p { one v p } ) one v p two and then there will be s n ( p v ( p one and n ( p v p two so yeah i will search that look back into this ah calculation we did and see ( where where we made a mistake ok but everything else is the same that we did here so there are two different s in which this ah sentence can be parse two different ways so now once you know this i will say that use the previous ah example so that is book the flight through houston from the other grammar and try to get its parse tree so noWn this example what we saw there are two possibilities so can you think of the possiblit [ ies ] why there are two possibilities in this case a pilot likes flying planes ok so whether he likes to fly the plane or whether he likes to see flying planes something like that so there are interpretation thats why there are two different ( parse parse edge of this sentence so each individual parse with we will denote one particular interpretation now ( by using ( this context by this c k y algorithm we denote all the possible parse trees using my grammar but i have no way of saying which parse is more probable than the other parse i cannot assign some probabilities to them ok so something if i have so this is ( the sentence ( the man saw ( the dog with the telescope it has two different interpretation in terms of two different parse edges whatever i have covered till noWt cannot tell me which parse is more probable than other so now we would like to have a way in which we can assign them the probabilities that this parse edge is more probable than the other and for that what we use is called probabilistic context free grammars so this is simple extension of context free grammar where in addition to whatever we have seen in context free grammar each rule is also signed some probability ok so as you see in the formulation it is t n s and r they are exactly same as what we had seen in the context free grammar plus there is something called p so i am assigning a probability distribution over ( the rules and only the constraint here is that from a given non terminal ( the lateral side the probability generating anything should add up to one so if there are five possibilities if i add all ( the five possibilities ( the rule for all the five possibilities they should add up to one ok so this is the constraint so probability in ( the rule gives { the gives } ) gives the { gives the } probability ) gives the probability of each rule p r so the constraint is for all x in non terminals probability of x to gamma for all the possible gammas should add up to one ok so let me give you an example ( so so this is one ( simple cnf sorry simple p c f g in chomsky normal form ah ( so so what do you see here from s there is only one rule s goes to n ( p v p ok so because there is only one rule it has a possibility of one ( from from v p there are two rules ( v p can give v followed by n ( p or v p followed by pp ( so so the constraint is that for the possibilities of these two rules should add up to one so that ( is what is happening here the first rule has a possibility of point seven second rule has a possibility of point three so there is two add up to one now pp gives me ( p n p only one rule with pp on left hand side so this is the possibility of one p gives me only with again possibility one v gives me saw this is the possibility one now all these rules in the right hand sides are starting from the n p { n p } ) ( p n p giving to me n p pp all the words let us call as like astronomers ears and ( so on so all these should add up to one and you can see that this actually happen ( point four plus ( point one ( point five ( point six eight ( point seven two point nine and one so all these possibilities are adding up to one so this ( is the constraint that is being followed ( the rules have the same format as in context free grammar but you shall have a possibilities now how does ( it help it helps in that i can now assign possibilities to each individual parse tree so suppose this is one parse tree ( astronomers for the sentence astronomers saw starts with ears how do i find the probability of ( this parse tree this is the parse so i know s gives me n ( p and v p yes this is the first rule that i am applying now as my p c f g the probability of this rule is one s giving n ( p v p is one so i have this one here n p giving ( astronomers deriving astronomers is probability point one v ( p deriving v and p is probability point seven v giving saWs one n p { giving n p } ) giving n p vp is point four n p giving stars is point one eight and so on these are the rule probability ( as per ( my grammar as my p c f g so what i will { do i will } ) do i will just multiply all these probabilities one times { point one times } ) ( point one times point { seven times point } ) seven times point { one times point } ) one times point { four times point } ) four times point one ( eight times point one eight this is my probabilities of this parse trees and if i get a second parse tree where instead of ( v p giving me ( v and n p v { p giving v } ) ( p giving v p followed by pp i can again compute ( its probability by multiplying its corresponding rule probabilities and i can find ( the probabilities of both the parse trees individually ok so now how do i use that to compute ( the probability of the tree that is simply ( the product of ( the probabilities of all the rules that i used to generate this ( and probability of assignments and the probability sentences is nothing but find out all ( the parse trees and ( the probabilities of the individual parse trees and just sum them up ok so probability of this sentence is ( the sum of the { probabilities of the } ) probabilities of the trees that have this as their yield that is another way of saying that those parse trees are used to generate this particular string so in the first case i had this sentence and they were to parse trees i can compute the probabilities of individual ( one so p ( t one and p t two i can find out and then to find ( the probability of the whole sentence i just add up these two probabilities p ( t one plus p t two and that gives me the probability of this whole sequence similarly if i have another sentence like book the dinner flight and ( as as per a different grammar i can compute the parse ( trees the two parse trees compute ( the probabilities for the individual one ok so parse tree one here book the dinner flight is one point six two into ten to the power minus six and book the dinner flight will have a probability of ( two point two eight into ten to the power minus seven so one thing i can immediately see is that the first one parse is more likely interpretation than the second one lets look at some ( of the features of probabilistic context free grammars so why we started with this formulation so we said that using the context free grammar given a sentence we can find out all the possible parse trees but you are not able ( to assign any probabilities to that so by using p c f g if for a given string the number of parse trees are increasing i can also assign the probabilities for each individual parse tree so that gives me some plausibility which parse tree is more probable than ( the other one for the given string so this is important but at the same time we should understand that although by using p c f g i can compute what is ( the probability of the sentence by taking all ( the parse trees taking the individual probabilities and adding them up ( this is this just not giving a very good ah plausibility of the sentence ok this is only looking at the structural factors not some lexical co occurrence so in general to find ( the probability of the sentence you would prefer to use language model than a p c f ( g p c f g is good only for find the probability of a ( parse tree which parse is more probable than another one yes if the ( it it helps ( in some cases like in real text you might find some grammatical mistakes so p c f g ( will allow that but will give you V low probability so in one case you can also probably find out which sentence has some grammatical mistake if the p c f g is giving it low probability and yeah this is something that i said earlier so in practice this is not good for modeling ( the probability of the sentence ok language model is much better than p c f g so why is that the case so a simple example is if we have the same sentence i have two different trees one is smaller than ( the other the smaller tree will always have a higher probability than the larger one because all the probabilities are less than one and for a larger tree you are multiplying the probabilities ok so yeah the take away here is that you would use p c f g to find out ( what what are all the probabilities ( for different parse trees for a sentence ok and try to choose the best one so now once we have this formulation of p c f g there are some interesting questions that we would like to explore using that so suppose i am given a sentence w one m ok i am given a grammar g and there are various parse trees t is one of those so what is the most likely parse for this sentence given the grammar so which parse is tree t gives me maximum probabilities argmax of probability given ( the sentence and the grammar ok then what is the probability of the ( sentence probability of the sentence given by grammar and then finally how do we learn the rule probabilities of my grammar g which these are the three interesting questions that we would like to answer in the next lectures so for example how do i find the most likely parse of a { of a } ) ( a of a sentence so one simple solution is find out all the possible trees and take ( the one with the highest probability but is there any efficient method for doing that what is the probability of a sentence again i can find out probabilities of the individual parse trees add them up this gives me ( the probability of the sentence but can i do at by extortion numerating all that and adding it or is this some other method and finally there is some interesting question that how do i learn ( the rule probabilities in ( the grammar g and the answer to this will be similar to what we saWn the case of ah head and markov models so there { are so there } ) ( are so there are they will be again two ways of running the parameter one will be when the ah i am given the corpus and some labeled parse trees i can use them to find ( the probabilities under the scenario where i am given ( the corpus but not the parse tree i am only given the { grammar grammar } in the grammar in ( the sense cfg not the rule probabilities then how do i learn the parameters so this will be again V interesting topic and there you can use some ideas where that i had shown earlier for the ah algorithm so in the next lecture we will start trying to answer some of these questions thank you 