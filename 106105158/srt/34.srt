1
00:00:18,529 --> 00:00:23,730
so hello everyone welcome to the third lecture
of this week so we are talking about distribution

2
00:00:23,730 --> 00:00:29,669
semantics and we had already discussed what
is a generic framework of distribution semantics

3
00:00:29,669 --> 00:00:35,070
how do you build models by taking different
sorts of targets and context and how do you

4
00:00:35,070 --> 00:00:39,600
fill up all the antigen of matrix now in this
lecture we'll talk about some of the some

5
00:00:39,600 --> 00:00:45,650
applications for distribution semantic models
also how do i compare similarity across two

6
00:00:45,650 --> 00:00:52,289
words and then we'll move towards some a structured
model of distribution semantics

7
00:00:52,289 --> 00:00:58,640
so starting with some application so so let's
talk about this problem of term mismatch that

8
00:00:58,640 --> 00:01:05,430
we see in the case of information retrieval
so what is the problem so so information retrieval

9
00:01:05,430 --> 00:01:11,390
the user is giving a query that is in terms
of certain words that he feels are describing

10
00:01:11,390 --> 00:01:18,220
his information in it and so he is trying
to ma so the system is trying to match this

11
00:01:18,220 --> 00:01:23,560
this set of keywords to all the documents
that are there in the depositary and by by

12
00:01:23,560 --> 00:01:28,189
doing this matching they are trying to find
out what are the potential document that can

13
00:01:28,189 --> 00:01:33,439
be returned to the user
now what is one problem here so it might happen

14
00:01:33,439 --> 00:01:38,000
that the particular concept that the user
has in mind the user expressing by using certain

15
00:01:38,000 --> 00:01:44,710
words but the documents has have the same
concept using a very very different word so

16
00:01:44,710 --> 00:01:49,700
so they are the the words are similar on the
semantic label but on the surface label they

17
00:01:49,700 --> 00:01:54,649
are two different keywords or two different
words so this is called a term mismatch problem

18
00:01:54,649 --> 00:01:59,719
and one of the key issues in information retrieval
is how do we solve this term mismatch problem

19
00:01:59,719 --> 00:02:05,240
how do i find out that the user typed this
particular term but he might also be interested

20
00:02:05,240 --> 00:02:10,890
in the other term that is semantically similar
but on the surface form it is different

21
00:02:10,890 --> 00:02:18,140
so let's see an example so suppose ah there
is a user query insurance insurance cover

22
00:02:18,140 --> 00:02:23,600
which pays for long term care and it might
happen that the rol relevant document that

23
00:02:23,600 --> 00:02:29,090
contains the answer to the user query may
have words like medicare premiums insurers

24
00:02:29,090 --> 00:02:32,790
etcetera that are not directly provided in
the user query

25
00:02:32,790 --> 00:02:37,440
now one question is how do i find out that
these documents that contain some similar

26
00:02:37,440 --> 00:02:45,680
words are also relevant to this user query
so for this task one can use issue semantic

27
00:02:45,680 --> 00:02:50,970
models for query expansion so query expansion
is one of the techniques that is used for

28
00:02:50,970 --> 00:02:56,720
this term mismatch problem so idea is that
i the user has given a query with certain

29
00:02:56,720 --> 00:03:02,159
words can i try to find out some other words
that are semantically similar to the user

30
00:03:02,159 --> 00:03:07,430
query words if i find out some words then
i append those words to the user query and

31
00:03:07,430 --> 00:03:13,080
this is called the expended query query expansion
process and now i give this expended query

32
00:03:13,080 --> 00:03:20,099
to to my search engine and then retrieve the
documents so now what we will see in in brief

33
00:03:20,099 --> 00:03:25,989
can we use our distribution semantic models
for this problem of query expansion so so

34
00:03:25,989 --> 00:03:31,223
what is the idea so once user gives a query
we reformulate it using the terms is a relevant

35
00:03:31,223 --> 00:03:36,840
or that are related to the terms that are
already there in the ah in the query find

36
00:03:36,840 --> 00:03:43,209
out some related terms and try to use that
to improve the retrieval performance so so

37
00:03:43,209 --> 00:03:48,650
how do we find out the related terms so firstly
i have the query terms that the user is giving

38
00:03:48,650 --> 00:03:54,129
find out what are the distributional vectors
and take a function combination of all the

39
00:03:54,129 --> 00:03:57,549
distributional vectors and obtained the expanded
query

40
00:03:57,549 --> 00:04:02,750
so suppose that the query has three terms
find out what are the related terms to all

41
00:04:02,750 --> 00:04:08,650
this three terms in the distributional sense
and make a linear combination of all such

42
00:04:08,650 --> 00:04:16,190
ah possibilities and give the expanded query
to the user so how will it look like so suppose

43
00:04:16,190 --> 00:04:22,340
the user gives a query like catastrophic health
insurance so what you are doing so in your

44
00:04:22,340 --> 00:04:28,130
distribution vector suppose you are finding
out what are the terms that are related to

45
00:04:28,130 --> 00:04:32,229
each of the three terms catastrophic health
and insurance

46
00:04:32,229 --> 00:04:44,060
ok so what will happen so user has given these
three terms 

47
00:04:44,060 --> 00:04:49,520
and suppose you are using words at con as
context so the different words were there

48
00:04:49,520 --> 00:04:56,460
w i w n and for each word so you are computing
what is the co occurrence so it can be say

49
00:04:56,460 --> 00:05:02,020
p m i value what is the p m i between catastrophic
and w one so like that you are computing for

50
00:05:02,020 --> 00:05:08,919
all the words so now what you might do you
might say ok what are the words that are associate

51
00:05:08,919 --> 00:05:14,889
with all the three words so that are having
high p m i values so one way is you just add

52
00:05:14,889 --> 00:05:25,330
all the p m i values p m i q w i for all words
q in my query

53
00:05:25,330 --> 00:05:31,750
ok so for each word w why i i complete a score
like that what is the p m i of that query

54
00:05:31,750 --> 00:05:36,389
one of the query word with this word w i and
add your all the query words and then i can

55
00:05:36,389 --> 00:05:42,069
sort it and maybe i can normalize it with
respect to the highest term so what will happen

56
00:05:42,069 --> 00:05:49,930
at the end of that if i can i can sort these
course and find out what are the words w i

57
00:05:49,930 --> 00:05:57,500
w i prime etcetera that are very much related
to the query terms and all these we have already

58
00:05:57,500 --> 00:06:02,850
covered how do a compute this course once
we this course i can compute this symbol formula

59
00:06:02,850 --> 00:06:06,610
and find out what are the terms in a decreasing
order

60
00:06:06,610 --> 00:06:11,390
so this one part way but you can have any
other sort of functional combination so this

61
00:06:11,390 --> 00:06:17,110
is a simple addition but you can have multiplication
and other sort of function combinations also

62
00:06:17,110 --> 00:06:22,940
so suppose we we do that so what happens to
the actual query so this is my ac actual query

63
00:06:22,940 --> 00:06:27,390
catastrophic health insurance and if i try
to use this technique to find out some other

64
00:06:27,390 --> 00:06:33,819
related terms i get terms like this so i have
terms like surtax h c f a medicare and h m

65
00:06:33,819 --> 00:06:42,690
o s medicaid h m o beneficiaries premiums
and so on now so these terms are taken by

66
00:06:42,690 --> 00:06:47,930
ah from a news corpus that is u s news corpus
so they are certain terms that are relevant

67
00:06:47,930 --> 00:06:54,000
to the u s ah medical ah system so what are
the so what you are seeing in this in the

68
00:06:54,000 --> 00:07:00,040
expected terms so some terms are like broad
explanation terms so that means if you look

69
00:07:00,040 --> 00:07:09,639
at may be some ah ah or some other thesaurus
you can find out such kind of ah similar terms

70
00:07:09,639 --> 00:07:14,530
so you will say ok medicare beneficiaries
premiums or some broad explanation terms so

71
00:07:14,530 --> 00:07:21,300
they are ready to the query in addition you
also get some very very specific domains domain

72
00:07:21,300 --> 00:07:29,139
term like here h c f a that is u s domain
term for health care financing administration

73
00:07:29,139 --> 00:07:35,330
similarly h m o for health maintenance organization
h h s so these are very very domain specific

74
00:07:35,330 --> 00:07:40,680
terms so if you know that you query in in
a cert is coming from a certain domain this

75
00:07:40,680 --> 00:07:45,460
approach can be very very helpful to also
find out terms that can ex used that can be

76
00:07:45,460 --> 00:07:48,740
used to explain the query and not also a domain
specific

77
00:07:48,740 --> 00:07:53,669
ok so let's take another example so these
are so i'm showing here some trec topics so

78
00:07:53,669 --> 00:07:58,870
trec is a text t o conference so they organize
various competitions for information retrieval

79
00:07:58,870 --> 00:08:04,849
so they are actual queries from the trec ah
data so you have a query like ocean remote

80
00:08:04,849 --> 00:08:11,319
sensing and when you use this method you can
find some other terms like radiometer landstat

81
00:08:11,319 --> 00:08:18,639
ionosphere c n e s altimeter nasda meteorology
and so on and ah again as we seen saw in the

82
00:08:18,639 --> 00:08:24,960
earlier query there are some broad explanation
terms like radiometer lanstat and ionosphere

83
00:08:24,960 --> 00:08:29,330
that are connected to the query and they are
some domain specific terms like c n e s and

84
00:08:29,330 --> 00:08:33,599
nasda here
so this is one very nice ah application of

85
00:08:33,599 --> 00:08:38,620
distribution sem semantic models you have
some ex existing query you want to match it

86
00:08:38,620 --> 00:08:42,819
to some document why don't you expand this
query by using some related terms and this

87
00:08:42,819 --> 00:08:48,890
is the idea and this we can use in general
for any rid matching task you are having do

88
00:08:48,890 --> 00:08:54,020
different text data and you are trying to
match these so try to find out if there are

89
00:08:54,020 --> 00:08:58,780
somehow related on the semantic label using
distribution semantic models and use this

90
00:08:58,780 --> 00:09:08,410
idea to find out if they are similar or not
now so ah so once we have computed the distributional

91
00:09:08,410 --> 00:09:13,829
semantic model so i have the vectors for different
words ok so different target words i have

92
00:09:13,829 --> 00:09:21,180
the vectors now how do i compute the similarity
between different words so it depends on what

93
00:09:21,180 --> 00:09:26,819
is your representation that you are using
for computing the ah semantics or the what

94
00:09:26,819 --> 00:09:30,851
is the vector representation if it is the
binary vector you will use different sort

95
00:09:30,851 --> 00:09:36,799
of similarity methods than if it is a ah real
valued vectors or if it is a probability distribution

96
00:09:36,799 --> 00:09:40,069
you will use a different sort of similarity
matrix

97
00:09:40,069 --> 00:09:45,290
so as is the framework allows you to use any
of these ah vector representations so let

98
00:09:45,290 --> 00:09:48,790
us see if you use if you are using any of
these what kind of similarity methods you

99
00:09:48,790 --> 00:09:53,930
can use to compute similarity between two
words so let us say my ah so have words x

100
00:09:53,930 --> 00:10:00,470
and y and they are denoted using some binary
vectors so as such any sort of distribution

101
00:10:00,470 --> 00:10:07,350
you can also convert into binary vectors
so how do i com ah compare ah how do i find

102
00:10:07,350 --> 00:10:12,000
similarity between two word two words where
the representation is binary vectors so for

103
00:10:12,000 --> 00:10:17,970
binary vectors we have some standard methods
like using dice coefficient ok so what is

104
00:10:17,970 --> 00:10:24,160
that ah two times intersection of x and y
divide by length of x plus length of y and

105
00:10:24,160 --> 00:10:35,470
what do i mean by this so x and y are binary
vectors suppose x and y are binary vectors

106
00:10:35,470 --> 00:10:42,900
and let us say ah the size of the my vector
the dimension are say since it is a nineteen

107
00:10:42,900 --> 00:10:51,820
dimensions ok and my x is one one one one
in the first ten dimension and zero zero in

108
00:10:51,820 --> 00:11:02,839
the rest nine dimensions on the y is zero
in the first nine dimension and one in the

109
00:11:02,839 --> 00:11:09,589
last ten dimensions
ok so now i have two vectors x and y and i

110
00:11:09,589 --> 00:11:14,819
want to compute the similarity between these
two vec vectors suppose i'm using dice coefficient

111
00:11:14,819 --> 00:11:22,690
so the formula is two times x intersection
y divided by length x plus length y so this

112
00:11:22,690 --> 00:11:29,510
is the length of x intersection y now what
is a x intersection y so that is only one

113
00:11:29,510 --> 00:11:36,370
element ok so x intersection y has one element
that is one so this will be simply one two

114
00:11:36,370 --> 00:11:41,870
times one length of x how many ones are there
ok so we'll not take the length of the vector

115
00:11:41,870 --> 00:11:45,920
because otherwise it will be the same for
everything everything will have the same size

116
00:11:45,920 --> 00:11:52,480
so length of x means how many entries are
one so here it will be ten for y also it is

117
00:11:52,480 --> 00:12:00,860
ten ten plus ten so this will a point one
ok so now this is ok if you have the binary

118
00:12:00,860 --> 00:12:07,020
ah values suppose you do not have the binary
values so suppose my values are like my x

119
00:12:07,020 --> 00:12:15,370
could be point three point five point seven
point zero one and so on so if you want to

120
00:12:15,370 --> 00:12:26,199
use these measures so one simple way could
be convert them to binary and now converting

121
00:12:26,199 --> 00:12:32,070
would mean you would put a threshold that
if the value is below this threshold then

122
00:12:32,070 --> 00:12:36,730
you put it to zero above this threshold you'll
put it to one so suppose here threshold is

123
00:12:36,730 --> 00:12:41,980
point zero five so what you'll do this will
go to zero this will go to one this will go

124
00:12:41,980 --> 00:12:47,640
to one this will go to one and here you'll
check whether less than zero point zero five

125
00:12:47,640 --> 00:12:53,140
then go to zero if greater than then go to
one like that so you can convert any such

126
00:12:53,140 --> 00:13:00,142
vector into binary representation and then
use this dice coefficient now are there some

127
00:13:00,142 --> 00:13:07,500
other methods of computing similarity also
so we have jaccard coefficient and overlap

128
00:13:07,500 --> 00:13:14,630
coefficient so jaccard coefficient is x intersection
y divided by x union y now what is the difference

129
00:13:14,630 --> 00:13:18,630
between jaccard and dice in what scenario
jaccard will gave give a different value than

130
00:13:18,630 --> 00:13:26,230
dice coefficient so let's try out the same
example so i'm using the jaccard as x intersection

131
00:13:26,230 --> 00:13:35,190
y divided by x union y ok so in this case
what is x intersection y this will remain

132
00:13:35,190 --> 00:13:42,350
as one only one entry is one and what is x
union y now x union y will contain all the

133
00:13:42,350 --> 00:13:49,300
elements this will be nineteen so what you
are seeing for the same two vectors dice give

134
00:13:49,300 --> 00:13:55,250
the value of point one and jaccard gives one
by nineteen that is closely point zero five

135
00:13:55,250 --> 00:14:02,550
so this give much smaller value
ok so why is that so you are seeing here if

136
00:14:02,550 --> 00:14:10,880
there are very small number of shared entries
jaccard further penalizes ok so that's what

137
00:14:10,880 --> 00:14:15,020
you are seeing here there are very only one
entry common among twenty so jaccard is giving

138
00:14:15,020 --> 00:14:24,660
further penalty
now what will overlap do so overlap is x intersection

139
00:14:24,660 --> 00:14:30,040
y divided by minimum of x y so can you think
of a scenario where overlap it can becomes

140
00:14:30,040 --> 00:14:37,620
one but say jaccard will not become one so
this will happen when ah one of x and y is

141
00:14:37,620 --> 00:14:47,439
completely incoud included in the ah other
so so let us say let us take a different case

142
00:14:47,439 --> 00:15:02,370
ah suppose my x is one one one one one zero
zero zero zero zero and y is one one one one

143
00:15:02,370 --> 00:15:12,970
one one one one zero zero ok so in this case
what would be the jaccard it will be x intersection

144
00:15:12,970 --> 00:15:23,069
y divided by x union y so this will be five
divided by eight and what will be overlap

145
00:15:23,069 --> 00:15:34,860
this will be x intersection y divided by minimum
of x and y ok so this is again five and what

146
00:15:34,860 --> 00:15:41,010
you mean of a x and y again five this will
become one so that is if if one of the vector

147
00:15:41,010 --> 00:15:48,549
is completely subsumed by another vector overlap
is one ok and that will not happen in jaccard

148
00:15:48,549 --> 00:15:52,850
dice coefficient and similarly if there are
a small number of shared entries jaccard will

149
00:15:52,850 --> 00:15:59,799
give us smaller value so that means depending
on what kind of ah ah what kind of similarity

150
00:15:59,799 --> 00:16:05,830
want to use you you can have either dice jaccard
overlap suppose in your task you want to find

151
00:16:05,830 --> 00:16:11,150
out if one of the words is completely subsumed
by another you will use overlap and not jaccard

152
00:16:11,150 --> 00:16:17,140
ok but if you want to see some other criteria
you can choose one of the three methods

153
00:16:17,140 --> 00:16:22,169
so here so what you have seen jaccard coefficient
penalizes small number of shared entries while

154
00:16:22,169 --> 00:16:26,429
overlap coefficient uses the concept of inclusion
where the one of the entries completely included

155
00:16:26,429 --> 00:16:34,240
in the other one now this is a vectors of
binary vectors suppose they are ah real number

156
00:16:34,240 --> 00:16:40,589
values so like x and y so the say the sa n
number of real number values so then you can

157
00:16:40,589 --> 00:16:44,000
use simple cosine similarity or euclidean
distance

158
00:16:44,000 --> 00:16:50,550
ok so you can use cosine similarity and euclidean
distance now what is the difference between

159
00:16:50,550 --> 00:16:57,640
the two if the my vectors are not normalized
cosine similarity euclidean distance are different

160
00:16:57,640 --> 00:17:03,079
ok but if they are normalized they will be
the same and they will give the same sort

161
00:17:03,079 --> 00:17:07,829
of ranking and that you can do a very simple
exercise if they are normalized they will

162
00:17:07,829 --> 00:17:14,490
give me the same sort of ranking between the
ah similarity of vectors now on the other

163
00:17:14,490 --> 00:17:20,650
hand suppose my distributions are probability
distributions so i am devot denoting different

164
00:17:20,650 --> 00:17:24,699
vector size probability distribution my space
of the context vectors

165
00:17:24,699 --> 00:17:30,020
so then how do i compute the similarity i'll
use different measures that are used for computing

166
00:17:30,020 --> 00:17:33,980
similarity or distance in the case of probability
distribution so what are the common measures

167
00:17:33,980 --> 00:17:42,870
so you can use k l k l divergence is so that
is sigma i p i log p i by q i so this so k

168
00:17:42,870 --> 00:17:49,232
l divergence is asymmetric ok so if you use
divergence between p and q and q and q and

169
00:17:49,232 --> 00:17:53,560
p they will come out they may come out to
be different so that's why there is a symmetric

170
00:17:53,560 --> 00:18:00,580
divergence also that is information radius
so p and p plus q by two and q and q plus

171
00:18:00,580 --> 00:18:07,950
p p by two fo find the k l divergence between
these and add this that is information radius

172
00:18:07,950 --> 00:18:13,170
and also you can use is a very simple formula
like l one norm so you have p i q i find out

173
00:18:13,170 --> 00:18:15,470
the l l one norm
so what is the difference between p i minus

174
00:18:15,470 --> 00:18:21,020
q i sum over all i so that is you have different
sort of representation and you can u use different

175
00:18:21,020 --> 00:18:29,320
similarity value similarity matrix now so
let us talk about what are the ah different

176
00:18:29,320 --> 00:18:34,040
what are some other sort of distribution semantic
models that all that also try to use some

177
00:18:34,040 --> 00:18:40,370
specific instructions in the sentences and
we try to motivate them using this example

178
00:18:40,370 --> 00:18:44,840
so that is what is the difference between
an attributional similarity task and a relational

179
00:18:44,840 --> 00:18:50,600
similarity task so what is ash attributional
similarity so that is i'm given two words

180
00:18:50,600 --> 00:18:56,920
like do dog and wolf and i want to find out
how similar they are so similarity between

181
00:18:56,920 --> 00:19:01,700
dog and wolf will depend on how much their
attributes are similar that that's why it

182
00:19:01,700 --> 00:19:04,680
is called attributional similarity and by
using distributional semantics how do you

183
00:19:04,680 --> 00:19:09,280
capture that what are the other words they
are co occurring with are they co occurring

184
00:19:09,280 --> 00:19:12,170
with similar sort of words if they are co
occurring with similar sort of words they

185
00:19:12,170 --> 00:19:18,680
will have a high attribution similarity and
what is relation similarity so that is slightly

186
00:19:18,680 --> 00:19:22,911
different
that is now i'm talking about pairs so that

187
00:19:22,911 --> 00:19:30,550
is i have two pairs a b and c d are they relationally
similar that is how many co similar relations

188
00:19:30,550 --> 00:19:37,050
that they have so example would be like one
pair is dog and bark second is cat and meow

189
00:19:37,050 --> 00:19:42,620
so now dog and bark do they share similar
sort of relation as cat and meow so this is

190
00:19:42,620 --> 00:19:48,170
simple different type of task ok and so this
that's why first one is called attribution

191
00:19:48,170 --> 00:19:52,240
similarity how much the attribution similar
among the two words second is called relation

192
00:19:52,240 --> 00:19:57,660
similarity i have the pair the relation between
this pair does that hold also for the other

193
00:19:57,660 --> 00:20:04,780
pair ok and this also gives tries to many
analogy testing task so a is to b as c is

194
00:20:04,780 --> 00:20:10,840
to what so we'll see how do we extend our
distributional seman similarity or semantics

195
00:20:10,840 --> 00:20:18,360
models to also capture all these cases
now for that we will talk about a different

196
00:20:18,360 --> 00:20:23,100
sort of matrix and this will be called pair
pattern matrix so till now you are talking

197
00:20:23,100 --> 00:20:27,850
about target context matrix so let us see
if you can use a similar idea for building

198
00:20:27,850 --> 00:20:31,490
a pair pattern matrix so what do i mean by
this

199
00:20:31,490 --> 00:20:36,760
so here the row vectors will correspond to
various pairs of words like mason stone and

200
00:20:36,760 --> 00:20:45,100
carpenter wood and the column would be various
patterns in that in the sentences these words

201
00:20:45,100 --> 00:20:52,690
occur with so like here x cuts y x works with
y etcetera and then you compute the similarity

202
00:20:52,690 --> 00:20:59,420
of rows to find similar pairs of words so
now so what what do i mean by this so till

203
00:20:59,420 --> 00:21:08,720
now what we were doing we had words like dog
cat and i was finding out these representation

204
00:21:08,720 --> 00:21:15,750
in various contexts so i'm finding out vectors
for dog vector for cat and trying to match

205
00:21:15,750 --> 00:21:26,010
these ok and this was my simple attributional
similarity so now what am i doing i'm trying

206
00:21:26,010 --> 00:21:31,820
to compute a similarity between pairs so now
my rows are different pairs so like the pairs

207
00:21:31,820 --> 00:21:50,640
can be say dog bark cat meow or it can be
like here mason stone carpenter wood so like

208
00:21:50,640 --> 00:22:00,190
that they can be various pairs now these are
my rows now what do the columns denote now

209
00:22:00,190 --> 00:22:12,230
columns would be various patterns so pattern
could be like x cuts y x works with y and

210
00:22:12,230 --> 00:22:17,300
so on these are various patterns now what
are x and y you can think of this as my patterns

211
00:22:17,300 --> 00:22:24,110
x y sorry pairs x y
now what ha how will i fill this matrix simple

212
00:22:24,110 --> 00:22:29,830
way would be i have x and y x can take value
like mason and y can take stone for a given

213
00:22:29,830 --> 00:22:36,610
pair now i will go through my coppers and
see what are the various patterns in which

214
00:22:36,610 --> 00:22:44,520
these words co occur with so suppose there
is a sentence mason works with or ah mason

215
00:22:44,520 --> 00:22:49,560
works with stone or carpenter works with wood
let's take a sentence carpenter works with

216
00:22:49,560 --> 00:22:57,160
wood so here so i have pattern x works with
wood x fits for carpenter y fits for wood

217
00:22:57,160 --> 00:23:01,800
so i'll say ok there is a plus one here similarly
there will be all sorts of pattern here in

218
00:23:01,800 --> 00:23:08,750
which x and y can occur and i'll fill which
pair occurs with what patterns so now this

219
00:23:08,750 --> 00:23:18,210
is my pair pattern matrix pair pattern matrix
and then i can once i have this matrix i can

220
00:23:18,210 --> 00:23:25,030
compute which pairs occur in similar patterns
and then they are called ah relationally similar

221
00:23:25,030 --> 00:23:33,310
they have similar relation at the other pair
so so then so we can talk about the extended

222
00:23:33,310 --> 00:23:38,990
distributional hypothesis this that much given
by lin and pantel so what is the idea patterns

223
00:23:38,990 --> 00:23:44,110
that co occur with similar pairs tend to have
similar meanings so here we are talking about

224
00:23:44,110 --> 00:23:50,510
in terms of our columns so here suppose what
they are saying if a pattern p one and pattern

225
00:23:50,510 --> 00:23:57,140
p two if they co occur with similar sort of
pairs then they are giving similar sort of

226
00:23:57,140 --> 00:24:02,960
meanings
ok and and therefore i can use this matrix

227
00:24:02,960 --> 00:24:07,820
to compute semantic similarity of patterns
also so i can find out the semantic similarity

228
00:24:07,820 --> 00:24:13,830
of the pairs also the patterns so suppose
i'm given a pattern like x solves y and i

229
00:24:13,830 --> 00:24:20,220
want to find out what are other similar ah
patterns as x solves y how will i do that

230
00:24:20,220 --> 00:24:25,460
i will first enumerate all the possible patterns
that can occur with with x and y then i find

231
00:24:25,460 --> 00:24:30,140
out all the possible pairs how many times
they co occur with various patterns so i'll

232
00:24:30,140 --> 00:24:39,200
fill this matrix and then i'll find out for
this pattern like x solves y what are some

233
00:24:39,200 --> 00:24:43,940
other patterns that us that are having similar
pairs as x solves y

234
00:24:43,940 --> 00:24:56,650
ok and what are patterns like x is solved
by y sorry y is solved by x ok suppose it

235
00:24:56,650 --> 00:25:01,770
occurs with similar pairs as x solves y i'll
say that this and this are same and that will

236
00:25:01,770 --> 00:25:08,760
be the idea its like here y is solved by x
y is resolved in x and x resolves y so what

237
00:25:08,760 --> 00:25:16,570
do you find all these patterns occur with
similar pairs so they can be also called similar

238
00:25:16,570 --> 00:25:23,309
now so now what is one thing so for dealing
with this pair pattern matrix words will not

239
00:25:23,309 --> 00:25:30,480
be my basic context units so how do i capture
and represent this sort of information like

240
00:25:30,480 --> 00:25:38,650
x solves y and y resolved by y x how do i
capture this information so for that i'll

241
00:25:38,650 --> 00:25:47,220
need a formalism that can capture semantic
relations and also ah various syntactic information

242
00:25:47,220 --> 00:25:52,960
can be captured and for that we will go back
to our dependency based formalism to capture

243
00:25:52,960 --> 00:26:01,920
this kind of information what is the idea
so let us say ah i have a sentence like the

244
00:26:01,920 --> 00:26:10,380
teacher eats a red apple so i can first formulate
a dependency graph for this sentence now i

245
00:26:10,380 --> 00:26:16,050
can use this dependency relations to say that
only some sort of dependency relations are

246
00:26:16,050 --> 00:26:23,270
interesting and others are not interesting
ok so for example i can say that eat is not

247
00:26:23,270 --> 00:26:29,610
a legitimate context for red ok although eats
and red co occur with the very small distance

248
00:26:29,610 --> 00:26:36,430
context window i can say that ok the relation
det object an and a modifier together do not

249
00:26:36,430 --> 00:26:42,640
form a very nice context so i'will not use
this co occurrence so i can be selective in

250
00:26:42,640 --> 00:26:46,510
choosing what kind of co occurrence information
i will use and what kind of co occurrence

251
00:26:46,510 --> 00:26:51,590
information i will not use and i may also
give them different sort of weights so for

252
00:26:51,590 --> 00:26:56,970
example i can say that the object relation
connecting eat an apple will be different

253
00:26:56,970 --> 00:27:03,950
than the modified relation connecting red
and apple ok so this relation det object and

254
00:27:03,950 --> 00:27:11,240
a modifier at can be different relations so
till now what was happening i was only seeing

255
00:27:11,240 --> 00:27:14,779
if this word co occurrence with this word
or not

256
00:27:14,779 --> 00:27:20,470
so now we have started talking about in different
terms this word co occurs with another word

257
00:27:20,470 --> 00:27:26,730
in this context ok so with using a det object
relation with using an adjective modifier

258
00:27:26,730 --> 00:27:32,970
relation and so on now so so from the parser
i can get all these relations so how do i

259
00:27:32,970 --> 00:27:42,170
further use those so ah so we'll say that
to qualify as a context a word must be linked

260
00:27:42,170 --> 00:27:48,080
by some interesting lexicon syntactic relation
so wha what do i mean by lexicon syntactic

261
00:27:48,080 --> 00:27:53,840
relation a good dependency path should adjust
between the two words so in in simple terms

262
00:27:53,840 --> 00:27:59,270
i can only use of use a single edge between
two words ok but in general you can also talk

263
00:27:59,270 --> 00:28:05,870
about ah larger path larger len length of
edge between the two words so let's take it

264
00:28:05,870 --> 00:28:10,980
simply for the simple paths of length one
between two words

265
00:28:10,980 --> 00:28:17,690
so what will i do so let us see i have a sentence
the virus affects the body's defense system

266
00:28:17,690 --> 00:28:24,370
and you get the dependency parse now from
the dependency parse i can extract the various

267
00:28:24,370 --> 00:28:39,830
triplets like system det object affects body
possessive system so these are the various

268
00:28:39,830 --> 00:28:46,970
ah topples i can extract from a dependency
graph so i will have things like this system

269
00:28:46,970 --> 00:28:55,890
det object effects and so on now how do i
use those for my ah structured model so let

270
00:28:55,890 --> 00:29:07,620
us try to ah have a look so we will have words
like system d o b j and affects yes and i

271
00:29:07,620 --> 00:29:15,330
have many such pairs many such topples now
how do i use those for my distributional semantics

272
00:29:15,330 --> 00:29:24,960
so there are actually many ways you can do
this so one simple way would be just forget

273
00:29:24,960 --> 00:29:36,360
dependency information so that means you will
have system affects and so on so that is you

274
00:29:36,360 --> 00:29:42,510
going back to your earlier relation where
you are not using the instruction so this

275
00:29:42,510 --> 00:29:49,510
you can again convert into that kind of model
the word system is here and what is the co

276
00:29:49,510 --> 00:29:56,760
occurrence with the word affects and you will
have one and so on there is one way so where

277
00:29:56,760 --> 00:30:00,360
you forget the dependency information but
that's not what you wanted right you wanted

278
00:30:00,360 --> 00:30:06,140
this information for some reason so another
option could be i combine dependency information

279
00:30:06,140 --> 00:30:17,420
with the context so my context is now structured
so that is i'll have system and my context

280
00:30:17,420 --> 00:30:25,390
is det object of the verbs affects ok so these
are my context now

281
00:30:25,390 --> 00:30:40,610
so then i can represent system 
in this context det object of affects det

282
00:30:40,610 --> 00:30:46,140
object of other verbs and subject of these
verbs and so on ok so now you see immediately

283
00:30:46,140 --> 00:30:50,600
my dimensions are different from here here
i had only words here i had verbs and some

284
00:30:50,600 --> 00:30:56,300
relation together but i'm going back to the
same sort of matrix format and still i cannot

285
00:30:56,300 --> 00:31:07,630
use this dog is to bark and cat is to meow
i cannot do it here now what is some other

286
00:31:07,630 --> 00:31:10,850
sort of representation from here that you
can gather

287
00:31:10,850 --> 00:31:16,530
so other could be i combine these two in my
rows this become my target and this come become

288
00:31:16,530 --> 00:31:27,799
my context so that would be third would be
so like system and affects coming my target

289
00:31:27,799 --> 00:31:38,740
and det object is my context so now we can
talk about my pair matrix so i have systems

290
00:31:38,740 --> 00:31:46,690
and affects these are my pairs and pattern
right now only dependency relation det dependency

291
00:31:46,690 --> 00:31:56,580
relation det object and subject and so on
ok and this is the general framework pair

292
00:31:56,580 --> 00:32:06,290
pattern now here we are talking about only
very simple paths of length one what is the

293
00:32:06,290 --> 00:32:10,830
det relation between these two words you are
free to choose a higher order path it can

294
00:32:10,830 --> 00:32:18,750
be ok there is a path of length det object
n subject and so on and use that at your patterns

295
00:32:18,750 --> 00:32:23,270
here
ok so so that can help you to capture all

296
00:32:23,270 --> 00:32:29,460
these things like x solves y so you can also
use what are the different words that occur

297
00:32:29,460 --> 00:32:33,750
in between these two words if there are some
other words occurring that can be your one

298
00:32:33,750 --> 00:32:37,690
of the pattern so here you have complete freedom
of choosing what are your pattern so these

299
00:32:37,690 --> 00:32:42,460
are simple dependency graph for for simplicity
but you can choose any other patterns that

300
00:32:42,460 --> 00:32:47,980
you would like these patterns come from may
be dependency graph come from the word co

301
00:32:47,980 --> 00:32:52,700
occurrence how what are the words that are
occurring between these words and so on and

302
00:32:52,700 --> 00:32:56,309
then you can compute similarity between various
pairs

303
00:32:56,309 --> 00:33:06,140
ok so now quickly so we have this data and
i can use that to get this information or

304
00:33:06,140 --> 00:33:10,290
this the other sort of information or the
third sort sort of the information that we

305
00:33:10,290 --> 00:33:17,610
saw now let us see one simple application
like how do we use that to find out selection

306
00:33:17,610 --> 00:33:22,350
preferences of the verbs now what do i mean
by this selection preferences now different

307
00:33:22,350 --> 00:33:29,460
verbs for their different argument like object
subject they prefer a particular type of noun

308
00:33:29,460 --> 00:33:34,299
and this information is useful in many task
like dependency parsing so for eat i want

309
00:33:34,299 --> 00:33:41,679
to know that eat will prefer only certain
ah sort of nouns as objects ok you can eat

310
00:33:41,679 --> 00:33:49,700
only verys very ah specific things so like
so how do we compute this selection preferences

311
00:33:49,700 --> 00:33:56,890
for verbs so we have seen that that from a
parsed corpus i can ah compute these vectors

312
00:33:56,890 --> 00:34:04,071
like virus and system i can put them in this
space how many times car occurs as object

313
00:34:04,071 --> 00:34:09,940
of carry so you are carrying car you are buying
car you are driving car you are eating car

314
00:34:09,940 --> 00:34:12,450
and you are storing car and flying car and
so on

315
00:34:12,450 --> 00:34:17,869
ok so what are the number of times and this
is some normalized values vegetables how many

316
00:34:17,869 --> 00:34:24,169
times are they carried bought eaten stored
and so on ok so this you can compute from

317
00:34:24,169 --> 00:34:30,609
the corpus once you have the parse so this
gives you some sort of representation that

318
00:34:30,609 --> 00:34:37,720
what kind of objects come into that can be
used ah ah what kind of words that can come

319
00:34:37,720 --> 00:34:45,100
as object of the verb buy what kind of words
will come as object of the word verb drive

320
00:34:45,100 --> 00:34:54,050
and so on ok but suppose you want to build
a prototype that is in general what kind of

321
00:34:54,050 --> 00:35:00,750
words can come as object of the verb eat from
the corpus here one simple thing i can do

322
00:35:00,750 --> 00:35:06,250
i can find out which words has have a high
value so you know vegetables can be eaten

323
00:35:06,250 --> 00:35:11,250
biscuits can be eaten so they have a high
value but suppose i want two hundred prototype

324
00:35:11,250 --> 00:35:15,800
for a new wor the words that is not occur
in the corpus how likely it is to come as

325
00:35:15,800 --> 00:35:23,560
a object of the verb eat so what will i do
so what is the simple method so i would say

326
00:35:23,560 --> 00:35:29,119
ok let me find out what are the words that
are having a high weight in this dimension

327
00:35:29,119 --> 00:35:35,770
so you know vegetables biscuits fruits etcetera
will ga will have a high weight in this dimension

328
00:35:35,770 --> 00:35:42,830
now my hypothesis will be words that are similar
to these words like vegetable biscuits can

329
00:35:42,830 --> 00:35:47,010
also be eat now how do i find out words that
are similar to vegetables biscuits and so

330
00:35:47,010 --> 00:35:53,360
on so for that i'll build a prototype that
is ok what are these words so these are words

331
00:35:53,360 --> 00:36:01,550
that can be carried bought stored right so
that means any other words set can also be

332
00:36:01,550 --> 00:36:09,580
carried bought stored might also be ah eaten
ok so then i will find out all the other words

333
00:36:09,580 --> 00:36:14,130
that have high weights in these dimensions
other than eat because whatever is having

334
00:36:14,130 --> 00:36:17,830
high dimension eat i can easily capture here
but what are the other words that are having

335
00:36:17,830 --> 00:36:23,750
high dimension in all these high weights in
all these other dimensions that also can become

336
00:36:23,750 --> 00:36:29,430
my prototype
so what will we do suppose i want to compute

337
00:36:29,430 --> 00:36:34,850
the selection preferences of the nouns as
object of the verb eat i'll take some top

338
00:36:34,850 --> 00:36:40,160
n words like vegetables biscuit that have
high weight in this dimension of object eat

339
00:36:40,160 --> 00:36:45,350
then i take the complete vectors so that is
of all these top nouns so this will words

340
00:36:45,350 --> 00:36:54,100
like that can be consumed bought carried stored
etcetera now this becomes my object prototype

341
00:36:54,100 --> 00:36:58,560
now given any noun try to match it with this
with this object prototype and you will see

342
00:36:58,560 --> 00:37:03,580
that how likely it is to come as a object
of the verb eat and that is the generic method

343
00:37:03,580 --> 00:37:10,410
of finding selection preferences of an word
noun for any ah word noun to come as a object

344
00:37:10,410 --> 00:37:16,480
ah or subject for verb so we talked about
ah how do you extend your distribution semantic

345
00:37:16,480 --> 00:37:22,680
method for using a structured models ok a
lot of work again a lot of research has gone

346
00:37:22,680 --> 00:37:28,060
to this si this domain so you have already
touched the basic and i hope if you if you

347
00:37:28,060 --> 00:37:32,520
need this idea you on your own you can you
can think about how do you use different sort

348
00:37:32,520 --> 00:37:40,490
of interestings context interesting targets
and and solve various problems so in the next

349
00:37:40,490 --> 00:37:45,620
lecture we will talk about another very import
important interesting idea that is what are

350
00:37:45,620 --> 00:37:52,060
word vectors so and word embedding and how
do we obtain them from the corpus and and

351
00:37:52,060 --> 00:37:57,220
what are different tasks they can be used
on ok so i'll see you in the next lecture

352
00:37:57,220 --> 00:37:57,470
thank you

