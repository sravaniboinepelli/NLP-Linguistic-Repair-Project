1
00:00:17,480 --> 00:00:21,779
okay so welcome that for the final lecture
for second week so in the last lecture we

2
00:00:21,779 --> 00:00:27,410
were discussing about the basic of language
modeling and there we we just hinted about

3
00:00:27,410 --> 00:00:34,110
the one problem about the zeros in language
models so today in this lecture we will discuss

4
00:00:34,110 --> 00:00:40,140
how not only how to handle this zeros in language
modeling's but also how do you go about evaluating

5
00:00:40,140 --> 00:00:43,870
your language model that you have learned
from some data [vocalized noise] ok so these

6
00:00:43,870 --> 00:00:52,010
are the two main topics for this lecture ok
so so we start with the evaluation of language

7
00:00:52,010 --> 00:00:57,190
models so so what is the basic criteria for
evaluating a language model so one symbol

8
00:00:57,190 --> 00:01:02,840
indication could be a language model is better
if it is assigning a high probability to the

9
00:01:02,840 --> 00:01:07,570
actual sentences the real sentences and a
low probability sentences that are not so

10
00:01:07,570 --> 00:01:14,110
grammatical that can be one criteria for evaluating
language models but can but this has to be

11
00:01:14,110 --> 00:01:19,750
done manually so you will have to check what
is a probability that your model is assigning

12
00:01:19,750 --> 00:01:25,330
to various sentences that are real most of
the sentences that are not real that are not

13
00:01:25,330 --> 00:01:34,820
grammatical but can we do that in an automatic
manner so for that we might we may also use

14
00:01:34,820 --> 00:01:40,140
some sort of training and test data so so
what is the idea behind using this training

15
00:01:40,140 --> 00:01:48,739
and test data so i have some corpus and i
learn my language model on some part of that

16
00:01:48,739 --> 00:01:52,039
okay
so this is my training set now once i have

17
00:01:52,039 --> 00:01:58,149
learned that language model from my training
set i will test whether it is providing a

18
00:01:58,149 --> 00:02:03,679
good high probability for my test data and
i will have some evaluation evaluation metric

19
00:02:03,679 --> 00:02:09,429
for finding out how good this model is doing
on my test data and by this method i can i

20
00:02:09,429 --> 00:02:13,459
can even compare across different language
models so if i have learned three different

21
00:02:13,459 --> 00:02:19,260
language models for example i can find out
which one does better on my test data and

22
00:02:19,260 --> 00:02:25,790
that will be the best model ok so in general
what are the different ways in which language

23
00:02:25,790 --> 00:02:30,909
models can be evaluated so as such there are
two different criteria for evaluation one

24
00:02:30,909 --> 00:02:36,120
is called extrinsic another one is called
intrinsic ok we will discuss what are these

25
00:02:36,120 --> 00:02:41,870
individually [vocalized noise]
so what do i mean by extrinsic evaluation

26
00:02:41,870 --> 00:02:46,209
of my language models [vocalized noise] so
suppose i have learned two different language

27
00:02:46,209 --> 00:02:53,790
models a and b from some training data now
what do we do in extrinsic evaluation we try

28
00:02:53,790 --> 00:02:57,799
to use them on certain task so remember some
of the applications that we have discussed

29
00:02:57,799 --> 00:03:02,629
offline language models so they can used in
the spelling correction or in speech recog

30
00:03:02,629 --> 00:03:08,120
recognition and all that so now what i will
do in this extrin extrinsic evaluation is

31
00:03:08,120 --> 00:03:15,209
that once i have learnt two different models
i will try apply them to these tasks individually

32
00:03:15,209 --> 00:03:19,330
and then see what is the performance that
i am obtaining for each of this task by different

33
00:03:19,330 --> 00:03:26,349
models now i will say whichever model is giving
a better performance on these task is my preferable

34
00:03:26,349 --> 00:03:32,590
model so this is called extrinsic evaluation
or task based evaluation ok so here we have

35
00:03:32,590 --> 00:03:37,110
discussed if we have mentioned three different
tasks a spelling correction a speech recognition

36
00:03:37,110 --> 00:03:41,700
and machine translation i will have two models
a and b i will get accuracy values for these

37
00:03:41,700 --> 00:03:45,950
three tasks and i will compare the accuracy
and tell which language model is preferable

38
00:03:45,950 --> 00:03:51,779
than other so this is my extrinsic evaluation
but if there is some intrinsic way of evaluation

39
00:03:51,779 --> 00:03:56,969
without actually applying it on some task
ok

40
00:03:56,969 --> 00:04:02,139
so for that so we have the motion of perplexity
so this is how we evaluate language model

41
00:04:02,139 --> 00:04:08,230
for for for intrinsic evaluation so intuition
comes from the shannon game if you heard about

42
00:04:08,230 --> 00:04:14,519
this name so so what is that what is that
game how well can you predict the next word

43
00:04:14,519 --> 00:04:22,019
given sudden context ok so let us see some
examples so i have the first sentences here

44
00:04:22,019 --> 00:04:26,460
i always order pizza with cheese and and there
is a word that you have to predict may be

45
00:04:26,460 --> 00:04:30,960
you can say pepper or whatever and similarly
the second sentence the president of india

46
00:04:30,960 --> 00:04:37,860
is and you can predict the next word similarly
in the third sentence i wrote a and you can

47
00:04:37,860 --> 00:04:44,861
predict letter or program or whatever ok
so now in my data i know what are the words

48
00:04:44,861 --> 00:04:50,540
that will fill up these blanks now suppose
i have two different language models i will

49
00:04:50,540 --> 00:04:57,030
find out which of these fills up these blanks
better than the other one ok so whichever

50
00:04:57,030 --> 00:05:01,670
one is good at predicting the next word is
my preferable language models so one thing

51
00:05:01,670 --> 00:05:09,660
you can easily say from here the unigram models
are probably not built for this task ok so

52
00:05:09,660 --> 00:05:13,920
suppose you want to predict the next word
you just you need a model what would happen

53
00:05:13,920 --> 00:05:20,760
so if you remember unigram models do not use
the context at all so you will just end up

54
00:05:20,760 --> 00:05:25,970
providing every at every place the word that
is having the highest probability ok

55
00:05:25,970 --> 00:05:30,120
so probably unigram models are not good for
this task but you can apply a bigram models

56
00:05:30,120 --> 00:05:34,031
the the model which uses the previous word
trigram models that uses the previous two

57
00:05:34,031 --> 00:05:44,040
words and so on [vocalized noise] ok so so
finally among different language models a

58
00:05:44,040 --> 00:05:50,610
better model is one that you will assign a
higher probability to the actual word ok now

59
00:05:50,610 --> 00:05:57,450
what is the formal definition of perplexity
so in general what we are trying to find out

60
00:05:57,450 --> 00:06:03,820
the best language model that predicts an unseen
test data ok so how do i define perplexity

61
00:06:03,820 --> 00:06:11,410
ok so so this is a simple definition of perplexity
i have some test data i am finding out the

62
00:06:11,410 --> 00:06:16,970
inverse probability of test data normalized
by the number of words that i am seeing in

63
00:06:16,970 --> 00:06:21,870
my test data ok so thats means finding the
probability of test data and then normalize

64
00:06:21,870 --> 00:06:27,150
it with respect to the number of words and
its inverse probability ok that means if i

65
00:06:27,150 --> 00:06:33,800
have a low perplexity i have a better model
ok so formally i can define my per perplexity

66
00:06:33,800 --> 00:06:38,410
like that suppose in my test data i have words
w one to w n i am finding the probability

67
00:06:38,410 --> 00:06:45,740
of this whole sequence and normalizing it
by minus one by n here

68
00:06:45,740 --> 00:06:53,360
ok so now now this is is a general definition
now the next question that you might have

69
00:06:53,360 --> 00:06:57,520
is that where does the language model come
into picture in this definition is simple

70
00:06:57,520 --> 00:07:02,490
definition of finding probability of this
whole address w one w n and sub with some

71
00:07:02,490 --> 00:07:08,960
normalization now where does the language
model come into picture now remember the chain

72
00:07:08,960 --> 00:07:15,180
rule that we discussed of probabilities if
i have this sequence w one to w n how can

73
00:07:15,180 --> 00:07:19,001
write the probability of the sequence in terms
using the chain rule of probability so that

74
00:07:19,001 --> 00:07:23,210
is what you see here so i can write the same
expression like that one divided by probability

75
00:07:23,210 --> 00:07:30,430
of w i given the previous i minus one words
ok this is in general this definition of probability

76
00:07:30,430 --> 00:07:38,630
w one to w n now now can you see how do we
apply language models in this definition so

77
00:07:38,630 --> 00:07:44,870
in language model we take one particular assumption
about using this chain rule that is how many

78
00:07:44,870 --> 00:07:50,560
previous context will i will be be using ok
so you can in place of this you can replace

79
00:07:50,560 --> 00:07:54,790
any of the model that you have learnt so suppose
you want to replace with your bi bigram model

80
00:07:54,790 --> 00:07:58,890
that means you want to find out what is the
probability that the bigram model will give

81
00:07:58,890 --> 00:08:04,350
for this utterance so what you will do so
we simply replace here the bigram model probability

82
00:08:04,350 --> 00:08:09,100
so that will give you the perplexity for the
bigram model that you have learnt or trained

83
00:08:09,100 --> 00:08:14,150
using some training data ok so you will feed
in all this probability w i given w i minus

84
00:08:14,150 --> 00:08:18,410
one in that model and that will give you the
perplexity of the bigram model [vocalized

85
00:08:18,410 --> 00:08:22,610
noise]
ok so now just to give you an intuition what

86
00:08:22,610 --> 00:08:27,640
do i mean what the number of number that i
will get indicates perplexity value what does

87
00:08:27,640 --> 00:08:35,200
that indicate let us take a simple example
[vocalized noise] so suppose i have a sentence

88
00:08:35,200 --> 00:08:45,200
that contains n random digits ok and i have
a model that assigns a prob a probability

89
00:08:45,200 --> 00:08:51,220
of one by ten to each digit ok now i want
to find a perplexity of the sentence using

90
00:08:51,220 --> 00:08:55,440
that model that will give me a probability
of one by ten to each digit so what will be

91
00:08:55,440 --> 00:09:01,199
the perplexity let us try to use the definition
ok so what how the did we define perplexity

92
00:09:01,199 --> 00:09:09,680
that is probability of this whole sequence
yes w one w n to the power minus one by n

93
00:09:09,680 --> 00:09:15,230
yes so can you try to fill in the values suppose
my model gives a probability of one by ten

94
00:09:15,230 --> 00:09:24,500
to each word so this would be one by ten to
each word to the power n for n different words

95
00:09:24,500 --> 00:09:29,920
n to the power minus one by n so that is nothing
but one by ten to the power minus one and

96
00:09:29,920 --> 00:09:36,939
that will give me ten so this tells me [vocalized
noise] that the perplexity of this model is

97
00:09:36,939 --> 00:09:43,170
ten the model that assigns a probability of
one by ten to each digit ok now this might

98
00:09:43,170 --> 00:09:49,439
give you a hint of what this number indicates
ok so let us take another example from some

99
00:09:49,439 --> 00:09:54,040
test data oath so what kind of perplexity
values we get and what can how we can interpret

100
00:09:54,040 --> 00:09:59,610
this values [vocalized noise]
so an experiment was experiment was conducted

101
00:09:59,610 --> 00:10:05,670
over wall street journal corpus so in training
they had thirty eight millionwords on which

102
00:10:05,670 --> 00:10:09,930
the language model was trained and for testing
they had one point five million words on which

103
00:10:09,930 --> 00:10:15,720
perplexity was computed and they trained three
different models unigram model bigram model

104
00:10:15,720 --> 00:10:22,180
and trigram model now and they found out what
is the perplexity of the model in using the

105
00:10:22,180 --> 00:10:29,230
test set so these are the numbers that were
found for unigram model perplexity was nine

106
00:10:29,230 --> 00:10:34,879
sixty two bigram one seventy and trigram one
zero nine so now let us try to answer this

107
00:10:34,879 --> 00:10:42,370
question what is this value of nine sixty
two perplexity in unigram indicate ok so what

108
00:10:42,370 --> 00:10:51,170
it means is that whenever my model is trying
to assign a word it is it is as if it is has

109
00:10:51,170 --> 00:10:56,970
to choose among nine sixty two different possib
possibilities at each individual choice point

110
00:10:56,970 --> 00:11:01,749
independently and randomly ok
so this is this means the model is very very

111
00:11:01,749 --> 00:11:07,079
perplexed so if the perplexity is high my
model is very very confused if it is low my

112
00:11:07,079 --> 00:11:12,259
model is not so much confused so in this case
what you are seeing if i use a unigram model

113
00:11:12,259 --> 00:11:17,310
perplexity is nine sixty two the model is
very very perplexed but if i go for a bigram

114
00:11:17,310 --> 00:11:22,720
model it is one seventy the model is not so
much perplexed now trigram model it becomes

115
00:11:22,720 --> 00:11:29,519
even better its only one hundred nine ok so
thats what you are seeing unigram per perplexity

116
00:11:29,519 --> 00:11:36,149
of nine sixty two means the model is as confused
on the test data as if it had it had to choose

117
00:11:36,149 --> 00:11:40,199
uniformly and independently among nine six
sixty two different possibilities for each

118
00:11:40,199 --> 00:11:45,220
word ok and that you can also relate with
your previous example because every time it

119
00:11:45,220 --> 00:11:50,709
had it had to choose among ten different possibilities
for each digit ok because it is about giving

120
00:11:50,709 --> 00:11:54,760
a probability of one by ten to each digit
[vocalized noise] ok so now you understood

121
00:11:54,760 --> 00:12:03,029
what the perplexity means
now so once we have built a language model

122
00:12:03,029 --> 00:12:09,540
we can also use it for other tasks so one
very interesting task is called shannon visualization

123
00:12:09,540 --> 00:12:17,149
method that is can i use this language model
to visualize or generate sentences ok so if

124
00:12:17,149 --> 00:12:23,060
you have read the original paper of shannon
on the mathematical information theory so

125
00:12:23,060 --> 00:12:28,329
there he uses this method to generate various
sequences of words ok so what is the idea

126
00:12:28,329 --> 00:12:34,839
suppose i have learnt a language model can
i use that to generate various sequences of

127
00:12:34,839 --> 00:12:42,100
words or sentences so how do we actually apply
this shannon visualization method [vocalized

128
00:12:42,100 --> 00:12:48,480
noise] so suppose i have to generate a sentence
so what i will do i will have to generate

129
00:12:48,480 --> 00:12:55,449
a sentence so i will first choose a random
bigram that is starting the sentence as be

130
00:12:55,449 --> 00:13:00,480
the probability so what do i mean by that
[vocalized noise]

131
00:13:00,480 --> 00:13:08,920
ok so i am learning my bigrams so i have in
with the start of the sentence whether the

132
00:13:08,920 --> 00:13:15,449
word w one occurs with what probability with
the start of this sentence what w two occurs

133
00:13:15,449 --> 00:13:20,120
with some probability so this is a complete
probability distribution that will add up

134
00:13:20,120 --> 00:13:27,600
to one now if i have to generate a sentence
i have to choose one among this possibilities

135
00:13:27,600 --> 00:13:34,009
and this will depend on the probability of
that bigram ok so this you can think of as

136
00:13:34,009 --> 00:13:39,069
multinomial distribution and you are sampling
one word from this multinomial distribution

137
00:13:39,069 --> 00:13:44,851
so suppose you pick picked up a particular
word here ok so you start generating a sentence

138
00:13:44,851 --> 00:13:52,410
you start n w two ok now i have to choose
the next word so that is the next step choose

139
00:13:52,410 --> 00:13:59,339
a random bigram wx as per its probability
so what i will do now i will go to the distribution

140
00:13:59,339 --> 00:14:10,929
w to n different words w one w two w two and
so on and from this distribution again from

141
00:14:10,929 --> 00:14:18,480
this multinomial distribution i will sample
one sample one word suppose i find sum w fifty

142
00:14:18,480 --> 00:14:24,420
ok again i will try to a sample word with
its distribution w fifty and the next word

143
00:14:24,420 --> 00:14:29,840
now the question is when do i stop when do
i say that my sentence is complete i will

144
00:14:29,840 --> 00:14:35,670
say my sentence is complete once i sample
a word with the end of the sentence ok at

145
00:14:35,670 --> 00:14:41,869
some point i sample wi and the next word is
end of the sentence i say my sentence is closed

146
00:14:41,869 --> 00:14:49,910
ok so this is the whole idea of shannon visualization
method

147
00:14:49,910 --> 00:14:58,050
so i do that until i choose end of the sentence
and we will see one example from one corpus

148
00:14:58,050 --> 00:15:02,850
so from restaurant corpus that we discussed
in the last lecture suppose i have learned

149
00:15:02,850 --> 00:15:07,670
my bigram model and i want to generate a sentence
so how will i do that i choose start of a

150
00:15:07,670 --> 00:15:12,709
sentence find out the first word i find i
then i take the first word as i choose the

151
00:15:12,709 --> 00:15:18,040
next word how do i choose choose the words
by sampling from the multinomial distributions

152
00:15:18,040 --> 00:15:24,760
ok so in this case if we sample we might end
up with getting the sentence i i want want

153
00:15:24,760 --> 00:15:29,910
to to eat eat chinese food and after food
we get the end of the sentence so i want to

154
00:15:29,910 --> 00:15:36,540
eat chinese food would be a sentence that
is generated by this method

155
00:15:36,540 --> 00:15:43,480
now suppose we try to use this method over
the shakespeares corpus so there so we have

156
00:15:43,480 --> 00:15:48,620
some number of tokens and the vocabulary size
is twenty nine thousand sixty six ok just

157
00:15:48,620 --> 00:15:54,339
to give you an indication that bigrams are
actually very very sparse so if you take the

158
00:15:54,339 --> 00:15:57,680
voc vocabulary size of twenty nine thousand
something how many bigrams are possible so

159
00:15:57,680 --> 00:16:04,480
what do you mean by a bigram two words together
so i can take say v square are the possible

160
00:16:04,480 --> 00:16:10,679
bigrams because every possible combination
can occur but so this gives me a number of

161
00:16:10,679 --> 00:16:16,230
eight forty four million possible bigrams
but in the corpus how many bigrams were actually

162
00:16:16,230 --> 00:16:20,069
observed so we find there were only three
hundred thousand bigrams that we have observed

163
00:16:20,069 --> 00:16:27,249
in the corpus so this is very very sparse
ok so now suppose i build various language

164
00:16:27,249 --> 00:16:33,679
models from this corpus and try to generate
various paragraphs and sentences so what do

165
00:16:33,679 --> 00:16:38,689
we actually observe ok you will see what happens
if i take unigram model bigram model trigram

166
00:16:38,689 --> 00:16:45,920
model and and high order models
so so here you are seeing if you take unigram

167
00:16:45,920 --> 00:16:52,809
model you are getting some words that are
probably very popular in shakespeares but

168
00:16:52,809 --> 00:16:59,100
the sentence themselves do ah are not making
sense ok so you see the word like which here

169
00:16:59,100 --> 00:17:03,089
it occurs independent of any other words and
this becomes a complete sentence that will

170
00:17:03,089 --> 00:17:08,150
not probably something that you will see in
the shakespeares corpus so now suppose you

171
00:17:08,150 --> 00:17:13,559
go to the bigram model ok now so you start
making some sense so you have words sequences

172
00:17:13,559 --> 00:17:21,720
like what means ok i confess and all sorts
ok they have good bigrams but if you's try

173
00:17:21,720 --> 00:17:26,429
to observe the sentence probably they are
not making much sense now if you go to trigram

174
00:17:26,429 --> 00:17:32,460
model then again you have getting some more
sense five first if shelled i and the shell

175
00:17:32,460 --> 00:17:38,659
forward it should be should be bent they have
some nice sequences again and this is starts

176
00:17:38,659 --> 00:17:44,820
making a much more sense in terms of sentences
and if you go to quad terms then you are getting

177
00:17:44,820 --> 00:17:51,120
something that is resembling ok king henry
henry watt i shall go see the traitor gloucester

178
00:17:51,120 --> 00:17:55,960
ok so this look like a valid sentences from
the shakespeares corpus so that is the idea

179
00:17:55,960 --> 00:18:00,100
as you go to higher and highder higher order
model the kind of sentences you generate will

180
00:18:00,100 --> 00:18:05,630
be something that actually resemble the corpus
from which we are hm training this language

181
00:18:05,630 --> 00:18:10,080
model
okay so this is my visualization method so

182
00:18:10,080 --> 00:18:16,390
now we will try to go to the problem of a
smoothing that we discussed in the last lecture

183
00:18:16,390 --> 00:18:23,539
so remember what the problem was in my language
model suppose i am training for bigrams so

184
00:18:23,539 --> 00:18:28,419
so yes so just take the statistic that we
saw from shakespeares corpus there were eight

185
00:18:28,419 --> 00:18:33,169
hundred forty four million possible bigrams
out of which only three hundred thousand bigram

186
00:18:33,169 --> 00:18:39,010
actually occur so now if you assign the probability
to each bigram very few will will get a probability

187
00:18:39,010 --> 00:18:44,710
greater than zero others will get a probability
of zero now suppose you are taking a test

188
00:18:44,710 --> 00:18:50,720
data and finding out how much it resembles
shakespeares corpus and whenever you see a

189
00:18:50,720 --> 00:18:55,340
bigram you are taking probability from the
trained language model now suppose a bigram

190
00:18:55,340 --> 00:18:59,830
occur that is not there in the shakespeares
corpus imagine the probability is zero now

191
00:18:59,830 --> 00:19:04,610
what happens to perplexity value remember
this is simply the multiplication of all the

192
00:19:04,610 --> 00:19:09,870
different probabilities so this one becomes
zero ok so that will not to very very helpful

193
00:19:09,870 --> 00:19:15,730
so you actually would like to give it some
probability so that this does not become zero

194
00:19:15,730 --> 00:19:20,890
ok thats why we will study topic of smoothing
how we can assign different probability values

195
00:19:20,890 --> 00:19:28,970
ok to get get around this problem of zeros
ok so let us take a simple example suppose

196
00:19:28,970 --> 00:19:35,730
i am learning my trigrams and my training
data i have seen the following sentences ok

197
00:19:35,730 --> 00:19:41,090
i have seen these many occurrences denied
the allegations denied the reports denied

198
00:19:41,090 --> 00:19:46,950
the claims denied the request ok and i am
learning a model of finding out the word after

199
00:19:46,950 --> 00:19:52,740
denied the ok
now suppose in my test data i see these two

200
00:19:52,740 --> 00:19:57,601
occurrences denied the offer and denied the
loan so what would happen to the perplexity

201
00:19:57,601 --> 00:20:02,190
of my model ok so in the test data i have
seen these two occurrences that were not there

202
00:20:02,190 --> 00:20:09,570
in the training data so so probability of
offer given denied the will be zero same with

203
00:20:09,570 --> 00:20:15,820
probability of loan given denied the so the
test set will be assigned a probability of

204
00:20:15,820 --> 00:20:21,919
zero and the perplexity cannot be defined
ok and that was thats what we were saying

205
00:20:21,919 --> 00:20:28,679
initially so how can i go around this problem
so that i can compute my perplexity even if

206
00:20:28,679 --> 00:20:34,179
there are certain bigrams or trigrams they
did not occur in my training data so what

207
00:20:34,179 --> 00:20:43,039
is the idea of smoothing so idea is suppose
so this is what we were seeing i am computing

208
00:20:43,039 --> 00:20:48,500
the trigram model for the word w after the
occurrence denied the and suppose in my data

209
00:20:48,500 --> 00:20:58,059
i see four different words right i see allegations
reports claims and request in total seven

210
00:20:58,059 --> 00:21:04,580
words and i can assign the probability to
each of these as three by seven two by seven

211
00:21:04,580 --> 00:21:11,610
one by seven and one by seven ok and that
is what you are seeing in this plot three

212
00:21:11,610 --> 00:21:16,390
by seven two by seven one by seven one by
by seven so these adds up to one now what

213
00:21:16,390 --> 00:21:21,289
about the probabilities for the other other
trigrams like denied the attack denied the

214
00:21:21,289 --> 00:21:27,470
man denied the outcome these are also very
very feasible trigrams so here what will happen

215
00:21:27,470 --> 00:21:31,899
we will assign a probability of zero to all
of these yes so all these have a probability

216
00:21:31,899 --> 00:21:40,010
of zero now what is the idea of smoothing
the idea of a smoothing is can i take some

217
00:21:40,010 --> 00:21:45,110
probability mass from each of these four words
and assign that to the three words or any

218
00:21:45,110 --> 00:21:55,649
other words that i have not seen in my training
data ok so that is

219
00:21:55,649 --> 00:22:00,240
can i steal some probability mass from these
four words to assign some probability mass

220
00:22:00,240 --> 00:22:07,200
to the other words in my co in my data so
that is suppose here i have taken a probability

221
00:22:07,200 --> 00:22:17,040
mass of point five each from the four words
ok so that means i get a probability mass

222
00:22:17,040 --> 00:22:27,649
of two by seven and that mass i distribute
among all the other words in my corpus and

223
00:22:27,649 --> 00:22:31,820
this can be distributed in multiple different
ways ok we will see some solve some possible

224
00:22:31,820 --> 00:22:36,529
ways in which we can distribute this poss
this stolen mass to some oth to the other

225
00:22:36,529 --> 00:22:43,230
words that we were dint see in my training
data ok and how much mass has to be distributed

226
00:22:43,230 --> 00:22:54,260
that also we will see so there are different
methods that that do that ok so the basic

227
00:22:54,260 --> 00:23:03,000
idea is clear so i exactly we do that ok so
a simple method is called laplace smoothing

228
00:23:03,000 --> 00:23:10,120
or add one estimation so what is the idea
so pretend as if you have seen every n gram

229
00:23:10,120 --> 00:23:16,409
one more time that we actually did in my training
data so that is suppose i saw it only once

230
00:23:16,409 --> 00:23:22,010
so i will pretend as if i have seen it twice
so what will happen to the n grams that i

231
00:23:22,010 --> 00:23:28,320
have not seen at all in my training data so
i will pretend as if i have seen them once

232
00:23:28,320 --> 00:23:37,110
ok so this is simple idea so so we will just
add ones to their their actually counts that

233
00:23:37,110 --> 00:23:44,750
we found from the training data we will just
add one to that so remember this maximum likelihood

234
00:23:44,750 --> 00:23:51,159
estimate for this bigram probability of w
i given w i minus one we find it using number

235
00:23:51,159 --> 00:23:56,399
of times if we observe w i minus one followed
by w i divide by the number of times i observe

236
00:23:56,399 --> 00:24:02,070
wi minus one in my training data so this is
my definition of mle so how do i estimate

237
00:24:02,070 --> 00:24:07,929
language model or a bigram model using mle
now how do i use the laplaces smoothing there

238
00:24:07,929 --> 00:24:13,929
so what did we say we will pretend as if we
have seen each n gram one more time than we

239
00:24:13,929 --> 00:24:23,029
actually did so what we will do here so we
will add one to the actual count ok so will

240
00:24:23,029 --> 00:24:29,450
do that for each possible bigram but to ensure
that the probability adds up to one i have

241
00:24:29,450 --> 00:24:34,659
to normalize i have to make some modifications
to my denominator so that so that they are

242
00:24:34,659 --> 00:24:45,950
normalized so what will i add to my denominator
ok so to get this answer you can see how many

243
00:24:45,950 --> 00:24:53,549
different bigrams will be there for which
i will be adding one so how many different

244
00:24:53,549 --> 00:25:00,559
wis will be there that will be number of words
in my vocabulary ok that means i will add

245
00:25:00,559 --> 00:25:09,850
one capital v times so to normalize i will
also have to add a v here in the denominator

246
00:25:09,850 --> 00:25:17,750
ok and that is essentially the idea of my
add one smoothing that i add a one to my actual

247
00:25:17,750 --> 00:25:28,049
n gram count and add a v in my denominator
two to to normalize it ok so this is my add

248
00:25:28,049 --> 00:25:34,460
one estimate number of counts plus one divide
by the count of the unigram wi minus one plus

249
00:25:34,460 --> 00:25:38,980
the vocabulary size so that is the very very
simple smoothing technique that you can use

250
00:25:38,980 --> 00:25:45,730
in general ok so that does not require a lot
of ah fancy estimates you can just get your

251
00:25:45,730 --> 00:25:53,309
language model and easily apply this smoothing
method ok

252
00:25:53,309 --> 00:25:59,399
now we can all when we apply this add one
smoothing method we can also talk about what

253
00:25:59,399 --> 00:26:04,139
is the effective bigram count so let me understand
what do i mean by this effective bigram count

254
00:26:04,139 --> 00:26:11,720
so what i mean is so you see here you have
modified your actual counts or the probability

255
00:26:11,720 --> 00:26:19,880
so what is the idea so now what is in effective
what would have been the count in your act

256
00:26:19,880 --> 00:26:25,490
actual training data such that you have got
the same would have got the same probability

257
00:26:25,490 --> 00:26:40,300
ok so what i am saying so this is your new
probability probability wn given wn minus

258
00:26:40,300 --> 00:26:48,679
one as per the add one smoothing yes so question
here is what would have been my effective

259
00:26:48,679 --> 00:27:03,080
bigram count c star w n minus one w n such
that if i do the mle estimate pmle w n given

260
00:27:03,080 --> 00:27:10,289
w n minus one i will get the same value as
this ok so how do i get the effective bigram

261
00:27:10,289 --> 00:27:17,700
counts so this effective bigram count if i
would divide by counter w n minus one i should

262
00:27:17,700 --> 00:27:24,149
have got this probability so thats means i
can compare c this probability with this probability

263
00:27:24,149 --> 00:27:30,289
and that will give me what is my effective
bigram count ok so thats how i define my effective

264
00:27:30,289 --> 00:27:37,200
bigram count so we will see some example so
if i apply this for my restaurant corpus so

265
00:27:37,200 --> 00:27:46,250
what kind of ef effective bigrams do we observe
ok so we remember this is what this is the

266
00:27:46,250 --> 00:27:52,590
counts that we see in my bigram this my restaurant
corpus ok so we saw that in the last lecture

267
00:27:52,590 --> 00:27:56,860
i wont occurs very high number of time sense
so on so this is the actual bigram that we

268
00:27:56,860 --> 00:28:02,679
see now suppose i apply add one smoothing
so so you cannot apply to this data right

269
00:28:02,679 --> 00:28:09,970
exactly because is not the complete data this
is just a small screenshot ok so now suppose

270
00:28:09,970 --> 00:28:14,190
you use add one smoothing and then if you
find that what is my effective bigram counts

271
00:28:14,190 --> 00:28:18,950
so what kind of effective bigram count do
you see you see in this table certain counts

272
00:28:18,950 --> 00:28:25,090
are zero so one thing you would ah assume
that after applying this smoothing technique

273
00:28:25,090 --> 00:28:29,690
these will not be zero this will be greater
than zero and whatever for having a high value

274
00:28:29,690 --> 00:28:34,179
should have a smaller value because some mass
would be sh esti would be stolen from there

275
00:28:34,179 --> 00:28:38,580
to give it to the words they did not occur
and thats what exactly you will see when we

276
00:28:38,580 --> 00:28:44,379
apply this add one smoothing and then do the
effective bigram counts so here so these are

277
00:28:44,379 --> 00:28:49,559
my effective bigram counts so you see here
whenever the word i want the bigram i want

278
00:28:49,559 --> 00:28:54,169
occur eight hundred twenty seven times initially
the effective bigram count now is only five

279
00:28:54,169 --> 00:28:59,350
twenty seven on the other hand i too did not
occur at all in my training data but now it

280
00:28:59,350 --> 00:29:02,460
has a effective bigram count of point six
four

281
00:29:02,460 --> 00:29:08,000
another observation you can make from here
given the previous word for the next word

282
00:29:08,000 --> 00:29:13,389
if that occur zero times the effective bigram
count remains the same so its point six four

283
00:29:13,389 --> 00:29:18,520
whenever the previous word is i for the words
like to chinese food and lunch but this varies

284
00:29:18,520 --> 00:29:23,720
across different previous words so if you
take the word like want for wants this value

285
00:29:23,720 --> 00:29:27,860
becomes point three time for to it becomes
point six three ok so this depends on the

286
00:29:27,860 --> 00:29:35,669
previous word and that you can also see why
ok because i am i am doing this plus v to

287
00:29:35,669 --> 00:29:40,679
the count of the unigram yes and that will
be different for different words thats why

288
00:29:40,679 --> 00:29:50,379
this value will be also different for different
words [vocalized noise] ok so in general so

289
00:29:50,379 --> 00:29:54,720
so here we talked about the add one estimation
but in general there are some simple variants

290
00:29:54,720 --> 00:29:59,690
of this also ok so one simple variant is called
add k var add k estimation so we are adding

291
00:29:59,690 --> 00:30:05,169
one to each bigram so why why want why cant
we do some general thing like k k can be point

292
00:30:05,169 --> 00:30:12,059
five or more than one depending on how big
your data is ok so this is called add k estimation

293
00:30:12,059 --> 00:30:20,759
so here what is the idea we add k to each
count and accordingly we will ah add kv to

294
00:30:20,759 --> 00:30:26,420
my denominator yes so this is actually the
same as add one ah estimation if i take k

295
00:30:26,420 --> 00:30:31,639
is equal to one
now i can also make some variant here so suppose

296
00:30:31,639 --> 00:30:44,950
i say kv is equal to m ok so now i can write
it as this plus m and this plus m by v yes

297
00:30:44,950 --> 00:30:52,639
this is effective the same now m is k times
v so this is another variant and this also

298
00:30:52,639 --> 00:31:00,500
gives me an idea on how i can improve this
basic smoothing method so here what i am doing

299
00:31:00,500 --> 00:31:09,290
i am adding m by v to each word effectively
i am doing m times one by v to all the v words

300
00:31:09,290 --> 00:31:19,769
in my vocabulary ok and if we add up for all
the words in a vocabulary m by v you get an

301
00:31:19,769 --> 00:31:27,940
m so now can we do something better there
the idea is instead of adding mtimes a uniform

302
00:31:27,940 --> 00:31:38,320
one by v to each word can you add m times
the unigram probability for the word and you

303
00:31:38,320 --> 00:31:42,820
say this will add up to one for all the words
you will effectively end end up guiding the

304
00:31:42,820 --> 00:31:49,750
same values in numerator and denomi denominator
when you normalize the probability but this

305
00:31:49,750 --> 00:31:59,460
might be a better estimate ok why is that
so let us first see all these valuations ok

306
00:31:59,460 --> 00:32:07,120
so this is when i replace kv by m thats what
i get and in place of one so here in place

307
00:32:07,120 --> 00:32:12,950
of one by v i can also replace the probability
of the word ok that will be a different smoothing

308
00:32:12,950 --> 00:32:16,999
but what we are saying this might be a better
way of doing a smoothing this called unigram

309
00:32:16,999 --> 00:32:21,470
prior smoothing so so let us just discuss
this point why this might be a better way

310
00:32:21,470 --> 00:32:27,679
of doing a smoothing ok so remember what we
are trying to do here we are trying to find

311
00:32:27,679 --> 00:32:33,419
a probability or different words that do not
occur in my training data wi given w i minus

312
00:32:33,419 --> 00:32:47,750
one ok take a word w i that is very very common
ok so like i have a word government that is

313
00:32:47,750 --> 00:32:56,009
very very common ok and i might have some
other word like may be something like smoothing

314
00:32:56,009 --> 00:33:05,330
ok that may be other word w one w two ok what
are you doing in your add one smoothing or

315
00:33:05,330 --> 00:33:11,900
add k smoothing you are just studying k or
k or one and dividing by the count of the

316
00:33:11,900 --> 00:33:23,990
previous word plus kv and this will be same
for both the words yes so now so what is the

317
00:33:23,990 --> 00:33:31,180
idea of unigram prior smoothing idea is that
if i know that this word is more common in

318
00:33:31,180 --> 00:33:35,399
my corpus then this word probably i can say
that the probability of this word occurring

319
00:33:35,399 --> 00:33:39,320
of a w i minus one will also be higher than
the probability of this word occurring after

320
00:33:39,320 --> 00:33:44,240
wi minus one and that i am trying to exploit
this data i have from my corpus and i am trying

321
00:33:44,240 --> 00:33:49,820
to exploit that for doing my smoothing ok
so this is called unigram prior smoothing

322
00:33:49,820 --> 00:33:55,330
ok so in general so there are many other ways
of doing smoothing also so we discussed add

323
00:33:55,330 --> 00:33:59,740
one smoothing and the institution behind unigram
prior smoothing but there are other advance

324
00:33:59,740 --> 00:34:05,580
models of smoothing also that we will see
in the next lecture so in week three ok

