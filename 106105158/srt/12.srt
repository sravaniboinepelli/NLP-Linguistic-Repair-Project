1
00:00:17,520 --> 00:00:23,950
ok so welcome back for the third week of this
course so in the last week where we ended

2
00:00:23,950 --> 00:00:28,490
you are discussing about language modeling
we talked about the basis of language modeling

3
00:00:28,490 --> 00:00:33,800
then we talked about what ah how to be evaluate
a language model and then we came to the topic

4
00:00:33,800 --> 00:00:39,230
of smoothing ok so what was the simple idea
we discussed what is my advanced smoothing

5
00:00:39,230 --> 00:00:48,710
where i add one to each individual count and
accordingly i do some normalization my denominate

6
00:00:48,710 --> 00:00:54,440
and we added by saying there are certain simple
variance possible like at k i can also do

7
00:00:54,440 --> 00:01:01,050
the unigram prior smoothing by instead of
adding uniform weight to each word i at a

8
00:01:01,050 --> 00:01:04,430
weight that is depending on the probability
of that word unigram probability for that

9
00:01:04,430 --> 00:01:10,530
word and that we were arguing that might work
better than simply adding uniform count to

10
00:01:10,530 --> 00:01:16,560
each of the bigrams
there are we had certain parameters like what

11
00:01:16,560 --> 00:01:22,539
what will be a best way of choosing k a and
m in different models ok so simple answer

12
00:01:22,539 --> 00:01:28,970
would be so you ah there is no unique value
that you can choose suppose you have some

13
00:01:28,970 --> 00:01:35,110
held out data so we can try to find out for
which value of k and m you are getting the

14
00:01:35,110 --> 00:01:39,120
best publicity for that held out data and
that can be one possible way of choosing the

15
00:01:39,120 --> 00:01:44,300
values of k and m ok
so now today we will talk we will go beyond

16
00:01:44,300 --> 00:01:53,580
the advanced smoothing and talk about some
other advanced a smoothing methods ok so we

17
00:01:53,580 --> 00:01:58,690
will start with two different a smoothing
methods called good turing smoothing and kneser

18
00:01:58,690 --> 00:02:05,220
ney smoothing so so what is the intuition
behind using a good turing smoothing so intuition

19
00:02:05,220 --> 00:02:12,890
is essentially i want to find out what should
be the probability for the words that i have

20
00:02:12,890 --> 00:02:19,640
not seen in my training data so what give
turing is smoothing says ok so why don't you

21
00:02:19,640 --> 00:02:24,370
use the estimate of the things you have seen
once in your training data you estimate about

22
00:02:24,370 --> 00:02:29,890
the things we have not seen similarly whatever
you have seen twice use that estimate that

23
00:02:29,890 --> 00:02:36,530
things you have you will seen once and like
that ok so this this this just tries to adjust

24
00:02:36,530 --> 00:02:41,950
the probability mass so that the things that
was seen twice are used to find the probability

25
00:02:41,950 --> 00:02:46,761
for the things that was seen once things that
was seen once are used to estimate the probability

26
00:02:46,761 --> 00:02:53,980
for the things that will not seen at all ok
so so the idea is that you we have to use

27
00:02:53,980 --> 00:03:00,830
the count of things we have seen once to find
the count of things we have never seen ok

28
00:03:00,830 --> 00:03:08,070
so for that let us give ah let us first define
what is the frequency of frequency ok we used

29
00:03:08,070 --> 00:03:14,380
this one of the earlier lectures also so we
take this three sentences ok i am here who

30
00:03:14,380 --> 00:03:21,770
am i i would like from this sentences i am
trying to construct the frequency of frequency

31
00:03:21,770 --> 00:03:27,760
so that is how many words occurs once how
many word occurs twice and so on

32
00:03:27,760 --> 00:03:34,561
so so if you see here the word i occurs tries
m occurs twice and the other four words occurs

33
00:03:34,561 --> 00:03:41,350
once so what will my frequency of frequency
i will find out how many of words occurs once

34
00:03:41,350 --> 00:03:45,520
with frequency one so that is my n one so
there are four words here that occur only

35
00:03:45,520 --> 00:03:51,450
once so n is four now you will see how many
words occur twice so how many words have a

36
00:03:51,450 --> 00:03:56,890
frequency of two so that will i will have
one here n three is one so this is my frequency

37
00:03:56,890 --> 00:04:04,830
of frequency ok so four words occur once one
word occurs twice one word occurs tries

38
00:04:04,830 --> 00:04:12,020
now how by use that in good turing estimate
so what is the idea so this we discussed earlier

39
00:04:12,020 --> 00:04:17,510
so i want to reallocate the probability mass
of n grams that occur r plus one times in

40
00:04:17,510 --> 00:04:23,670
the training data to the n grams that occur
r times ok especially you want to see the

41
00:04:23,670 --> 00:04:29,259
n grams that occur one once to find the probability
mass for the n grams that occur zero times

42
00:04:29,259 --> 00:04:35,409
but you that you do that in general ok so
the n grams that occur r plus one times used

43
00:04:35,409 --> 00:04:42,631
that for r times ok
so now in particular reallocate the probability

44
00:04:42,631 --> 00:04:49,520
mass for n grams that were seen once for those
that were never seen so now we should see

45
00:04:49,520 --> 00:04:56,710
formally what will be the effective probability
mass n count that we will get by this good

46
00:04:56,710 --> 00:05:04,270
turing estimate ok so let us see suppose i
have a i want to find out four words that

47
00:05:04,270 --> 00:05:12,680
occurred c times words that occurs c times
what will be the effective adjusted count

48
00:05:12,680 --> 00:05:18,970
after applying good turing estimation ok
so now how we defined that so what would happened

49
00:05:18,970 --> 00:05:26,529
by training data i will have words that occur
zero times words that occur one time let it

50
00:05:26,529 --> 00:05:35,580
be n zero n one words that occur twice n two
and so on so n two numbers that occur twice

51
00:05:35,580 --> 00:05:44,370
similarly now there are n c number of words
that occurs c times n c plus one words that

52
00:05:44,370 --> 00:05:53,830
occurs c plus one times ok
now i want to find i want to use good turing

53
00:05:53,830 --> 00:06:02,069
to estimate the the count for these words
that occur c times so what it be say we said

54
00:06:02,069 --> 00:06:09,080
we will use this probability mass and give
it to these words now how to we distribute

55
00:06:09,080 --> 00:06:14,650
so what is the probability mass associated
with the words that occurs c plus one times

56
00:06:14,650 --> 00:06:25,319
ok
so now suppose there are n words or nth bigrams

57
00:06:25,319 --> 00:06:36,110
in total ok so now what is the probability
mass for the bigrams that occurred or n gram

58
00:06:36,110 --> 00:06:41,449
we can say in general that occurred c plus
one times you see how many n grams are there

59
00:06:41,449 --> 00:06:45,210
there are n c plus one different n grams so
what will be the probability mass for them

60
00:06:45,210 --> 00:06:59,210
it will be 
n c plus one times c plus one divided by the

61
00:06:59,210 --> 00:07:09,020
total number of n grams that are n that is
a probability mass yes and this i am distributing

62
00:07:09,020 --> 00:07:17,770
to the words that occur c times now how many
words are there there are n c words so what

63
00:07:17,770 --> 00:07:25,690
is the probability that each of them will
get divided by n c yes so now so this is the

64
00:07:25,690 --> 00:07:30,690
probability mass for that each of them will
get now what will be the effective count remember

65
00:07:30,690 --> 00:07:38,479
how we define the effective count c star such
that if i if i divided by n merely this will

66
00:07:38,479 --> 00:07:44,280
give me the same probability so that will
tell me c star is n c plus one times c plus

67
00:07:44,280 --> 00:07:49,419
one divided by n c and that's what we have
written here ok so n c in general is the n

68
00:07:49,419 --> 00:07:56,599
grams that ah seen exactly c terms in the
corpus so by using this definition i can find

69
00:07:56,599 --> 00:08:03,569
out what is effective count for any word that
is occur n c number of times in the corpus

70
00:08:03,569 --> 00:08:13,809
ok
so that's what we have seen so this this is

71
00:08:13,809 --> 00:08:19,910
the probability for the words things let us
in c times c star n yes c star is my nothing

72
00:08:19,910 --> 00:08:26,240
but my effective n gram count ok now what
about the words that will not seen at all

73
00:08:26,240 --> 00:08:30,340
that are the words that that or whenever i
am saying words here it means the n terms

74
00:08:30,340 --> 00:08:36,029
in general so what about the n grams that
did not occur at all matter in data

75
00:08:36,029 --> 00:08:42,640
so frommy definition i will use the probability
mass of the words that was seen once ok so

76
00:08:42,640 --> 00:08:49,240
what will be the probability mass so there
are by definition there are n one words that

77
00:08:49,240 --> 00:08:57,050
was seen once so each one of them occurs once
so what is the probability mass it is n one

78
00:08:57,050 --> 00:09:09,480
by n yes and this probability mass i will
give to the words that was seen zero times

79
00:09:09,480 --> 00:09:15,310
yes
now you might have a question how much of

80
00:09:15,310 --> 00:09:19,620
this each of them individual we will get so
this is the probability mass that are the

81
00:09:19,620 --> 00:09:24,520
word the n grams are did not occurs in the
corpus to get that we will get but how much

82
00:09:24,520 --> 00:09:29,100
of this individually each of them will get
so if you can just divided by the number of

83
00:09:29,100 --> 00:09:34,900
n grams that did not occur in the corpus so
we will see some by some example how we can

84
00:09:34,900 --> 00:09:47,440
be estimate this value ok
so here if my count is zero so good turing

85
00:09:47,440 --> 00:09:53,150
estimate for the things that for the frequency
c when c zero is n one by n that we just saw

86
00:09:53,150 --> 00:09:57,030
ok so that is a effective probability mass
that i will give to the all the words or n

87
00:09:57,030 --> 00:10:03,240
grams they did not occur in the training data
ok so this gives me nice methods of estimating

88
00:10:03,240 --> 00:10:09,330
the probability for the words are did not
occur in my training data

89
00:10:09,330 --> 00:10:16,410
now there are certain complications that you
might have when you applying this good turing

90
00:10:16,410 --> 00:10:23,000
estimate so for a small c generally you will
see that n c greater than n c plus one that

91
00:10:23,000 --> 00:10:27,430
is a number of words having a frequency c
is greater than number of words having a frequency

92
00:10:27,430 --> 00:10:33,530
c plus one so remember what is what would
be empiric empirical law that shares that

93
00:10:33,530 --> 00:10:36,910
so that is a japes law that's that is shared
that there is a universal relationship so

94
00:10:36,910 --> 00:10:43,590
this is generally good it but it might happen
when we go to very very high frequencies certain

95
00:10:43,590 --> 00:10:50,410
ah certain certain frequency are not observing
that so this is possible ok

96
00:10:50,410 --> 00:10:56,400
so what do we do in general so when we go
to higher and higher frequencies for larger

97
00:10:56,400 --> 00:11:02,342
ks or larger cs here we can replace it with
some sort of best fit power law and we will

98
00:11:02,342 --> 00:11:12,330
do some normalizations ok so instead of doing
it for each individual k i might actually

99
00:11:12,330 --> 00:11:20,130
just fit so the so the i have n one n two
and so on and when i go to the higher values

100
00:11:20,130 --> 00:11:25,540
i simply sit fit some sort of power law and
that is so that it is normalized and i will

101
00:11:25,540 --> 00:11:29,710
obtain the same probability mass that is one
ok

102
00:11:29,710 --> 00:11:35,390
so this is power law can can be fit it in
general so in in various toolkits that you

103
00:11:35,390 --> 00:11:41,630
might very you might use this methods they
they do it for only few values of k summation

104
00:11:41,630 --> 00:11:46,630
few values of k it maybe k is equal to five
or k is equal to ten ok in then the the formula

105
00:11:46,630 --> 00:11:52,570
that we were using might have to be real adjusted
ok but the basic idea was is same what we

106
00:11:52,570 --> 00:12:06,540
have seen here ok
so now suppose you apply this good turing

107
00:12:06,540 --> 00:12:12,790
smoothing methods for a corpus like a p neswire
academic academic press news there are twenty

108
00:12:12,790 --> 00:12:19,340
two million words and so that's why you are
seeing for different counts what is the good

109
00:12:19,340 --> 00:12:25,540
turing estimate that was seen this is for
bigrams ok so what yours observing in the

110
00:12:25,540 --> 00:12:30,640
table so for the bigrams that occurred zero
number of times the good turing estimate was

111
00:12:30,640 --> 00:12:36,860
point zero zero zero zero two seven zero ok
for once point four four six for twice point

112
00:12:36,860 --> 00:12:42,870
one two six now so how do i actually get this
number point zero zero zero zero two seven

113
00:12:42,870 --> 00:12:51,120
zero so we can just do that quickly
so remember for the words that occur zero

114
00:12:51,120 --> 00:12:57,590
times what is the total probability that we
are giving them getting the probability of

115
00:12:57,590 --> 00:13:05,520
n one divided by n ok now what is the probability
given to the individual word so you multiplied

116
00:13:05,520 --> 00:13:22,130
by n zero and n zero is the number of bigrams
that occurring zero times ok what would be

117
00:13:22,130 --> 00:13:27,830
the effective count effective count we will
remove this n yes so it will be n one divided

118
00:13:27,830 --> 00:13:34,730
by n zero now what is n one so you need to
find out how many bigrams occurred once yes

119
00:13:34,730 --> 00:13:39,800
that you can find from training data how do
you find n zero that is how many bigrams did

120
00:13:39,800 --> 00:13:46,920
not occurred in my data now if you remember
from shakespeare corpus so we ah how do we

121
00:13:46,920 --> 00:13:53,080
find out how many bigrams did not occur at
all so we need to see what is my vocabulary

122
00:13:53,080 --> 00:13:55,840
size what are different sym ah different unigrams
ok

123
00:13:55,840 --> 00:14:08,130
suppose in my vocabulary size of v that is
my number of unigrams so how many bigrams

124
00:14:08,130 --> 00:14:14,741
are possible v square bigrams are possible
so how many actually occurred suppose i know

125
00:14:14,741 --> 00:14:24,990
that out of this some n b bigrams occurred
in my corpus so v square minus nb gives me

126
00:14:24,990 --> 00:14:31,650
the bigrams that did not occur at all in my
corpus so this will become my n zero number

127
00:14:31,650 --> 00:14:38,320
of bigrams that did not occurred in my corpus
so this value i obtain by finding out n one

128
00:14:38,320 --> 00:14:43,240
number of bigrams that occur once divided
by v square minus n v and these are number

129
00:14:43,240 --> 00:14:54,630
of bigrams in total so that is the unique
bigrams that occur in my corpus ok

130
00:14:54,630 --> 00:14:58,410
now what is the other objection that you might
have from this table while looking at this

131
00:14:58,410 --> 00:15:04,980
table so for zero and one this is different
but once you start seeing from two on words

132
00:15:04,980 --> 00:15:09,010
what is the pattern that you seen you see
that most of the effective counts are the

133
00:15:09,010 --> 00:15:14,340
original count minus some value like point
seven five ok so you see all of these a roughly

134
00:15:14,340 --> 00:15:20,290
original value minus point two seven to seven
five so now one might are give why don't i

135
00:15:20,290 --> 00:15:28,800
just you see minus point zero five seven point
zero seven five ok in maybe give a different

136
00:15:28,800 --> 00:15:33,790
value to the initial two components that is
also done in practice ok so this is called

137
00:15:33,790 --> 00:15:41,070
the absolute discounting that should you will
see next

138
00:15:41,070 --> 00:15:45,290
so the idea is that why don't i substract
something like point seven five or some d

139
00:15:45,290 --> 00:15:52,790
from each bigram that occur more than say
more than once or or whatever ok and i can

140
00:15:52,790 --> 00:15:57,430
have one or two different values of d for
some initial bigrams and this is called the

141
00:15:57,430 --> 00:16:03,570
absolute discounting methods and you you can
probably interpolated with my unigram probability

142
00:16:03,570 --> 00:16:08,170
that we saw in the last lecture to give it
give a weight estimate yes this is called

143
00:16:08,170 --> 00:16:13,910
linear inter ah interpolation with the unigram
probability of the absolute discounting method

144
00:16:13,910 --> 00:16:19,230
and that is the formula that we have written
here so this is i am doing some sort of discounting

145
00:16:19,230 --> 00:16:25,740
from each bigram count yes
plus i am interpolating it with my unigram

146
00:16:25,740 --> 00:16:32,190
probability with weight some lambda w i minus
one you do we need a weight here so that when

147
00:16:32,190 --> 00:16:44,180
i saw this probability probability absolute
discounting [noises] w i given w i minus one

148
00:16:44,180 --> 00:16:49,760
for each individual w i this will add up to
one that's why i need some sort of normalization

149
00:16:49,760 --> 00:16:59,960
constant that will make sure that this will
add up to one 

150
00:16:59,960 --> 00:17:02,170
ok
so we can keep probably some values of d d

151
00:17:02,170 --> 00:17:08,069
for count one and two and otherwise i can
use this particular interpolation methods

152
00:17:08,069 --> 00:17:15,490
now that's why the idea of our other advanced
smoothing model comes that was our kneser

153
00:17:15,490 --> 00:17:22,630
ney smoothing man methods so what is that
ok so that is can we do better than using

154
00:17:22,630 --> 00:17:29,590
the regular unigram correct ok so so see so
when before we first started with a simple

155
00:17:29,590 --> 00:17:36,870
ah advanced smoothing we were giving uniform
weight to each ah each particular n gram uniform

156
00:17:36,870 --> 00:17:41,430
addition then from they moved on to unigram
prior we said ok why do we give uniformly

157
00:17:41,430 --> 00:17:45,559
we we give a higher weight to the words that
occur more often in my corpus

158
00:17:45,559 --> 00:17:49,570
now we have seen can be do better than even
this and that's why we will see the kneser

159
00:17:49,570 --> 00:17:56,570
ney smoothing idea so what is the idea so
know again let us go to the shannon game ok

160
00:17:56,570 --> 00:18:04,290
suppose at this sentence i can't see without
my reading and have to filling this blank

161
00:18:04,290 --> 00:18:10,260
and suppose possibilities are glasses and
francisco and all of us will say ok i should

162
00:18:10,260 --> 00:18:20,030
fill in glasses now take a scenario where
reading glasses doesn't occur in my training

163
00:18:20,030 --> 00:18:24,660
data and reading francisco also doesn't occur
in my training data

164
00:18:24,660 --> 00:18:31,380
now if i use the previous model what will
i do the probability of filling in glasses

165
00:18:31,380 --> 00:18:39,860
versus francisco depend on their unigram probabilities
and suppose this is my data from some yours

166
00:18:39,860 --> 00:18:51,760
corpus so i will find probability of francisco
is higher than probability of glasses ok that

167
00:18:51,760 --> 00:18:57,309
can very well happened and that will that
that means i will add up filling it francisco

168
00:18:57,309 --> 00:19:02,020
here there is not the correct word correct
choice here ok so can we do something better

169
00:19:02,020 --> 00:19:10,480
than simply using their unigram probabilities
that is the idea of kneser ney smoothing so

170
00:19:10,480 --> 00:19:19,320
francisco is more common than glasses in my
data but can i use this idea that in my data

171
00:19:19,320 --> 00:19:25,720
whenever see francisco it occurs only after
san in in this occurrence san francisco

172
00:19:25,720 --> 00:19:30,750
on the other hand whenever i see glasses it
occurs some ah after mul multiple different

173
00:19:30,750 --> 00:19:41,060
words ok so this is the intuition that is
i will see given a word what are the other

174
00:19:41,060 --> 00:19:49,940
words it comes with ok so here if i take word
is francisco it occurs maybe with only one

175
00:19:49,940 --> 00:19:59,900
word yes only one word san but glasses occurs
with many other words right it can occur with

176
00:19:59,900 --> 00:20:04,810
different different words and this so so both
glasses are occurs occurs with ten different

177
00:20:04,810 --> 00:20:12,370
words and francisco occurs with only one different
word so what is the intuition if a word occurs

178
00:20:12,370 --> 00:20:17,160
with many other words that means it is more
likely to complete this particular bigram

179
00:20:17,160 --> 00:20:21,780
that we are seeing right now
so reading after reading a word is more likely

180
00:20:21,780 --> 00:20:26,960
to occur if it has already completed many
other bigram so if it is more commonly used

181
00:20:26,960 --> 00:20:31,640
bigrams with many many other words this will
not happen with francisco because francisco

182
00:20:31,640 --> 00:20:36,870
is used only after san so given a new context
probably san francis francisco is not good

183
00:20:36,870 --> 00:20:43,340
choice but glasses are occur many many other
different words so after read ah completely

184
00:20:43,340 --> 00:20:50,850
new context glasses is a better choice than
francisco so formally how this method used

185
00:20:50,850 --> 00:20:55,610
this intuition
so in instead of using simple probability

186
00:20:55,610 --> 00:21:01,290
of this word unigram probability each probability
continuation word so that is how likely is

187
00:21:01,290 --> 00:21:06,559
this word to appear as a novel continuation
what do you mean by novel continuation how

188
00:21:06,559 --> 00:21:16,520
likely it is to complete a bigram or in n
gram so how do i find out the probability

189
00:21:16,520 --> 00:21:21,200
so for each word to find this probability
as we said we have to count the number of

190
00:21:21,200 --> 00:21:27,460
bigram types it completes so how many bigrams
where it occurs as the last one the final

191
00:21:27,460 --> 00:21:33,110
word ok
now for each bigram whenever it occur the

192
00:21:33,110 --> 00:21:38,900
first time it was like a novel continuation
so that's what we mean by novel continuation

193
00:21:38,900 --> 00:21:45,260
probability here that how many bigrams it
is complete so this will be simply it will

194
00:21:45,260 --> 00:21:50,430
be proportional to the number of bigrams it
is completing so here how many w i minus one

195
00:21:50,430 --> 00:21:58,440
are there such that w occurs after that in
my corpus so that is the property of continuation

196
00:21:58,440 --> 00:22:05,020
ok it's a same thing how what is the probability
of continuation that should depend on the

197
00:22:05,020 --> 00:22:11,929
number of bigrams types it completes now how
should i normalized it i want to find this

198
00:22:11,929 --> 00:22:17,040
probability for each word in the corpus yes
so this is proportion to the number of bigrams

199
00:22:17,040 --> 00:22:24,760
types it completes so i should find out in
total how many bigram types are there that

200
00:22:24,760 --> 00:22:30,300
each word is completing so that is effective
their number of bigram types in total not

201
00:22:30,300 --> 00:22:35,160
totals the number of bigram types how many
different bigram types are there ok so here

202
00:22:35,160 --> 00:22:39,850
also we are only taking the types how many
different bigram types we are completing

203
00:22:39,850 --> 00:22:51,850
so i will normalized it by using all possible
bigrams so here there says all possible bigrams

204
00:22:51,850 --> 00:22:56,080
because i am saying the count is greater than
zero in my training data how many bigram types

205
00:22:56,080 --> 00:23:03,660
i have seen so this will give me the probability
continuation the word the w ok so once i have

206
00:23:03,660 --> 00:23:09,800
found this probability i can even go back
to my unigram prior based absolute discounting

207
00:23:09,800 --> 00:23:17,100
method and replace p w i by p continuation
w i ok and that is my kneser ney smoothing

208
00:23:17,100 --> 00:23:22,160
model so instead of using the unigram priors
for smoothing i used this kneser ney smoothing

209
00:23:22,160 --> 00:23:35,940
by using the continuation probability ok
so if i do that a frequent word like francisco

210
00:23:35,940 --> 00:23:43,260
occurring only in one context of san will
have a low continuation probability so this

211
00:23:43,260 --> 00:23:49,730
will give an model with kneser nay smoothing
you see so all these are there is all same

212
00:23:49,730 --> 00:23:57,740
this is same now instead of p w i we are put
p continuation w i that we get from the previous

213
00:23:57,740 --> 00:24:02,060
ah slide whatever map whatever ah formula
we wrote in the previous slide that will give

214
00:24:02,060 --> 00:24:07,720
me this continuation p w i ok
so now this one question here that we discus

215
00:24:07,720 --> 00:24:14,210
even the in the earlier example how do we
find lambda w i minus one so what was the

216
00:24:14,210 --> 00:24:20,549
intuition i gave the hint was that lambda
w i minus one should be such that summation

217
00:24:20,549 --> 00:24:31,320
over k n smoothing of w i given w i minus
one for all w i in my vocabulary should be

218
00:24:31,320 --> 00:24:38,390
one so now again take it as a simple exercise
that suppose i want to find out what will

219
00:24:38,390 --> 00:24:45,890
be the value of lambda w i minus one for this
particular formula ok and you can assume that

220
00:24:45,890 --> 00:24:52,130
d less between zero to one ok for this particular
case what is the value of lambda lambda w

221
00:24:52,130 --> 00:24:59,770
i minus one that i will get ok so answer is
also on this slide so you will get lambda

222
00:24:59,770 --> 00:25:05,390
w i minus one of this so try to find out if
i have to cons satisfy the constant of the

223
00:25:05,390 --> 00:25:09,200
summation of the probabilities for all the
words should add up to one what is the value

224
00:25:09,200 --> 00:25:21,020
of lambda that i add up with ok in that will
give you this value particular value ok

225
00:25:21,020 --> 00:25:27,060
now so other than these two models of good
turing and kneser ney smoothing there are

226
00:25:27,060 --> 00:25:32,750
other other smoothing models at you might
be able to use ok so one simple idea instead

227
00:25:32,750 --> 00:25:39,890
of using a new smoothing model why don't we
combine various models so remember we always

228
00:25:39,890 --> 00:25:44,481
doing that somewhere so you computing bigram
i was trying to use the probability of the

229
00:25:44,481 --> 00:25:50,190
unigram for unigram priors smoothing now can
i generalized this idea suppose i am finding

230
00:25:50,190 --> 00:25:55,440
out the trigram probability trigram language
model now from my data i can compute the trigram

231
00:25:55,440 --> 00:26:02,160
language model probabilities bigram and uni
unigram using the m l e can i try to effectively

232
00:26:02,160 --> 00:26:07,790
combined these different probability models
to find my trigrams language model ok

233
00:26:07,790 --> 00:26:11,299
so there are actually two very very popular
ways in which this can be done one is called

234
00:26:11,299 --> 00:26:16,450
interpretation based methods another is called
back of language models so you will just see

235
00:26:16,450 --> 00:26:25,390
ah some simple intuition how this can be combined
together so so what we have seen as i increased

236
00:26:25,390 --> 00:26:31,710
my n power of my n gram model increases we
also see saw that in my when i saw the perplexity

237
00:26:31,710 --> 00:26:37,700
values when i take a larger n gram i get lower
perplexity i get better perplexity that means

238
00:26:37,700 --> 00:26:43,290
so that means the the the model becomes much
better when i take a larger n but what is

239
00:26:43,290 --> 00:26:48,210
the problem that happens if i take a larger
n remember why we did all this is smoothing

240
00:26:48,210 --> 00:26:54,700
because when i am taking larger n data becomes
more and more is fast so many of the actual

241
00:26:54,700 --> 00:26:57,750
n grams become zero the probability become
zero

242
00:26:57,750 --> 00:27:06,679
so at one side higher n gram is better a trigram
model is better other side is very very fast

243
00:27:06,679 --> 00:27:14,100
so why cant i take advantage of the both both
things that is i take a higher order n gram

244
00:27:14,100 --> 00:27:19,280
model whenever i have the data but whenever
i do not have the data i try to use the low

245
00:27:19,280 --> 00:27:24,620
n gram model so i do that togetherness single
model so that is the basic idea of these model

246
00:27:24,620 --> 00:27:33,750
that we will see
so now general approach is can be combined

247
00:27:33,750 --> 00:27:39,669
multiple n gram models to get a single model
that can had that can also take the advantage

248
00:27:39,669 --> 00:27:46,799
of larger n gram model but avoid this fastness
problem ok so that is some cases might have

249
00:27:46,799 --> 00:27:53,549
to use less context whenever you do not have
information about larger data larger context

250
00:27:53,549 --> 00:28:01,020
so in back off what you will do suppose i
am computing a a trigram language model so

251
00:28:01,020 --> 00:28:09,270
w i given w i minus one w i minus two whenever
this trigram occurs in my training data i

252
00:28:09,270 --> 00:28:13,900
will use the probability as for my m l e but
if i does not if it does not occur in my training

253
00:28:13,900 --> 00:28:20,929
data i will back off to my bigram language
model so that means whenever the count of

254
00:28:20,929 --> 00:28:25,460
w i minus one w i is greater than zero i will
go to bi bigram suppose that is also zero

255
00:28:25,460 --> 00:28:31,570
when i go to unigram this is idea of back
off language model so if you have good evidence

256
00:28:31,570 --> 00:28:37,429
you go to trigram otherwise you go to bigram
otherwise you go to unigram ok

257
00:28:37,429 --> 00:28:42,130
on the other hand interpolation you compute
all three and just interpolate than together

258
00:28:42,130 --> 00:28:50,559
you try to mix these together ok so so let
us see what do will do in back off language

259
00:28:50,559 --> 00:28:57,910
models so idea is that i am computing a trigram
model w i given w i minus two ah i minus one

260
00:28:57,910 --> 00:29:10,990
and so here this is the previous word and
this is previous to previous word ok so if

261
00:29:10,990 --> 00:29:20,090
i do not have counts for w i minus two w i
minus one w i suppose this is zero then i

262
00:29:20,090 --> 00:29:26,920
use probability w i given w i minus one
now suppose count of w i minus one w i is

263
00:29:26,920 --> 00:29:40,060
also zero then you go to probability w i ok
now seen but you might add up having some

264
00:29:40,060 --> 00:29:46,200
problems with how do i normalized the final
probability distribution that i will get ok

265
00:29:46,200 --> 00:29:53,380
so so this is the particular formula formula
of back off that you can use so back off for

266
00:29:53,380 --> 00:30:04,280
this trigram model w i given w i minus two
w i minus one is p hat w i given w i minus

267
00:30:04,280 --> 00:30:07,330
two w i minus one if the count is greater
than zero

268
00:30:07,330 --> 00:30:13,960
now what is p hat this is actually similar
to my maximum likelihood estimate that you

269
00:30:13,960 --> 00:30:19,880
will get but it has been something has been
subtracted from there so some probability

270
00:30:19,880 --> 00:30:25,570
mass has been shown from there so there it
when it can be given to the other counts why

271
00:30:25,570 --> 00:30:31,510
we ne need that suppose you are computing
bigram this ah trigram back off probability

272
00:30:31,510 --> 00:30:34,250
model
so in general you are doing it after w i minus

273
00:30:34,250 --> 00:30:42,679
two w i minus one you are finding for various
words w one w two w three ok suppose this

274
00:30:42,679 --> 00:30:48,640
occurs three times this occurs two times this
occurs zero times so now once use the m l

275
00:30:48,640 --> 00:30:53,440
e method ah estimate immediately this will
have a probability of three by five this i

276
00:30:53,440 --> 00:30:57,030
have two probability two of two by five suppose
there is no other bigram and this will have

277
00:30:57,030 --> 00:31:04,160
a probability of zero and now you want to
use back off to go to two five use probability

278
00:31:04,160 --> 00:31:11,530
w three given w i minus one but here itself
probability mass adding up to one so how can

279
00:31:11,530 --> 00:31:18,430
you use the bigram probability so far that
you might have to reduce some mass from these

280
00:31:18,430 --> 00:31:23,870
two values so that's why we are writing p
hat and there are different ways you can reduce

281
00:31:23,870 --> 00:31:29,260
so we will take someone simple example we
are a doing it by some ah simple constant

282
00:31:29,260 --> 00:31:33,240
you are re ah reducing some simple constants
but this this can be proportional also you

283
00:31:33,240 --> 00:31:38,860
can multiply some ok
so will you will take that is my p hat it's

284
00:31:38,860 --> 00:31:44,539
a reduced probability value from my m l e
constant ok so now if the count is greater

285
00:31:44,539 --> 00:31:52,710
than zero i take p hat if the count is equal
to zero then i have to use the previous i

286
00:31:52,710 --> 00:31:59,049
have to back off to the bigram model so i
have to use probability w i given w i minus

287
00:31:59,049 --> 00:32:07,960
one and that's where i back off to this model
but what is interest thing that you have seeing

288
00:32:07,960 --> 00:32:13,410
here so this is a recursive definition i am
using the back off probability of w i given

289
00:32:13,410 --> 00:32:20,731
w i minus one and i multiplying with some
constant again to ensure that the probability

290
00:32:20,731 --> 00:32:26,090
mass adds up to one ok
so now so the recursive definition so how

291
00:32:26,090 --> 00:32:33,070
do we define probability w w i given w i minus
one again it's p hat w i given w i minus one

292
00:32:33,070 --> 00:32:42,800
if the count is greater than zero and p hat
w n if the count is equal to zero ok so i

293
00:32:42,800 --> 00:32:47,210
think it will help a if you take a simple
example and try to see how do we actually

294
00:32:47,210 --> 00:32:54,570
use this definition to compute the back off
probabilities ok

295
00:32:54,570 --> 00:33:00,910
so let's take an example ok so here we are
taking an corpus where we are having only

296
00:33:00,910 --> 00:33:07,490
four words a b c and d and this is one data
that is provided so what is this is data so

297
00:33:07,490 --> 00:33:18,820
this is it is telling how many times a occurs
after a b it occurs four times on the other

298
00:33:18,820 --> 00:33:26,000
hand b occurs after a b zero times c occurs
after a b zero times ok similarly a occurs

299
00:33:26,000 --> 00:33:32,370
after b five times b b occur after b three
times c occur after b zero times and also

300
00:33:32,370 --> 00:33:37,559
the individual word words a occurs eight times
b occurs nine times c occurs eight times d

301
00:33:37,559 --> 00:33:43,751
occurs seven times now given this data you
ask to compute the prob back off probability

302
00:33:43,751 --> 00:33:49,930
model of w n given w n minus one where the
previous word is b and previous to previous

303
00:33:49,930 --> 00:33:53,890
word is a ok
and you have also told that you can use this

304
00:33:53,890 --> 00:34:00,350
simple definition for p hat that is p x minus
one by eight suppose i want to use that to

305
00:34:00,350 --> 00:34:09,060
find out the back off probability distribution
ok now so that is nothing but probability

306
00:34:09,060 --> 00:34:20,490
back off of word given a and b a is pervious
to previous word and b is the previous word

307
00:34:20,490 --> 00:34:34,879
ok so let us take to do that so i want to
find out the back off probability w given

308
00:34:34,879 --> 00:34:46,480
a b so this would be w can take a b c and
d so right now from my table what are the

309
00:34:46,480 --> 00:34:52,619
counts i am seeing e occurs so this occurs
four times this occurs zero times this occurs

310
00:34:52,619 --> 00:34:58,119
zero times this occurs zero times ok so assumed
by definition of back off probability i can

311
00:34:58,119 --> 00:35:11,710
say that this probability will be p hat a
given a b because the count is greater than

312
00:35:11,710 --> 00:35:23,550
the zero but this will be lambda times previous
two words a b so this lambda a b a constant

313
00:35:23,550 --> 00:35:33,501
times back off probability of so i will go
to the previous word back off probability

314
00:35:33,501 --> 00:35:41,750
of b given b this will be lambda a b times
back off back off probability of c given the

315
00:35:41,750 --> 00:35:49,020
previous word b and this will be lambda a
b back off probability of d given b

316
00:35:49,020 --> 00:35:54,570
so what it say is that now we have to compute
the back off probability of b given b c given

317
00:35:54,570 --> 00:36:01,260
b and d given b ok so again we use the definition
so now i am computing back off probability

318
00:36:01,260 --> 00:36:13,820
of w given b and w is equal to a b c and d
now i see a occurs five times b occurs three

319
00:36:13,820 --> 00:36:22,869
times c occurs zero times d occurs zero times
so i show my definition this will be p hat

320
00:36:22,869 --> 00:36:34,410
a given b what will be that that will be five
by eight yes minus one by eight that is four

321
00:36:34,410 --> 00:36:42,970
by eight what will be this p hat b given b
three by eight minus one by eight that is

322
00:36:42,970 --> 00:36:47,619
two by eight now what will be this this is
this occurs zero number of times so this will

323
00:36:47,619 --> 00:37:01,420
be lambda of b times unigram probability of
c maybe p hat c ok

324
00:37:01,420 --> 00:37:11,030
now what is this we can see from my data what
is p hat c so c occurs a times yes and total

325
00:37:11,030 --> 00:37:18,420
number of so what is the total of my ah unigram
count if you will add up to thirty two ok

326
00:37:18,420 --> 00:37:30,260
so this so p c the mle varies so it's lambda
b times p c minus one by eight p c is eight

327
00:37:30,260 --> 00:37:42,780
by thirty two yes so this will become eight
by thirty two one by four minus one by eight

328
00:37:42,780 --> 00:37:56,420
that will be lambda b times one by eight ok
similarly for d is equal to zero i can write

329
00:37:56,420 --> 00:38:10,040
lambda b times p hat d that is that is lambda
b times p d minus one by eight ok that is

330
00:38:10,040 --> 00:38:17,330
lambda b times p d here was seven by thirty
two seven by thirty two minus one by eight

331
00:38:17,330 --> 00:38:26,390
ok so that would be lambda b times this will
be seven minus four three by thirty two ok

332
00:38:26,390 --> 00:38:32,410
so suppose you get this values
now how will you compute the the how will

333
00:38:32,410 --> 00:38:39,340
you go to the probability b a w given a b
you have to first find what is the back off

334
00:38:39,340 --> 00:38:45,800
probabilities of for these two two cases c
and d so far that you define the what is the

335
00:38:45,800 --> 00:38:51,320
value of lambda b now how do you find lambda
b if you normalized this right so you will

336
00:38:51,320 --> 00:39:04,120
say lambda b times one by eight plus three
by thirty two plus four by eight plus two

337
00:39:04,120 --> 00:39:10,210
by eight should give me one right this should
be one for if you sum over all the words so

338
00:39:10,210 --> 00:39:18,150
so what this is this say lambda b times seven
by thirty two is equal to one minus six five

339
00:39:18,150 --> 00:39:24,930
eight that is two by eight so what is lambda
b so we will get ok

340
00:39:24,930 --> 00:39:32,320
so you will get this eight by seven so once
a we substitute add up this eight by seven

341
00:39:32,320 --> 00:39:39,000
here that will give you this probability eight
so you will find this probability and this

342
00:39:39,000 --> 00:39:44,890
probability also so you have the back off
probability for this now put this back off

343
00:39:44,890 --> 00:39:51,720
probability here and we have a constant how
do you find this constant again you will have

344
00:39:51,720 --> 00:39:58,100
two normalized it lambda a b times summation
of this plus this probability should add up

345
00:39:58,100 --> 00:40:02,800
to one that will give you my lambda a b and
also give you this probability distribution

346
00:40:02,800 --> 00:40:18,440
so that's why you find the back off probability
distribution using this recursive definition

347
00:40:18,440 --> 00:40:23,940
ok and what do you linear interpolation we
have different unigram bigram trigram orders

348
00:40:23,940 --> 00:40:29,820
i try to interpret them in different proportions
ok so what we are doing here we have computed

349
00:40:29,820 --> 00:40:34,839
a trigram model bigram model unigram model
and different proportions lambda one lambda

350
00:40:34,839 --> 00:40:46,040
two lambda three are given to this ok
so such that they will add up to one in general

351
00:40:46,040 --> 00:40:51,300
you can also conditional lambdas on the context
ok what is your previous an and previous to

352
00:40:51,300 --> 00:40:57,750
previous word the lambda might depend on that
so this is the same same equation except that

353
00:40:57,750 --> 00:41:05,580
now lambda is a depending on the context ok
and now the question might be how do i choose

354
00:41:05,580 --> 00:41:11,410
a held out corpus how do i choose my lambdas
so that you will do as we say earlier you

355
00:41:11,410 --> 00:41:15,100
will have an held out corpus will find out
which values are lambda gives you the highest

356
00:41:15,100 --> 00:41:18,559
probability and that that lambdas you can
has for your smoothing ok

357
00:41:18,559 --> 00:41:26,900
so that's we will finish our topic of language
model so all so we covered lot of sim simple

358
00:41:26,900 --> 00:41:32,240
ah smoothing methods and some advanced smoothing
methods and then we will now go to the next

359
00:41:32,240 --> 00:41:38,040
topic of ah so we will start with the topic
of morphology ok so that will be the topic

360
00:41:38,040 --> 00:41:38,730
of the next lecture ok

