1
00:00:18,279 --> 00:00:22,009
welcome back for the second lecture of this
week so we have started talking about entity

2
00:00:22,009 --> 00:00:29,320
linking and we we talked about two different
approach so one approach where we use the

3
00:00:29,320 --> 00:00:34,010
keyphraseness in commonness to find out what
are the appropriate mentions and how do you

4
00:00:34,010 --> 00:00:40,010
link to them their ah corresponding reference
in the ah in the knowledge base and we were

5
00:00:40,010 --> 00:00:45,370
considering wikipedia as our knowledge base
and we found out one particular problem with

6
00:00:45,370 --> 00:00:50,260
using simple keyphraseness and commonness

7
00:00:50,260 --> 00:00:59,420
so so what is the problem so so in in commonness
what we were doing we were always taking the

8
00:00:59,420 --> 00:01:05,590
page that is having the highest ah commonness
so what will happen suppose i have word like

9
00:01:05,590 --> 00:01:11,390
tree and the commonness and commonness to
the sense tree is ninety two point eight two

10
00:01:11,390 --> 00:01:16,420
percent but the other concepts because they
occur rarely commonness is like two point

11
00:01:16,420 --> 00:01:22,090
nine four percent two point five seven percent
and so on so what you are seeing wherever

12
00:01:22,090 --> 00:01:27,659
the word tree occurs you will by default assign
it to the first sense of tree and you will

13
00:01:27,659 --> 00:01:31,950
not look at the context at all
so in this case so you have it article about

14
00:01:31,950 --> 00:01:37,180
depth first search and you have a sentence
where the word tree occurs and because of

15
00:01:37,180 --> 00:01:43,460
using commonness you will always assign it
to the sense tree but the correct sense says

16
00:01:43,460 --> 00:01:50,329
here is tree data structure but he it has
a very small commonness so only two point

17
00:01:50,329 --> 00:01:56,799
five seven percent so what you need to do
for ah assigning it to the correct ah data

18
00:01:56,799 --> 00:02:02,840
structure correct sense there is a tree data
structure for that you need to you you should

19
00:02:02,840 --> 00:02:08,090
be able to use the context here that is what
are the referent words that i am seeing in

20
00:02:08,090 --> 00:02:15,540
the context
now question is how best we can use the context

21
00:02:15,540 --> 00:02:20,780
now i do not want to use some random words
in the context ok i want to use the words

22
00:02:20,780 --> 00:02:27,819
in the context that are actually assign that
have actual correspondence to a wikipedia

23
00:02:27,819 --> 00:02:32,530
page so that i can find out something about
the wikipedia page how common this wikipedia

24
00:02:32,530 --> 00:02:41,200
page is to one of it's reference now there
we we fall into the same problem that how

25
00:02:41,200 --> 00:02:46,470
do i use a wikipedia reference to any of it's
page when the disambiguation is not yet happened

26
00:02:46,470 --> 00:02:50,690
so i am only at the ta at the stage of ah
disambiguation

27
00:02:50,690 --> 00:02:58,220
so how do i use the actual entities entity
page and for that if a nice tree can be used

28
00:02:58,220 --> 00:03:03,569
and idea is ok so with this word there are
many other words that are that are coming

29
00:03:03,569 --> 00:03:09,010
in this article or is is if this piece of
text and some of these will be appropriate

30
00:03:09,010 --> 00:03:15,830
mentions and they will into one or many ah
wikipedia pages now some of out of these at

31
00:03:15,830 --> 00:03:20,150
least some will be there that have a unique
disambiguation page in wikipedia so there

32
00:03:20,150 --> 00:03:25,709
is a unique page in wikipedia where they link
to and there i do not need to do any disambiguation

33
00:03:25,709 --> 00:03:31,379
so why don't i use only those pages which
have a ah unique place in wikipedia to find

34
00:03:31,379 --> 00:03:39,680
out what should be a good of a wikipedia page
for this entry tree so that's what is done

35
00:03:39,680 --> 00:03:45,400
so so this is the hypothesis that if you have
a sufficiently long text you can find out

36
00:03:45,400 --> 00:03:50,030
terms that do not require disambiguation at
all there will be some terms that have only

37
00:03:50,030 --> 00:03:58,190
one mention or one referent in wikipedia now
use this unable un unambiguous link in the

38
00:03:58,190 --> 00:04:03,840
document that context to disambiguate the
ambiguous ones so what is the idea here so

39
00:04:03,840 --> 00:04:10,360
you are given this article and you want to
find out what is the appropriate ah sense

40
00:04:10,360 --> 00:04:13,189
for this wordage
so in a sense i mean what is the appropriate

41
00:04:13,189 --> 00:04:19,870
link in wikipedia is it tree tree graph theory
data structures set theory etcetera so what

42
00:04:19,870 --> 00:04:24,970
i will do i will see what are the other mentions
in this page so what are the other things

43
00:04:24,970 --> 00:04:33,100
you have seen algorithm tree structure graph
backtracking uninformed search tree backtracks

44
00:04:33,100 --> 00:04:42,030
lifo stack and so on now among these there
will be some that have only one wikipedia

45
00:04:42,030 --> 00:04:46,270
page as the referent so these are ah shown
as box search

46
00:04:46,270 --> 00:04:51,830
so algorithm tree structure uninformed search
and lifo stack have only one wikipedia page

47
00:04:51,830 --> 00:04:58,370
so i will take these unambiguous links and
try to find out how close these foldings are

48
00:04:58,370 --> 00:05:04,940
to my all of these possible senses ok so how
close are these folding to this possible sense

49
00:05:04,940 --> 00:05:11,240
and the one sense that is having the the is
the closest to these four will be called by

50
00:05:11,240 --> 00:05:19,620
sense there should be the link to will to
which i will link my corresponding ah mention

51
00:05:19,620 --> 00:05:29,050
so how do we compute the relatedness score
and this can be very simple so so you you

52
00:05:29,050 --> 00:05:35,990
can initially start by ripping representing
each candidate sense and context term by a

53
00:05:35,990 --> 00:05:45,229
single wikipedia article ok so for example
what is happening now so a word like tree

54
00:05:45,229 --> 00:05:52,509
and tree corresponds to many different senses
and call them your article article one article

55
00:05:52,509 --> 00:05:59,290
two article three article four ok and if you
remember your word sense disambiguation it's

56
00:05:59,290 --> 00:06:05,819
like constructing various ah sense backs there
are four different senses they are like science

57
00:06:05,819 --> 00:06:11,520
science x now you are having a context back
kind of context track where you are saying

58
00:06:11,520 --> 00:06:19,060
ok in this context of tree i am finding four
different words word one word two word three

59
00:06:19,060 --> 00:06:24,830
word four now what is the property of each
words they link to one page in wikipedia

60
00:06:24,830 --> 00:06:37,641
ok so like a w one a w two a w three a w four
so now these are wikipedia articles and they

61
00:06:37,641 --> 00:06:46,340
are also wikipedia articles now now the problem
is select the sense article that is the that

62
00:06:46,340 --> 00:06:51,039
is most in common with all of the context
articles so that is among the four articles

63
00:06:51,039 --> 00:06:57,300
which is most common with all these four articles
ok and you will see what is the art max that

64
00:06:57,300 --> 00:07:01,770
is having the more similarity with these four
articles and this can be captured in many

65
00:07:01,770 --> 00:07:09,180
different ways so we will talk about one particular
method so and one go to the method is you

66
00:07:09,180 --> 00:07:14,849
just take the wikipedia link based method
that is two pages are similar if they are

67
00:07:14,849 --> 00:07:20,129
having many incoming and outgoing lon links
common

68
00:07:20,129 --> 00:07:26,169
ok so what is the idea so how do you find
out how similar a one is to a w one so i will

69
00:07:26,169 --> 00:07:33,740
say ok i have two articles a one a w one and
wikipedia i find out what are the incoming

70
00:07:33,740 --> 00:07:44,060
links to this article and what are the outgoing
links from here same i will do for this article

71
00:07:44,060 --> 00:07:55,080
and now once i have found this out i can see
ok what are the common links so how many articles

72
00:07:55,080 --> 00:08:05,400
in wikipedia article w a prime are linking
to both of these similarly what are the articles

73
00:08:05,400 --> 00:08:11,580
to which both of these link to and these are
very good measures for finding out how similar

74
00:08:11,580 --> 00:08:17,930
they are you see we can always do it by seeing
how similar they are by measuring their text

75
00:08:17,930 --> 00:08:25,250
similarity
how much text similarities is there you can

76
00:08:25,250 --> 00:08:30,910
capture cosine similarity and what or something
else but this is a nice link based measure

77
00:08:30,910 --> 00:08:42,440
that says ok how many pages linked to both
of these so what are the common in links and

78
00:08:42,440 --> 00:08:50,589
how many pages they both linked to that is
a common out links and this is again a nice

79
00:08:50,589 --> 00:08:55,290
measure in that it says ok this article refers
to both of these that means they they need

80
00:08:55,290 --> 00:08:59,310
to have something common similar they both
mentioned the same article again that means

81
00:08:59,310 --> 00:09:03,160
they they need to have something in common
and you will find out how many what fraction

82
00:09:03,160 --> 00:09:06,790
of incoming links are common
what fraction of for being links are common

83
00:09:06,790 --> 00:09:11,959
and that you will take it as a measure for
computing how similar these two articles are

84
00:09:11,959 --> 00:09:22,650
and this is also called by relatedness 
ok so we talked about keyphraseness commonness

85
00:09:22,650 --> 00:09:34,510
and this is relatedness and then you can find
out the relatedness of a candidate size sense

86
00:09:34,510 --> 00:09:40,280
by taking a weighted average of it's relatedness
with all of the context articles so that is

87
00:09:40,280 --> 00:09:45,839
to find out the relatedness of this sense
a one you will say ok it's relatedness with

88
00:09:45,839 --> 00:09:55,309
a w one will a w two a w three a w four ok
by using this measure then you take a average

89
00:09:55,309 --> 00:10:05,190
or a weighted average compute a weighted average
and this will be ok what is the relatedness

90
00:10:05,190 --> 00:10:11,309
of this sat sense a one a two a three a four
whichever as the highest you take that ok

91
00:10:11,309 --> 00:10:16,829
like if you see the previous slide so here
you were capturing showing relatedness of

92
00:10:16,829 --> 00:10:21,680
various senses and this tree data structure
had the highest relatedness sixty three point

93
00:10:21,680 --> 00:10:27,790
two six percent that uses the average of relatedness
with all these the four different context

94
00:10:27,790 --> 00:10:35,670
articles now so there is one term here we
are taking a weighted average now what should

95
00:10:35,670 --> 00:10:42,309
this weight depend on why should i weight
one of this higher than the other ones

96
00:10:42,309 --> 00:10:48,750
so again if you think about it it can it can
depend on which context term is more important

97
00:10:48,750 --> 00:10:54,549
than other another ok so what we have done
we have taken the context we have found out

98
00:10:54,549 --> 00:11:00,670
to all the words that are that are mentions
and from there whichever are so these were

99
00:11:00,670 --> 00:11:08,470
ah unambiguous whichever were an unambiguous
we are taking them as my context to find out

100
00:11:08,470 --> 00:11:15,150
the relatedness but some of these might be
more important for this topic or the document

101
00:11:15,150 --> 00:11:19,700
than others so so can i give a weight depending
on how important they are to the topic

102
00:11:19,700 --> 00:11:24,709
now the question again comes how do i know
which one are more important to the topic

103
00:11:24,709 --> 00:11:29,750
or the theme of this document and then you
see you can again use the idea of relatedness

104
00:11:29,750 --> 00:11:38,670
ok that means one among these four context
art ah senses articles the one that is having

105
00:11:38,670 --> 00:11:46,360
the highest relatedness with the others which
more which more appropriate to the theme of

106
00:11:46,360 --> 00:11:52,850
the theme of the document yes because there
is a theme of the document and the words that

107
00:11:52,850 --> 00:11:57,530
are appropriate should also be connected to
each other that means a word that is having

108
00:11:57,530 --> 00:12:04,199
a high relatedness with other words is it
should be given a high values ok and that

109
00:12:04,199 --> 00:12:11,459
is a nice method to also give a weight here
weight to different of these relatedness that

110
00:12:11,459 --> 00:12:22,069
is how related these this context article
is to the other context articles

111
00:12:22,069 --> 00:12:26,809
so in general there are so what are the things
that are used to give a weight to the context

112
00:12:26,809 --> 00:12:34,049
term so one is called link probability so
what is link probability ah so so again in

113
00:12:34,049 --> 00:12:38,650
your context you are finding say four or five
articles where the link is unambiguous there

114
00:12:38,650 --> 00:12:47,419
is only one link now you can use the link
probability itself that is among the four

115
00:12:47,419 --> 00:12:54,189
which one is like a keyword that is it always
links to something some words may not link

116
00:12:54,189 --> 00:12:58,910
may not alwa always link some words always
link so the words it always link should be

117
00:12:58,910 --> 00:13:04,230
given a high weightage because i know there
this is a more a specific term if a word is

118
00:13:04,230 --> 00:13:09,939
somewhat sometimes links sometimes not linked
it may not be a very important keyword so

119
00:13:09,939 --> 00:13:12,620
that can be one measure
what is the link probability probability that

120
00:13:12,620 --> 00:13:18,160
it will be given a link in general wikipedia
that is same as my key phraseness measure

121
00:13:18,160 --> 00:13:24,760
and certain thing we have already discussed
that is relatedness so find out how closely

122
00:13:24,760 --> 00:13:29,819
it relates to the central document by computing
it's average relatedness to all other context

123
00:13:29,819 --> 00:13:38,000
terms ok so i take for each word what is the
link probability or keyphraseness and relatedness

124
00:13:38,000 --> 00:13:41,689
now i have two different majors so how do
i take these together to compute the rela

125
00:13:41,689 --> 00:13:48,150
the weight of this word you can simply ah
take an average to provide the weight for

126
00:13:48,150 --> 00:13:55,689
each context ok so is that clear now so you
have some words in the context they are ambiguous

127
00:13:55,689 --> 00:14:03,910
sorry unambiguous for each word for each context
word you find out the relatedness of this

128
00:14:03,910 --> 00:14:10,600
ah mention in your mention sense one of the
sense taken weighted average and this weight

129
00:14:10,600 --> 00:14:15,760
depends on what is the link probability and
what is the relatedness with all the context

130
00:14:15,760 --> 00:14:22,310
terms and and by doing doing this method you
can find out ah relatedness of each of the

131
00:14:22,310 --> 00:14:31,170
four senses
now now the so so we have discussed ok we

132
00:14:31,170 --> 00:14:36,319
can take some mentions by some method and
then once with the mentions we can also give

133
00:14:36,319 --> 00:14:44,780
a link by ah finding out which of the candidates
are similar in in from with the context ah

134
00:14:44,780 --> 00:14:51,240
mentions but here there is an interesting
question that by using all this when when

135
00:14:51,240 --> 00:14:57,450
we are doing all this approach can i go back
and also improve my mention detection part

136
00:14:57,450 --> 00:15:03,620
so how i was detecting the mentions so i was
gathering all the n grams in the document

137
00:15:03,620 --> 00:15:11,199
and retaining only those whose probability
exceeds a very low threshold so that is i

138
00:15:11,199 --> 00:15:21,600
start with a text document there i take various
n grams it can be ok i i take certain pattern

139
00:15:21,600 --> 00:15:29,059
they are noun groups or something like there
is some patterns ah n grams i can say one

140
00:15:29,059 --> 00:15:37,580
two three whatever and i take some n grams
ok and then see what is the keyphraseness

141
00:15:37,580 --> 00:15:44,679
of each of these and whichever as ah if this
is above a threshold when i take it as a mention

142
00:15:44,679 --> 00:15:53,189
and then i then it's a mention and then i
go to my link disambiguation part but see

143
00:15:53,189 --> 00:15:59,540
what are you seeing that when i am finding
out the appropriate entities as mentions i

144
00:15:59,540 --> 00:16:05,290
am not using the context at all i am just
seeing is it a good word for ah is it a good

145
00:16:05,290 --> 00:16:10,370
key phrase or not does it have a good keyphraseness
or not overall so independent of a context

146
00:16:10,370 --> 00:16:14,640
is it a good mention or not
so question is can i also use the context

147
00:16:14,640 --> 00:16:19,970
to find out what are good mention and what
are not so good mentions and that's what we

148
00:16:19,970 --> 00:16:29,740
will see so it is the best method so all the
remaining phrases are disambiguated using

149
00:16:29,740 --> 00:16:37,060
the approach mentioned earlier yes so we have
whatever ah mentions we have found or whatever

150
00:16:37,060 --> 00:16:43,449
phrases we have found we do disambiguation
using an approach that we have

151
00:16:43,449 --> 00:16:48,550
so now by doing this approach you get to find
a lot of different things like what are the

152
00:16:48,550 --> 00:16:53,459
links what are the appropriate ah mentions
here what are the wikipedia pages they link

153
00:16:53,459 --> 00:16:58,150
to ok so you are getting some new wikipedia
pages also so now can you use this additional

154
00:16:58,150 --> 00:17:07,600
information to find out are they good ah candidates
for mention at all so so that is can you use

155
00:17:07,600 --> 00:17:16,400
that to find out which concepts should be
linked so here is one example ok so you are

156
00:17:16,400 --> 00:17:25,900
having a ah a wikipedia page ah at so it's
like a news article so democrats deal is clinton

157
00:17:25,900 --> 00:17:32,490
setback and you are having lot of ah so ah
various sentences are here now what is your

158
00:17:32,490 --> 00:17:38,540
approach in your approach you you take a various
mentions like hilary clinton occurs at various

159
00:17:38,540 --> 00:17:44,920
locations and try to find out what is the
entity in wikipedia it will link to similarly

160
00:17:44,920 --> 00:17:51,160
here barack obama nomination vote michigan
all these are link to their various wikipedia

161
00:17:51,160 --> 00:17:55,920
entities
so you get all this by your ah link disambiguation

162
00:17:55,920 --> 00:18:04,060
phrase now my question is can i use that together
to find out what are good mentions also for

163
00:18:04,060 --> 00:18:10,400
my text and for that we have to convert that
to some sort of a learning problem learning

164
00:18:10,400 --> 00:18:17,820
problem where i run my algorithm on a on a
data and see what are the good ma ga what

165
00:18:17,820 --> 00:18:21,870
are the mentions i am detecting what are the
links i am connecting to now once i have all

166
00:18:21,870 --> 00:18:27,640
this information someone gives me gold standard
that what are the good links here what are

167
00:18:27,640 --> 00:18:34,970
not the book links using that can i learn
what are my what are the good candidates to

168
00:18:34,970 --> 00:18:38,970
be mentioned what are the good what are not
so good candidates to be mention so like coming

169
00:18:38,970 --> 00:18:44,020
back to my example so i start with the text
data and i find out ok there are some mentions

170
00:18:44,020 --> 00:18:47,290
they have driver key phrase above a threshold
ok

171
00:18:47,290 --> 00:18:55,430
now i also link them to their wikipedia articles
some of these might be linked to the same

172
00:18:55,430 --> 00:19:07,140
article ok so that i do for this whole document
now suppose someone tells me that actually

173
00:19:07,140 --> 00:19:12,930
this is a good mention this is a good mention
this is a good mention but this is not so

174
00:19:12,930 --> 00:19:19,150
good mention ok this is good mention this
is not so good mention so once i have all

175
00:19:19,150 --> 00:19:30,550
this information can i develop a machine learning
method to detect ok given an article given

176
00:19:30,550 --> 00:19:40,330
in mention and it's approved wikipedia page
all the context is it a good ah mention at

177
00:19:40,330 --> 00:19:45,580
all so given a phrase with all these attributes
is it a good mention for this document or

178
00:19:45,580 --> 00:19:50,730
not
so now so you can say that once you given

179
00:19:50,730 --> 00:19:56,030
me the text all the all the steps that i have
taken are deterministic so i can apply ah

180
00:19:56,030 --> 00:20:00,440
keyphraseness i can find out the mentions
i can link them to their wikipedia pages so

181
00:20:00,440 --> 00:20:05,530
all this i can easily do but how would i get
these gold standards that doesn't inappropriate

182
00:20:05,530 --> 00:20:10,470
ah mention this is not inappropriate mention
and this is one of the bat bottlenecks so

183
00:20:10,470 --> 00:20:16,630
how do i get this ah actual links and not
so not good links and good mentions and not

184
00:20:16,630 --> 00:20:22,480
so good mentions
for that so now what is interesting idea here

185
00:20:22,480 --> 00:20:34,620
can you use wikipedia again so can use wikipedia
again so how would you use wikipedia for this

186
00:20:34,620 --> 00:20:41,860
so if you think think a bit so how you can
use wikipedia and that's actually very very

187
00:20:41,860 --> 00:20:52,220
easy so you take wikipedia and take some wikipedia
articles say a one a two a three a four a

188
00:20:52,220 --> 00:21:00,890
five so on now each of the article now forget
the hyperlink structure here so take it as

189
00:21:00,890 --> 00:21:10,840
a plain text and feed it to your algorithm
so algorithm takes a one as input plain text

190
00:21:10,840 --> 00:21:16,750
and runs this ok so your algorithm will run
this it will tell tell you what are the mentions

191
00:21:16,750 --> 00:21:23,870
what do they link to and so on
now because a one is already wikified so it's

192
00:21:23,870 --> 00:21:29,000
already in wikipedia you know what mentions
are good what mentions are not good so from

193
00:21:29,000 --> 00:21:34,310
there you can automatically construct your
gold standard ok and once you have the gold

194
00:21:34,310 --> 00:21:39,500
standard you can apply a machine learning
method to say ok which given a feature around

195
00:21:39,500 --> 00:21:46,500
this fridge is it a good mention in this context
or not and that will solve your problem so

196
00:21:46,500 --> 00:21:53,820
this is like you are learning to ah link using
wikipedia so using the wikipedia data and

197
00:21:53,820 --> 00:22:00,520
ah so very nicely you are taking it as a training
data and also constructing your gold standard

198
00:22:00,520 --> 00:22:04,570
from this without having some manual efforts
of labeling because otherwise you will see

199
00:22:04,570 --> 00:22:13,240
this labeling will take a huge amount of time
and this will help you do that automatically

200
00:22:13,240 --> 00:22:22,850
so so what will i do so now once i have taken
the wikipedia as in as input i know ok whichever

201
00:22:22,850 --> 00:22:27,910
phrases gave me a wikipedia article that was
actually there in the original article they

202
00:22:27,910 --> 00:22:35,840
are possible examples and whatever was not
there becomes a negative example and so yeah

203
00:22:35,840 --> 00:22:40,950
so you got the possible negative examples
and you feed it to your classifier and then

204
00:22:40,950 --> 00:22:47,400
you use various features around these various
mentions and articles to detect whether it's

205
00:22:47,400 --> 00:22:54,720
a good mention or not so you use various features
like like the places where they are mentioned

206
00:22:54,720 --> 00:23:01,000
to inform the classifier ab about which topic
should and should not be linked

207
00:23:01,000 --> 00:23:06,510
so now so now what are what can be these possible
features that you can use from a given n gram

208
00:23:06,510 --> 00:23:12,960
phrase so so let's see some features some
of some of these features are what you have

209
00:23:12,960 --> 00:23:19,560
already seen and some other features can depend
on how do people actually write ah a wikipedia

210
00:23:19,560 --> 00:23:27,340
article so what are the good features so one
feature that we can use is link probability

211
00:23:27,340 --> 00:23:37,460
ok so so that is for a given mention what
is the link probability now if it is occurring

212
00:23:37,460 --> 00:23:42,130
at multiple places ok like hilary clinton
clinton so they are occurring at different

213
00:23:42,130 --> 00:23:46,150
different ah variations
so what are they li their link probabilities

214
00:23:46,150 --> 00:23:51,930
at each of these places take either the average
or the maximum of this link probability and

215
00:23:51,930 --> 00:23:57,580
that can be one feature ok so this you are
doing jointly so hilary clinton and clinton

216
00:23:57,580 --> 00:24:02,830
together should they will linked or not and
here you are trying to use ok what is a link

217
00:24:02,830 --> 00:24:09,130
probability it at different places taking
average or also taking maximum both can be

218
00:24:09,130 --> 00:24:19,559
a features then you can use the relatedness
so how related these ah phrases are to the

219
00:24:19,559 --> 00:24:23,370
central theme of the document
so again you will find out what is the relatedness

220
00:24:23,370 --> 00:24:32,150
of these mentions with different ah unambiguous
links in the entire article so this can be

221
00:24:32,150 --> 00:24:37,120
another feature if they are very highly related
then only you will take them as your mentions

222
00:24:37,120 --> 00:24:41,180
if they are not related to the entire theme
of the document that means they are not probably

223
00:24:41,180 --> 00:24:47,970
not good ah candidates for mentions then you
can also use the disambiguation confidence

224
00:24:47,970 --> 00:24:53,440
that is when you are trying to do a disambiguation
over this ah mention

225
00:24:53,440 --> 00:24:59,070
how confident your classifiers if your classify
is not very confident that means you do not

226
00:24:59,070 --> 00:25:04,250
have sufficient context in those document
and it may not be a good mention at all so

227
00:25:04,250 --> 00:25:12,130
this confidence can also be one of the features
then you can use the generality that is when

228
00:25:12,130 --> 00:25:18,400
you are trying to link some ah phrases in
in your text what is the idea you do not want

229
00:25:18,400 --> 00:25:22,870
to link something that is very very generic
that all people already know about so you

230
00:25:22,870 --> 00:25:28,650
want to link the phrases that are very specific
so how can you know about the jan is ah how

231
00:25:28,650 --> 00:25:36,050
generic or specific a particular phrases further
you can use the the the category to your wikipedia

232
00:25:36,050 --> 00:25:42,700
and there you can see at what what depth in
the tree this particular ah mention comes

233
00:25:42,700 --> 00:25:50,510
in so if it comes at a very ah it's very top
label itself that means it is a ah it is a

234
00:25:50,510 --> 00:25:54,970
very generic term but this coming very low
in the tree that means say a specific term

235
00:25:54,970 --> 00:25:59,940
so specifi specific terms might be given a
high preference so this can also be ah like

236
00:25:59,940 --> 00:26:05,470
your feature what is the depth in the wikipedia
hierarchy tree and then you can also see how

237
00:26:05,470 --> 00:26:12,120
the documents are written that is where all
this entities mentioned so for example if

238
00:26:12,120 --> 00:26:17,170
it is a good entity it will be mentioned in
the introduction similarly it will be mentioned

239
00:26:17,170 --> 00:26:21,630
in the con conclusion of section of the of
article so if it is mentioned in the initial

240
00:26:21,630 --> 00:26:26,430
full lines or the last few lines it might
be important

241
00:26:26,430 --> 00:26:32,010
so you can simply measure the offset ah from
the beginning end and the end then you can

242
00:26:32,010 --> 00:26:36,700
also see the spread that is what is the distance
between then you shall mention in the last

243
00:26:36,700 --> 00:26:42,420
mention so that is how far does the response
across the document if the spread is high

244
00:26:42,420 --> 00:26:46,490
that means ok it might be a good mention if
the spread is low that means very to only

245
00:26:46,490 --> 00:26:50,420
cover very small topic of this document this
has been used

246
00:26:50,420 --> 00:26:55,980
so this can again be a feature for this task
and you can think of many of other features

247
00:26:55,980 --> 00:27:01,330
combine these features in your classifier
and then you are learning whether given this

248
00:27:01,330 --> 00:27:09,350
phrase with all these features is it a good
ah ah candidate for mention or not and this

249
00:27:09,350 --> 00:27:15,620
is like you are learning you are learning
to link using the wikipedia structure so as

250
00:27:15,620 --> 00:27:21,360
such you you take many different methods but
this is the basic conceptual idea about entity

251
00:27:21,360 --> 00:27:25,920
linking that how do you detect mentions different
methods

252
00:27:25,920 --> 00:27:32,360
how once you detect mentions how do you link
them to their ah appropriate entries in the

253
00:27:32,360 --> 00:27:36,480
wiki in the wikipedia or any other database
and can you use this task to also improve

254
00:27:36,480 --> 00:27:43,840
your mentions ok and you can take it in different
different applications take different databases

255
00:27:43,840 --> 00:27:50,220
and you can try out various variations for
this task so this so that's where we finish

256
00:27:50,220 --> 00:27:55,120
our discussions on entity linking so in the
next lecture onwards we will start talking

257
00:27:55,120 --> 00:28:00,460
about information extraction that is from
a from a document where there is a lot of

258
00:28:00,460 --> 00:28:05,880
unstructured data text data how can you identify
various entities and the relations between

259
00:28:05,880 --> 00:28:07,480
them
thank you

