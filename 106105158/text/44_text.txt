 so hello everyone welcome to the third lecture of this week so we are talking about topic models and we have discussed what is the formulation of latent dirichlet allocation and we say that what are the different parameter that we need to learn we need to learn ah mainly three parameters that is what are my ah ah topic distributions given a topic what is the probability of each word what are my per document topic proportion so that is my thetas and then per document per word topic assignment that is by z so this i have to compute so this i have to compute the posterior distribution of all this parameter given my observation observation is all my corpus all the documents that i am seeing so so in this lecture we will discuss one ah interesting method for doing that that is gibbs sampling and then in the end we will also talk about some simple applications that once we had learnt topic models over a corpus what are the some of the simple things that you can do ok so ah so for approximating the posterior probabilities of all these parameters the algorithm generally fall in two categories so one are sampling based algorithms so that is from the posterior you try to collect the samples of the distributions and then ah approximate it with the empirical distribution and that's what we will focus on in gibbs sampling and there are also variational methods so where we convert the inference problem to some sort of optimization problem and try to learn the parameters that we optimize that so so both the methods are very ah very much used in in the literature so we will see we will see gibbs sampling that is an easier to understand method that i will i will say so ah so gibbs sampling are some sort of markov chain ah monte carlo methods so what is the idea so having a high dimensional distribution so think about all the possible values that that your parameter can take all your betas ah theta inject they can take so huge number of values ok so idea here is you sample on an lower dimensional subset of variables and each subset is conditioned on whatever as it known ok so you do not computing the joint probability of everything you are assuming sum to be known and computing probability for others and that you keep on doing in terms for all the all the variables so sampling is done sequentially and proceeds until the sampled value is approximate my target distribution and it directly estimates the posterior distribution over z and uses this to provide estimates for beta and theta so will we will find out the distribution over z and then use that to compute my theta and beta and we will see how do we do that so so what is the idea so assume that you are you are having a corpus and in the corpus you have some documents and words ok now take so you will ah assume that in a in one of the iteration you are at a particular word that is ah the word token i and what you want to find out what is the topic assignment probability what's the probability that is ith token will be assigned to topic j probability z i is equal to j that's what you want to find out so now how how do you do that now re you represent the collection of documents by a set of word indices w i and document indices d i for this token i so it simply says that you has a lot of words find out what is the corresponding word token for this i that may be called w i w i will give you the particular index and similarly d i d i will be the particular document where you are having this token so now so what gibbs sampling does it consider each word token in turn and estimates the probability of assigning the current word token to each topic conditioned on the topic assignment of all other word tokens so what it is saying that in the particular iteration assume that given the topic assignment for all the other words except this word now based on this you try assign a topic to this word so what will happen we are having a set of documents ok so these are your documents and suppose you know you unique words these are the your words and here at a i th here at a ith token so the corresponding document will be called d i this is i token and the corresponding word will be called w i ok so now what you have to find out probability that the topic assignment for this word will be z that's what we have to find out and what is assumed by is that in this iteration you know the topic assigned for everything else so you know what are the topics that assigned for all different words involve that but you want to estimate to find out the topic for this particular i th word so you have to so how do you do that you first estimate this probability and this you do condition on everything else so write it like this probability z i at j given the topic assignment for all the words other words minus sign is everything other then this i and what is the word index and what is the document index this is what we want to find out now intuitively what should it depend on so what is the probability that this ith token will be will be given assigned the topic j what should it depend on so we think in terms of topic models this probability that the i th word should be assigned the topic z should depend on two things one is so we say that document consist of certain topics ok so if this topic is prevalent in the document there might be a high chance that this word is assigned this topic j so that is how likely is topic j to be assigned to d i what is the next thing how likely is that this word occurs in topic j ok is word w i for topic j so we have two things i need to know now how do i know this one how likely is topic j to be assigned to document d i you see i am given the topic assignment for all other words in this document so i find out what section of words in this document are assigned this topic j ok divide by the number of words so this i can computationally by this topic assigned now how do you compute this how likely each word w i for topic j i I will again see in topic j what are the different words that come and how many times word w i comes in this topic j and this i can used to to compute this probability and then i can multiply these two to find out what is the probability of a topic j been assigned to this word token i and this i will do for all excuse me topics j is equal to one to k so this will give me a multinomial distribution then to give a assignment to this word i will sample from this multinomial distribution and then from sampling i will assign one topic and i will i will assign the topic here so suppose the topic is five i will put this is five and then i will move to the next word token and then i will assume that this is not given and i will take everything as given do that find the topic assignment and that's what i keep on doing so now once the intuition clear let us go back to the formulation so yes from this conditional distribution we are sampling a topic and we are storing it as the new topic assignment for the word and this is written as this assignment probability that ith word has the has a topic j given everything else so so how do we achieve this how do we compute all these values for that i need to ah keep two different matrices so one is c w t of dimensions w times t another is c d t of dimensions d times t so ah so what are these matrices so one is c w t w times t and next is c d t d times t t is the number of topics w is the number of words and d is the number of documents so what do they store so c w j w t that is i take the word w and topic j so w j w t so this element is called c w j w t this contains the number of times word w is assigned to topic j not including the current inst current instants so we are taking at the current instants except that how many times this word is assigned this topic that you can find out from the your whole ah data you will actually keep this stable so we will just update the values ok so let me give an point point you know how many times this word is assigned topic j so we will do that for all the values in this matrix how many times this word is assigned this topic yes similarly a volume at c d t small d j this will contain for the document d topic j number of times topic j is assigned to some word token in document d that is how many words in the document d are assigned to topic j in that again you can you can have all the values so this will again be not included in the current distance we will find out how many times ah any word in document d is assigned this topic and we will do it for all the documents so this you will have these two matrices c w t c d t and you are missing now what is element missing now once you have these two matrices how do i compute probability z i is equal to j given z minus i w i d i and research may depend on two different things so let us see that so depends on two parts one is ah what is the probability of word w under topic j remember we are talking about two parts so how what will depend on so we are saying that it will depend on how likely is this word to come under topic j second is how likely is this topic to be assigned to document d two thing's how likely is this word to come on to topic j how likely is this topic to come under document d now how do i write this probabilities in terms of this matrix ah elements how likely is word w w to come under topic d j so that will be c i th word topic j w t summation over all the words so this will give p probability for this word summation over all words c w j w t ok similarly for how likely is topic z to come on the document d this would be c d i ok for the current ins instance j d t summation over now for all the topic that are assigned in this document so this will be summation over all my topics c d t c d t capital d t ok so this will be the simple formulation now ah so there are certain priors that you take here so you will have some sort you can can these are some sort of smoothing so here you have some smoothing parameter eta similarly it will be plus w times eta here alpha plus t times alpha so smoothing parameters and this will again to make it a probability it is normalized so instead of calling it equal to we will say proportional to this so now this gives you the formulation probability that the i th word token will be assigned topic j given all this so now you can say that what are these eta and alpha so this is my digital parameters that that we were seeing earlier and and you can also ah correlate this with their values so suppose your alpha is high if your alpha is high what would happen this value will not matter and all the topics will be assigned roughly equal probability and that's what was happening if you keep on increasing alpha you are going towards a distribution where all topics have some probabilities but if you alpha is very very small then what will matter is only discount and that's why you're going towards only a few topics so these are some so these are the intuitions same thing you can do with eta so if eta is high all the words will have equal probability of coming into topic if it is low then certain distributions will be preferred so now so this is the formulation for probability that z i is equal to j so left part here is the probability of w word w under the topic j so that is how likely the word is for a topic and the right part is the probability of topic j under the current topic distribution for the topic and this what we had seen also in the previous ah so when we are doing it on the paper in in the previous lecture now this will give you the probability distribution over all the topics given this stroke now what is the next thing you have this multinomial distribution you sample a topic from here so these the whole algorithm in a nutshell so is how do you start so each word token is assigned to a pre defined so random topic so a random topic in one to two so we have this whole collection and you assign some random topic random topic to each word now you you compute your two matrices c d t and c w t from that assignment now for each word token so in this situation what you will do for each word token you sample a new topic as per this distribution and you have seen that once you have the matrices formulated you can find out this probability distribution and you can sample a topic for sampling for a multinomial distribution and when you will sample a topic and put that topic in that in that ah for that word accordingly adjust your two matrices so now then you make complete single pass through all your words in your corpus that is called one gibb sample so this is your one gibb sample ok and then you will do it again and again and again so what happens is that initially for certain iterations you can call it as a burnin period initially burnin period so where you will not store those samples u use them to update the values but you will not store but after some word burnin period you will start storing this values now you will not store every conjugative values so what might happen because you are just using a previous values to compute the next one they may be very very correlated so we will have some regularly spaced at some regularly expressed interval you will store this samples so something like ah so you are doing this iterations over the full ah corpus so there will be some initial burinin period so you are computing gibb sample ok after burnin in period you will you will have reliable gibb sample but what will happen those that are very very close they will be highly correlated so will say i will store some regularly space intervals so say i'll store it up every hundred after every hundred relations i will store this so we have we will have cube multiple gibb samples now so each sample contains all your z ok all your z all your assignment is consist con contain and then and from here you can compute your beta and ah theta and then finally you can take an ah expectation over all these values and this will give your ah one particular ah approximation of a parameters if we take a expectation over various ah samples that you are getting from from gibb sample so once you have this z how do you compute your ah betas and theta that is very easy we were actually using that to compute z so betas are the probability that a topic j is assigned to the i th word so this will be from the matrix c w t so i will take c i j w t plus eta divide over all ah words c k j w t where is w eta similarly what is theta j d that is what is probability of topic j under document d it will be c d j d t plus alpha divide by summation over all topics c d k d t plus t alpha so once we have the z then we can compute here ah beta and theta also so yeah this values will correspond to the distribution of sampling a new token of word i from topic j and sampling a new token in document d from topic j now o it's just an example to explain what what it means to use gibb sampling so what is an example so this is like so we are taking in a artificial data and for a known topic model and applying the algorithm will check if we can come back to the same topic ah same ah topic distribution that we started with so what is done here let us say we have two topics so we are doing a generation now and then we will see whether gibbs sampling can infer back the original ah topic distributions so what is done in in genetic part let us say i have two topics topic one topic two and simply we are saying topic one assigns equal probability to three words money loan and bank and topic two assigns equal probability word river steam and bank so all these three are assigned the probability of one by three each fine so these are topic distributions now by arbitrary mixture of these two topics we are generating sixteen documents so thats how these sixteen documents look like so black means topic one and white means topic ah two ok so there are two topics so each document has some number of words from different topics so i am sorry you should initial look at only the number of ah number of balls that is how many words are there in the document now to apply gibb sampling so that is why you generated all these documents so this makes sense that you are generating documents that are having bank money loan lot of documents there is some document that contain only river stream bank so this are documents from only one topic these are document only from another topic and there are some documents that are mix in these two topics so that's how you're doing generating sixteen documents now your task is can you use gibbs sampling to find out what are the two topic distributions here so what is done for that initialize all the words to some topic so randomly that's why you're seeing the random assignments so black is topic one and white is topic two some random assignment from this random assignment you can have the two matrices c w t c d t and then what you will do in each iteration you will go to each word find out what is the probability that this word will be assigned to topics j sample from the distribution update a matrices and you get some gibb samples so once you do that so this is what you see after sixty four iterations of gibbs sampling can you see that it it actually ah ah looks very very in ah close to what we had initially so all this words are assigned to topic one and all these words are assigned to the next topic ok and you can see that bank money loan are been assigned the same topic in you say in a in a given document and that makes co ah lot of sense and from this ah particular sample you can also compute your betas and if you compute the betas they come out to be very very close to what you started with so started with each ah word having a probability of one by three and that's what is roughly what we obtain and this is just an explanation that how gibbs sampling can help you to recover what is a original topic distribution this is from artificial data but now you can do that for any real corpus we have the real corpus and we want to find out what is the topic distributions apply gibbs sampling and find out that so there are various tool kits available that i also disused in last lecture so where you can give a corpus you can define the number of topics and they can give you the the all this values what are theta what are betas what are vapor topic per per document per word topic distribution now once you have that what are sort of simple ah tasks that you can do with this so for example one very important task is can you compute similarity between two documents and how will you do that so suppose i have two document given in d two to compute the similarity between that i will see what is the topic distributions for d one and d two so so how if they are similar i will say that the two documents are similar but if their topic distributions are different i will say they are different plus so i have two documents d one and t two i find out what is theta for d one what is theta for d two now i can say distance between ah two distribution p q i can use the k l divergence summation p i log p i by q i so now i compute the k l divergence between the two topic distributions for d one and d two and that will get me how what is the distance between the two documents that is one every standard measure for finding out how how similar two documents are because this is asymmetric you can also do something like one by two d p q plus d q p so we can also use that as some sort of distance symmetric so that is you have now the corpus you find know the topic distributions now you can use that to compare the similarity between any pair of documents so that is one very very important application of topic models then you can find out what is similarity of ah the document with reciprocal query ah what is the probability that query is generated from a document that is what we study in information three rate that you have a lot of documents you want to find out given a query which document should be ah ranked higher this will be ah computed using what is the probability that the query is generated from this document so whichever document give the highest probability of generating the query is given the highest ah score now whats the formulation you want to find out probability of query given the document the query is nothing but a set of words so if we take it simply easily multiplication over all the words in my query probability w k given d i now how do i compute probability of word given d i there i am using the l d a ah the l d a model so i will say so i will marginalize it over all the topics so this will be covered all words in query summation over all topics probability w t given topic is j probability topic is j given document d ok this is nothing that comes from your beta directly that comes here from your theta directly and use that to find out what is the probability of this query given this document this is again a very interesting use then you can also use it to find out these two words are similar what is the probability of word w two given w one that is nothing but again you marginalize over all the topics and this you will-- compute from ah either your beta or by agent based here so this will give me given a word w one what are some of the likely words in my vocabulary and this you can find from computing words similarity and all that you have to using doing using distributional similarity and and other stuff so this is that's thy this is i given a nice method for capturing ah semantics between words documents even sentences so ah how do you validate at this words so that's we saw a simple experiment see at the word play by using topic model find out which words have the highest probability given the word play so probability of x given play find out some top top words then you ask some humans who say that when you hear the word play what words come into your mind that is humans so words like fun ball game work ground mate child etcetera they come to their mind then you try to see whether these two lists are similar and you find that many words are similar like ball game they come on top even in topic model so this is the interest interesting way of ah evaluating whether a model is doing well for capturing words in that so ok so we we discussed how do we use gibbs sampling to to estimate these parameters and some simple applications next we will also talk about some non para parametric base model and what are the different applications they can be used for thank you