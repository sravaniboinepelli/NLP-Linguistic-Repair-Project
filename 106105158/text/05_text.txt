 hello everyone welcome back to the final lecture of the first week so in the last lecture we were discussing about various empirical laws so in particular zipfs law and heaps law that how the whats the vocabulary are distributed in a corpus ok and we say that the distribution is not very uniform there was certain words that are very very common so we saw that roughly hundred hundred words in the vocabulary made for fifty percent of the corpus that by the time mean that number of tokens and on the other hand there are fifty percent who have words in the in the vocabulary that occur only once ok and we discussed whatever various relationships from among the numb the my vocabulary size and the number of tokens that i observe in a corpus and also how they grow with respect to each other and zipfs law gives gave me a relation between the frequency and the rank of a word so today in this lecture we will start with the basic key processing in in language so we will cover the the basic concepts and what are the challenges that one might face while doing the processing ok so we are going to the basics of text processing ok so we will start with the problem of tokenization as the name would suggest ok remember the name token token is an individual word in my corpus so now what happens when i am preprocessing the text in given in any language what i will face is a string of characters the sequence of characters now i need to indentify what are all the different words that are there in this in this sequence so now tokenization is a process by which i convert the string of characters into sequence of various words so i am trying to segment it by the various words that i am observing ok so now before going into what is tokenization i will just talk about a slightly related problem sentence segmentation ok so this you may or may not have to do always and it depends on what is your application so for example suppose you are doing classification for the whole document in to certain classes you might not have to go to the individual sentence and you can just talk about what are the various words that are present in in this document on the other hand suppose you are trying to find out what are the important sentences in this document in that application you will have to go to the individual sentence so now if you have to go to the individual sen sentence the first task that you will face is how do i segment these whole document into a sequence of sentences ok so this is sentence one one sentence two and so on and this task is called sentence segmentation so now you might feel that this is very trivial task but let us see is it trivial ok so what is sentence segmentation it's a problem of deciding where my sentence begins and ends so that i have a complete unit of words that that i call as a sentence now do you think there might be certain challenges involved suppose i am talking about the language english can i always say that wherever i have a dot it is the end of the sentence let us see so there are many ways in which i can end my sentence so i can have exclamation or question mark that ends the sentence and they are mostly unambiguous so whenever i have a exclamation or question mark i can say probably this is the end of the sentence but is the case the same with a dot so i can think of a scenario where i have a dot in english and but its not the end of the sentence so we can find all sorts of abbreviations right they end with a period like doctor mister mph ok so you have three dots here so you cannot each of there this as the end of your sentence so again you have numbers two point four four point three and so on so that means the problem of deciding whether a particular dot is the end of the sentence or not is not entirely trivial so i need to build certain algorithm for finding out is it my end of the sentence ok so in in text processing we we face this kind of problem in nearly every every simple every simple task that we are doing ok so even if it looks a trivial task we face with this problem that ok can i always call dot as a end of the sentence so how do we go about solving this now if you think about it whenever i see a dot or question mark or exclamation i always have to decide one of the two things is it the end of the sentence or is not the end of the sentence ok so any data point that i am seeing i have to divide into one of these two classes ok so if you call think of these as two classes end of the sentence or not end of the sentence so each point you have to divide into one of the two classes and this in general this problem in general is called classification problem ok you are classifying into one of the two classes now so the idea is very very sim simple so you have two classes and each data point you have to divide into one of the two classes so that means you have to build some sort of plural algorithm for doing that so in this case i have to build the binary classifier what they mean by a binary classifier there are two classes end of the sentence or not end of the sentence in general there can be multiple classes so now for each dot or in general for every word i need to decide whether this is the end of the sentence or not the end of the sentence so in general my classifica[tion]- classifiers that i will build can be some rules that i write by hand ok some simple if the nice rules or it can be some expressions i say my particular example matches with this set of expressions it is one plus if i doesn't match it is a class or i can build a machine learning classifier so in this particular scenario what can be the simplest thing to do let us see can we build a simple rule based classifier so so we we will start with a example of a simple decision tree so by decision tree i mean a set of if and i say statements ok so i am at a particular word i want to decide whether this is the end of the sentence or not ok ok so i can have the simple if then else kind of decision tree here so met a word and i the first thing i check is are there lots of blank lines after that so this would happen in a text whenever this is the end of the a paragraph and there are some blank lines so if you i feel that there are a lot of blank lines after after me that means after this word i may say ok this might be the end of the sentence with a good confidence so that's why the branch here says yes this is the end of the sentence but suppose there are not lot of blank blank lines then i will check if the final punctuation is a question mark exclamation or a colon in that case ok so there are quite unambiguous and i may say this is the end of the sentence now suppose it is not then i will check if the final punctuation is a period so if it is a period if it is not a period this is easy easy to say that this is not the end of the sentence but suppose this is a end of this is a period so again i cannot say for certain if it is the end of the sentence so i give a again check for simplicity i might have a list of abbreviations and i can check if the word that i am correcting facing is one of the abbreviations in my list if it is there i say ok this is not end of the sentence if it is so here i am etcetera or any other abbreviation if the answer is yes i am not end of the sentence if the answer is no that means this word is not an abbreviation and this will be the end of the sentence this is very very simple if then else rule rules ok this may not be correct but this is one particular ah way in which this problem can be solved in general you might want to use some other sort of indications we call them as various features these are various observations that you make from your corpus ok so what are some examples ok so suppose i see the word that is ending with dot can i use this as a feature whether my word starts with an upper case lower case cap all caps or is it number ok how will that help so let us see i have here i am here and my word is four point three so i am at dot i want to find out if it is the end of the sentence if i can say that the previous the current word is a number it's an high probability that this will be in number and it will not be the end of the sentence so this can be used as an another feature ok so again by feature you can think of a simple rule whether the word i am currently at is a number ok or i can use the fact where as the case of the word with dot its upper case or lower case so you so what happens generally in abbreviations we are mostly in upper case so suppose i have doctor and it starts with an upper case ok i can say that this might be an application ok saying with the lower case lower case will give me more probability that this is not an abbreviation similarly i can also use in the case of the word after dot ok so is it upper case lower case capital or number so how will that help so again whenever i have the end of the sentence the next word in general starts with a capital so again this can be used what can be some other features so i can have some numerical features so that is i will have certain thresholds what is the length of the word ending with dot is it if the length is small it might be an abbreviation if the length is larger it might not be an abbreviation ok and i can also use probably the what is the probability that the word that is ending with dot occurs at the end of the sentence ok so if it is really the end of the sentence it might happen then that in a large corpus this end sentence quite often same thing i I can do with the next word after dot is it the start of the sentence what is the probability that it occurs in the start of the sentence in a large corpus ok so you might be able to use any of these features to decide given a particular word is it the end of the sentence or not ok so now suppose i ask you this question do you have the same problem in other languages like hindi so in hindi you will see that in general there is only agenda that you use to indicate the end of the sentence and this is not used for any other purpose so this problem you will see is again language depen dependent this problem is there for english but not so for hindi but you will see there are other problems that do no exist for english language but are there for other indian languages ok we will see some of the those examples in the same branch ok now so how do we implement a decision tree so as you have seen this is simple if then else statement ok so now what is important is that you choose the correct set of features so how do you go about choosing the set of features we see in your from your data what are some observations that can separate my two classes here so my two classes here are end of the sentence and non the end of the sentence and what are the observations we were having ok in general it might be an abbreviation in the case of the word and that is before the dot maybe upper case or lower case and one of these might indicate one plus the other might indicate other plus so all these are my observations that i use as my features now whenever i am using a numerical features like the length of the word before dot i need to pick some sort of threshold ok that is whether the length of the word is between two to three or say more than three between five to seven like that so my tree can be if the length of the word is between five to seven i could one plus otherwise i could another another plus ok so now here is one problem suppose i keep on increasing my features it can be both ah numerical or non numerical features it might be difficult to set up my if then else rules by hand so in that scenario i can try to use some sort of machine learning technique to learn this decision tree ok in in the literature there are lot of such ah algorithms available that given a data and a set of features we will construct a decision tree for you ok so i will just give you so the names of some of the algorithms and the basic idea on this they work is that so at every point we have to choose a particular subject ok so you have to choose a feature value that it splits my data into certain parts and i have certain criteria to find out what is the best way to split so one particular criteria is what is the information given by this ok so these algorithm that we have mentioned here like i d three c four point five point cart they all use one of these criterions ok in general once you have identified what are your interesting features for these task you are not limited to only one classifier a decision tree you can also try out some other classifiers like support vector machines logistic regression and neural networks these all these are quite popular classifiers for various mbb applications so we will talk about some of these as we will go to some some advanced topics in this course ok now coming back to our problem tokenization ok we said that tokenization is a process of segmenting a string of charac characters into words finding out what are the different words in this question now remember we talked about token and type distinction suppose i give you a simple sentence here i have a can opener but i cant open these cans how many tokens are there if you count there are eleven different words eleven different occurrences of words so you have eleven word tokens but how many unique words are there so you will find there are only ten unique words ok which word repeats is the word i repeats twice so there are ten types and eleven tokens so my tokenization is to find out each of the eleven word tokens from the sentence in practice at least for english you can use certain toolkits that are available like nltk in python corenlp in java and you can you can also use the unix commands so in this course you will mainly be using nltk toolkit for doing all the pre processing task and in some other ah tasks as well ok but in general you can use any of these three ah possibilities so for english most of the the the problems that we will see are taken care of the tokenizers tokenizers that we have discussed previously ok but still it is good to know what are the challenges that are involved when i have i tried to design a tokenization algorithm ok see for example here you will see that it have if i encounter a word like finlands in my data so one question that i have is whether i treat it as simple finland as it is finlands or i I convert it to finlands by removing the apostrophe ok so this question you might also try to defer to the next processing step that you will see but sometimes you might want to tackle this in the same stuff ok similarly if you see what are do i treat it as a single token or two tokens what are this trouble you might have to solve in the same step whether i treat it as a single token or multiple tokens same with i am shouldn't and so on similarly whenever your name end at each like san francisco should i treat it as a single token or two separate tokens now remember when we were talking about some of the cases why and page hard so you might have to find out that this particular sequence of tokens is a single entity and treat it as a single entity not as multiple different tokens ok so this problem is related similarly if you find m dot p dot h whether you call it a single token or multiple tokens so now there are no fixed answers to to these and some of these might depend on what is the application for which you are doing this pre processing ok but one thing you can always keep in mind suppose you are doing if for the application of information trivial if the same sort of steps that you apply for your documents should be applied to your query as well otherwise you will not be able to match them perfectly ok so suppose if i am using it for information trivial so i should use the same convention for both both my documents as well as the queries ok so then another problem can be how do i handle hyphens in my day ok this looks again a simple problem but we we will see its not that simple so let us see some kind of examples what are the various sorts of hypens that can be there in my corpus so here i have a sentence from a research paper abstract and the sentence says this paper describes mimic an adaptive mixed initiative spoken dialogue system that provides movie show time information ok so in this sentence itself you see two different hyphens one is with initiative initiative another is show hyphen time ok so now can you see that these two are different hyphens the first hyphen is not in general that i will i will use in in my text ok second hyphen i can used in my text i can write show time with an hyphen but how did this hyphen initiative came into the corpus ok so re so we have given this a title end of line hyphen so what happens in research papers for example whenever you write a sentence you might have to do some sort of justification and that's where you end the line even if is not the end of this of of the word so you will you will end up with an hyphen ok so now when you are trying to pre process and when you are retrieving such kind of hyphens you might have to join these together and we should we have to say that this is a single word initiative and not initial hyphen tive ok but again this is this is not trivial because for show time you will not do the same show time you might want to keep it as it is then there are some other kind of hyphens like lexical hyphens so you might have these hyphens with ah various prefixes like co pre meta multi etcetera sometimes they are sententially determined hyphen hyphens also that is they put so that it becomes easier to interpret these angles like here case based hand delivered etcetera are optional similarly if you see in the next sentence three to five year direct mark marketing plan ok three to five year can be written perfectly without keeping the hyphens but here you are putting it so that it becomes easier to interpret that particular occurrence so again when you are doing tokenization your problem that how do i handle all these hyphens further there are various issues that might that you might face for certain languages but not others so for an example like in french if you have a token like lensemble so you might want to match it with ensemble ok so that might be a similar problem that we are facing in english but let us take something in german ok so i have this i have this big sentence here ok but the problem is that this is not a single word this is a compound composed of four different words and the corresponding english meaning is this one so your four words in in english so when you are putting in in french they make a compound so now what is the problem that you will face when you are processing the a german text and you are trying to tokenize it so you might want to find out what are the individual ah words in this particular compound so you need some sort of compound split up for example ok so this problem is there for german not so much for english ok so now what happens if i am making a language like chinese or japanese ok so here is a sentence in chinese so what do you see in chinese words are written without any spaces in between ok so now when you are doing the pre processing your task is to find out what are the individual word tokens in these chinese sentence so this problem is also difficult because in general for a given utterance of a sequence of characters there might be more than one possible ways of breaking into sequence of words and both might be perfectly valid possibilities ok so in chinese we will not have not have any space between words and i have to find out what are the places where i have to break each ah words and this problem is called tokenization word tokenization same problem happens with japenese and h[ere]- for the complications because they are are using four different steps like katakana hiragana kanji and romaji so these problems becomes a bit more serious now the same problem is there even for sanskrit ok so if some of you have taken a sanskrit course in in your class eighth or tenth so you might be familiar with the the rules of sandhians in sanskrit language ok so that is it this is a simple single sentence in sanskrit but this is a huge this looks like a sing single word it is not a single word it is composed of multiple words in sanskrit and they are combined with a sandi relation ok in this is stands for pro proverb in nice proverb in sanskrit that translates in english as one should tell the truth one should say kind words one should neither tell harsh truths not flat flattering lies this is a rule for all times this is a this is a proverb and this is a single sentence that talks about this proverb but there all the words are combined with sandhi relation so if we try to undo the sandhi this is what you will find at the segmented text ok so there are multiple words in this in this sentence they are combined to make a si make a single it looks like a single word now so this problem we saw in chinese japenese and sanskrit but in sanskrit the problem is slightly more complicated and why is that so in in japense and and in chinese when you try to combine various words together you simply concatenate them you put them one after another without without making any changes at the boundary it doesnt happen in sanskrit when you combine two words you also make certain changes at the boundary and this is called the sandhi operation ok so in this particular case since see here i have the word bruyat and the word na but when i am combining i am i am writing it bruyanna so you see here the the the letter t gets changed to n ok so that means when i am trying to analyze the sentence so this particular sentence in sanskrit i need to find out not only what are the breaks but what is the corresponding word from which this sentence you derived so from here to find out the the actual sen words are bruyat lesna that gives give me this bruyat and this is very very common in sanskrit that you are always combining words by doing a sandhi operation so this further complicates my problem of word segments word tokenization or segmentation ok so this is just a list from wikipedia what are the longest words in various languages then note this sentence is about the words you see in sanskrit the longest word is composed of four thirty one characters is a compound and then you have greek and afrikaans and and other languages in english you will see that the longest word is of forty five characters is non scientific so what is the the particular word in in sanskrit that is composed of four thirty one letters so this was from the varadambika parinaya campu by tirumalamba ok this is a single compound from his from his book ok so now when i talk about this problem of tokenization in sanskrit or in english this problem is also called word segmentation have a sequence of characters and you segment it to find out individual words now what is the simplest algorithm that you can think of let lets take as in the case of chinese ok so the simplest algorithm that works is a greedy algorithm that is called maximum matching algorithm so whenever you are given a string you start you point to it at the beginning of the string now suppose that you have the dictionary and the words that you are that you are currently seeing all should be in the in the dictionary so you will find out what is the maximum match as per my dictionary in the string you break there and put the pointer from at the next character and again do the same thing so so this greedily chooses what are actual words by taking the maximum matches and this works nicely for most of the cases ok now so so this related question now can you think of some cases where the segmentation will also be required for the english text in english in general we do not combine words to make a single single word ok we do not do that but what is the scenario where we we are doing that right now ok so does do hash tags come into mind so for example suppose i have hash tags like thank sachin and music monday so here different words are combined together without putting a boundary in between so if you are given a hash tag and you have to analyze that you have to actually segmented in into various words ok now so when i talk about sanskrit so so so this we have a segment to available at the site sanksrit dot inria dot fr so we will just briefly see what is the design principle of building a segmentor in sanskrit so first we have a geometry model that says how do i generate a sentence in sanskrit i have a finite alphabet sigma ok that means a set a set of various characters in sanskrit now from this finite alphabet i can generate a lot words ok that are composed of various number of ah phonemes or all letters from this alphabet now when i have a set of words i can now combine them together with an operation of sandhi thats what i mean by sigma star here here ok so w star here so i have a set of words w and i will do a kleene closure that means i can combine any number of words together but whenever i am combining words i am doing them by a sandhi operation this is the relation between the words so so i have my set of inflected words also called padas in sanskrit and i have the relation of sandhi between them and thats how i generate sentences but the problem is how do i analyze them so that is the inverse problem that is whenever i am given a sentence w i have to analyze it by inverting the relations of sandhi so that i can produce a finite set of word forms w one to w n ok and i am saying together with the proofs so that is a formal way of saying that but what i mean is that w one to w n whenever they combine by sandhi operation they give me the actual sandhis the initial sandhis ok so thats how the segment is segment is built now this is a snapshot from from the segmentor so i gave the same sentence there and and it gave me all the possible ways of analyzing the sandhis and it it says that there are one twenty different solutions ok so here whenever i have bruyana so you see there are two possibilities bruyat and bruyam thats ok like that it gives me all the possible ways in which this sentence can be broken into individual word tokens now this is another problem that i will have to find out what is the most likely word sequence among all these one twenty possibilities but we can use many many different models that we will not talk talk about in this lecture probably in some some other lectures ok so coming back to normalization so so we talked about this problem that the same word might be doing multiple different ways like u dot s dot a versus usa now i should be able to match them together ok especially if you are doing information retrieval we are giving a query and you are retrieving from some document suppose your query contains u dot s dot a if the document contains usa if you are only doing the surface able match you will not be able to map on to each other so that so so you will have to consider this problem in advance and do the pre processing accordingly of either your documents or the query but using the same sort same sentence so what i what we are doing by this we are defining some sort of equivalence classes we are saying usa and u dot s dot a should go to one class and the other same type we also do some sort of case folding that is we can reduce all the letters to lower case so whenever i have the word like ah w o r d i will always write small w o r d so that whenever even if it is starting the sentence and it occurs in capitals because of that in general i know that this is a word word w o r d but this is not a generic rule sometimes depending on application you might have certain exceptions for example you might put treat the name and it is separately so if you have a entity general motors you might want to keep it as it is without case folding similarly you might want to keep us fortunate districts in upper case and not do the case folding and this is important for the application of machine transition also because if you do a case folding here you will know u s in lower case that means something else versus u s that is in united states excuse me we also have the problem of lemmatization that is you have individual individual words like am are is and you want to convert them to their lemma that means what is the base form from which they are derived similarly car cars cars cars so all these are derived from car so again this is some sort of normalization we are saying all these are some sort of equivalence class because they come from the same word from so in the the problem of lemmatization is that you have to find out the actual dictionary head word from which they have derived ok and for that we use morphology ok so what is morphology i am trying to find out the structure of word by seeing what is the particular stem the headword and what is the affix that is applied to it ok so these individual units are called various morphemes ok so so you have a stems that are the hybrids and the affixes that are what are the different units like s for plural etcetera you are applying to them to make the individual word ok so my examples are like for prefix you have un anti etcetera for english and a ati pra etcetera for hindi or sanskrit suffix like ity ation etcetera and taa ka ke etcetera for hindi and in general you can also have some infix like you have the word like vid and you can infix n in between this is in sanskrit so we will discuss in detail about it in morphology later so so there is another concept you have lemmatization where you are finding the actual dictionary headword so there is also a concept called stemming where you do not try to find the actual dictionary headword but you just try to remove certain ah suffixes ok and you opt whatever you obtain is called a stem so this a crude chopping of various affixes in that in that word ok so this is again language dependent so what we are doing here words like automate automatic automation all will be reduced to a single ah lemma automatically so this is stemming so you know the actual lemma is automate with an e but here so i am just chopping off the affixes at the end so i am removing here this ic ion all and putting it to automate ok so this is one example so if you try to do a stemming here see you will find from example e is removed from compressed it is removed and so on so what is the algorithm that is used for for this stemming so we have the porters algorithm that is very very famous and this is again some since set of if then else rules ok so what are some examples here so what is the first step i take a word if it ends with sses i remove es from there and i end with ss so example is caresses goes to caress if not then i see whether the words end with ies i put it to i like ponies goes to poni ok if not i see if the word ends with ss i keep it as ss if not i see if the word ends with s i remove that s ok cats goes goes to cat but k caress does not go to caress with with only one s because this is step comes before if there is a double s and in the word i re written it otherwise if there is a single s i remove it that like that there are some other steps so if there is a vowel in the sen in the in my word and the word ends with ing i remove ing so walking goes to walk but what about king you see in k there is no vowel so king will be writtened as it is same is a vowel and there is an ed i remove this ed and i have this word played to play so you can see that vo what is the use of this heuristic of having this vowel if you didn't have this vowel you would have converted king to k ok and like that there are some other ways like if the word ends with ational then i will put it put ate so rational so relational to relate and if the words end with word ends with izer i convert i remove that r digitizer to digitize ator to ate and if the word ends with al i remove that al if the word ends with able i remove that able if the word ends with ate i remove that ate ok so like that these are some steps that i take from my corpus from each word i I converted to its step ok it does not give me the correct dictionary headword but still this this is a good practice in principle for information retrieval ok if you want to match the query with the documents so this is for this week so next week we will start with another pre processing task that is a spelling correction ok thank you