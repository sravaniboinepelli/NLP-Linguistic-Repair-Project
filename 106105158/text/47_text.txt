 so hello everyone welcome back to the we week ten of this course so we have been doing a lot of basics in this course and last week we finished our discussions on semantics and during semantics we also talked about various applications where you can use topic models and other ah things like distribution semantics and semantics so starting from this week we will focus mainly on applications so you will use both the basics that we have covered and some new methods for solving very specific tasks and for this course we have chosen the task that are very very popular in the field of n l p and text panel so this week we will start with ah so we will start the first two lectures was entity linking and then we will go towards information extraction so today we are starting with entity linking so so what is entity linking as such what is the problem there so we we were discussing sometimes in in in the starting and in some lectures that when we ah encounter text data lot of entities are mentioned there and it would be helpful for a task if you what are the different entities that are used in this particular text data now there are various knowledge basis and resources where you have a good list of all these entities and some descriptions of these available so suppose you can find out from a text what are the important entities here and if you can link them to a knowledge base then you can do a lot of further inferences over these for example this entity is a person and there in the in the database you will have ma lot of information about this and you can make use of all these information so this as if you are having some background knowledge about various entities given a text data when you encounter the entity you try to link it to the knowledge base and then you can extract the background knowledge about it and use that for different task in this particular data so this is a problem of entity linking so we can define it in this manner that entity linking is the task of identifying all mentions in text of a specific entity from a database or in ah ontology so what we are assuming here we have a database or we can also call it knowledge base ontology where i know what are the different entities that i that i need and with that inform with that entity i will have sa some different sort of information ah for example in wikipedia think of all the wikipedia pages you have you can call you can think of all these wikipedia pages at the entity pages and you have lot of information about the entities in each page so this is my knowledge base now when you encounter a text there your problem is find out if a particular ah n gram or a sequence of phrases sequence of words together correspond to an entity in the in the wikipedia and if so then link it to that wikipedia page and this is ah the overall i idea of entity linking problem so ah lot of different databases can be used for example researchers have used wikipedia lot then yago freebase etcetera and the task of entity linking when you are doing you can break it into two different phases so one phase would be you find out from the text what are the appropriate an candidates or entities that should be ah linked so this is called the entity ah mention detection part identify what are the mentions of entities that should be linked to the database and once you found out what are the important entities the next one would be to appropriately link it to the database that's the second from second part reference disambiguation or entity resolution now why would that be a problem see same as we discussed in the case of words is disambiguation with the same entity name there might be different reference ok so so for example newyork it can be newyork city and there might be movie with the name newyork there might be t v serial with the name newyork so when in a text you have a mention of newyork you want to know whether you want to link it to the newyork city or the film or t v series or something else ok so there is a problem of ah disambiguation here and this is also to be handled when we are solving the problem of entity linking find out what are the appropriate entities then appropriately link them to their ah entries in the database so what are the things your challenge is that one needs to handle in this problem so one is mean variations so the same entity can be written in many different ways ok so for example simple things like hilary clinton so in a in a text you can all you might tried it with only the name clinton with the last name clinton ok or maybe there is a middle name also involved here so you can write it in different different manners so your problem is you have to you have to handle all these name variations and all these should map to the appropriate entity in the knowledge base and then the second challenge is entity ambiguity that is the same string can refer to more than one entities so this is also we discussed newyork can refer to multiple entities so both these challenges are to be handled in the problem of entity linking so now ah for for this course what we will do we will take one particular database that is wikipedia as our ah base database so so we will always link a text to wikipedia so this will be our database by default for this lecture in the next lecture but in general you can use any other database and you can accordingly modify your approaches for that so one particular terminology so if you are using wikipedia as your knowledge base then this task of entity linking is called wikification or wikifying so you are taking a text and you are trying to wikify that so that means find out the entities that are important and then linked them to ah their appropriate wikipedia pages that's the problem of wikification so now let us going detail about this problem what are the different ah processing involved and what are the different techniques you can used so here is one example of ah entity linking or wikification as such so on the top what you are seen you are seeing a research article ok from physics domain and you are having ah so this like an abstract here so you are having text such as degeneracy is removed due to a geometric gradient onto a metasurface and so on so there is a text involved here now what what is the task you are trying to wikify that so that means you are trying to identify what are the encap ah important entities and what are their ah pages in wikipedia so if you see on the bottom so you are you are finding various links so spin optics so where optics is linked to some some different page an an example is from a degeneracy so link so this is linked to degenerate energy levels that is a page in wikipedia and you can also find out some specific content about that page if you just ah take your mouse over there and that is being done to many different words here optics control light photon helicity angular momentum and so on so that means these words are the appropriate mentions and they are then linked to their wikipedia pages so this is the process of wikification similarly here you are seeing a news article and so a in the article so with the news you are seeing various words are ah hyperlinks so there you can click this word and go to their appropriate wikipedia page also for example here in baghdad so it will open the wikipedia description of the word baghdad and you might just want to read the summary or if you want to know more you can click on this opening wikipedia and go to the wikipedia page so now so so we can also see why this might be very important application so so you can you can talk about reading news articles or scientific articles also any other text it can be tweet text so when you reading the article there might be many different terms that are very very specific domain and if you are not in expert in the domain you will not know what these terms stand for so then what you would do you will take this terms and try to search in the dictionary or some on google or some or may be on wikipedia and that will require a lot of time from your side to fully understand that article so what is wikification doing so it is helping you in that given a text it will automatically identify what are the important entities and it will link the wikipedia ah pages so it will avoid it will do all the tasks for you and you can even see the description in the same page or you can go to the wikipedia page and read more about it so that helps a lot in doing ah in in enhancing your reading for a certain ah news article or scientific article additionally it can also help if you are pla planning two certain task on that text ok you fo your own to get some semantics from the text then also having this knowledge that this entity corresponds to this wikipedia entity might ah help you in getting some knowledge from the wikipedia page and take it as a feature for your task so this is also one other application for this wikification so we have seen for scientific articles and news articles you can also do it for very short text like tweets so tweets have very little context so here you are seeing with four different tweets go gators and here the problem is what does this gator refer to so in wikipedia there are two different mentions that are possible florida gators football of florida gators mens basketball in this particular context it refers to florida gators mens basketball and you want to link it to that particular entity similarly here a stay up hawk fans we are going through a slump now but we have to stay positive go hawks so here you have entities like fans slump and hawks and they need to be linked to appropriate entities again you are seeing their multiple possibilities and you need to find out what is the appropriate mention wikipedia same with the other examples that you are seeing here so you can do it for also for short text like tweets with tweets there is a very little information and you want to find out what is the appropriate entity that this tweet links to so and you can also think of many other application on your own where this entity linking is important so now how is actually done so for that let us try to understand what are the different phrases again what do we need to ah do systematically so what are the common steps so first step is it will determine what are the linkable phrases and this step is also called mention detection that is from the text find out what phrases what n grams are to be linked for example we were seen words like baghdad and here gators what the what the mentions that word to be linked to the ah knowledge base and this is this is this approach for detecting these words is called mention detection now once we have found out what are the appropriate mentions for to be linked then what will be the next step next step would be you have to identify what are the possible candidates to which it can be linked ah right like in the previous slide we were seen gators can link can link to two different ah entries so identify what are the possible entries and this will call this is called the link generation part so you have to select what are the candidate entity links and what are the ah all the links you have to list somehow this is called link generation part now once you know what are all the links then what will be the next step you have to find out what is the most appropriate link for all the all these set and this will call the ah disambiguation part so this is use the context to disambiguate what is the appropriate link it this entity should be link to and you might also want to filter you might want to improve your whole task and we will see some examples for all these how do you filter and improve your task based on this so these are three main steps sometimes you might combine the first two steps also that is when you are detecting the wan mentions you also finding the candidates at the same time so that is also possible so now so let us let us see these steps again in the context of wikification so so you are having a text where you have this sentence degeneracy is removed ok and there are some words before and after so what is mention detection find out that the word degeneracy is to be linked is the appropriate mention so that's in green in this slide so that is a mention detection part then second part would be link generation find out all the appropriate all the possible phrases pages in the wiki ah in my database so here you can see the degeneracy ah occurs in mathematics in biology in graph theory and degenerate energy levels so there are four possible links so then you have a task of disambiguation that among the four links what should be the appropriate ah ah page wa to which this entity should be linked and that will be the third step and that is the disambiguation and you will say ok this is the fourth one degenerate energy levels is the appropriate ah entity page for my for my mention of degeneracy and these are three steps for this entity linking now so we might ah like to understand what are the some of the vasi basic features of wikipedia that can help us in designing an algorithm for wikification so wikipedia all of you know about wikipedia and you have been reading wikipedia for many of your ah for for knowing different concepts and all so so what do you seen wikipedia there is a page like that there is a title and certain texts about the page and you see there are various links also so there is an article then additionally they can be some redirect pages so you might have come across you are searching for something in wikipedia and it it redirects you to some other page in wikipedia so these are also ill ah lot of redirect pages in wikipedia then there are disambiguation pages we will see an example in the next slide then there are category template pages that allocate what are the different categories in wikipedia this category what are the subcategories and then there are some admin pages now what is important for our task is that there are lot of hyperlinks in wikipedia ok so what hyperlinks in wikipedia so different words and phrases are linked within the wikipedia itself so we will see that in wikipedia article certain concepts have a hyperlink and you click on the hyper hyperlink and you will go to the corresponding wikipedia page so there are lot of in links and out links that are going on within wikipedia so ah so united states for example whenever you have a ah phrase like united states you may have a link saying it is linking to the united states t v serial or american t v show etcetera so you will find if you will see the source this will be like that of the hyperlink so you will have a ah double parenthesis to denote what is the appropriate entity inside the source and that if you see the source wise you can find out and these are various hyperlinks that you have in wikipedia then you have a lot of disambiguation pages for some entities where a lot of different mentions are available you might have also categories in disambiguation that in the in the category of entertainment what are the possibilities in the in the category of politics what are the possibilities and so on so like here the you have for the entity newyork disambiguation page so you will have h places what are the possibilities then media entertain entertainment and you will see that in each category there are lot of entities that correspond to this ah the main entity newyork so you will see in in these cases is the same single entity can map to may be fifteen twenty different pages in wikipedia and they are nicely categorized in this disambiguation page categories may or may not be there in various pages so so what are so what so what do we need to about the whole architecture lot of pages in wikipedia and each page has some hm name that will be the identify you may be the identifier then lot of text involved in the text you have various hyperlinks where different phrases are linked to their own wikipedia pages and some entities will have their own disambiguation page where you will find out what are the different ah ways in which this entity can be used this maybe also under various categories so now so once you know about that so let's say some small statistics so that is we talked about word net for sense disambiguation sa we given a sentence we want to find out each word what is the appropriate sense in wordnet it corresponds to so in wordnet how many entities we had we had roughly eighty k eighty thousand different entity definitions and one hundred forty two thousand different sen senses on the other hand wikipedia is much larger repository so in wikipedia overall there are four million entity defin definitions and this keeps on increasing and there are twenty four million different senses so is much much larger in in compassion to ah wordnet so our task is from all these twenty four million senses find out given a text what are the entities import that are important and what which of the sense they they correspond to so now let us see what are the simple measures that we can think of for the three steps or let us say only the two steps the mention detection and disambiguation mention detection that is in a text whether a given n gram is in appropriate mention or not so so what we will do initially we will see some sort of measures that can be taken simply by the wikipedia structure or wikipedia data so let us see so let's talk about this mention detection part whether a particular phrase is a good candidate for a mention so so what will be a good measure for this so so if you think about using wikipedia structures we can say that ok wikipedia has lot of pages suppose i find out this particular n gram how many times it occurs in wikipedia and among whatever times it occurs what fraction of times it is actually linked to something ok so so what is the idea if an if a word is linked much more number of times that means it might be a good candidate for mention if it is not linked many times that means this may not be a very good candidate and this a very simple criteria that you can used from wikipedia so in this is called the keyphraseness of a word or also a phrase so number of wikipedia articles that use it as an anchor divided by the number of articles that mention it at all so that means i will take a word w and i will find out what are all the wikipedia pages where it occurs ok so it occurs suppose in five [pedia/wikipedia] articles article one article two article three four and five and among the five articles say four and two provide a link with this w so where w occurs with the link to some wikipedia page and a four also this w occurs with the link to a wikipedia page but a one a three a five do not provide a link so here w occurs without a link ok so now so what is this keyphraseness so keyphraseness is what fraction of the page in wikipedia is it linked wherever it occurs so five linked to so keyphraseness will be two by five ok and that is a good measure in that it will tell me ok whenever encounter a new phrase w how many times it is actually linked in wikipedia and use that to detect if it is a good mention at all so this is a very simple measure so we will say ah how many times it occurs and among whatever time it occurs how many times it is linked to another wikipedia article now here we do not worry about whether it is linked always to the same wikipedia article or multiple wikipedia articles the only thing is it is linked to something then we will considerate in the numerator so this is the keyphraseness for a for a word now what can be a good measure for disambiguation so now let's again think about it can we use wikipedia to to find out a good measure for disambiguation so so what can be the simplest measure that you can think of so i have a word and it can correspond to multiple entities ah so for example so let's say ah i have a word w and in general it can link to three wikipedia pages a one a two a three all are possible reference for this wikipedia page now what can be a good baseline to find out what is an appropriate ah disambiguation page so for that i can again use wikipedia so i will see in the whole wikipedia whenever w is link to something so link to these a one a two a three what fraction of times it is link to a a one suppose it link to a one ninety percent of the time a two eight percent of the time and a three two percent of the time and this can be an a good measure to say ok ninety percent of times w links to the article a one so this so by default i will say ok w will link to a one so that can be one simple measure and this is ah called the commonness so this is commonness so i will define the commonness for a word and a concept concept here are three concept three wikipedia concepts and what is the definition so the fraction of times a particular sense is used as a destination in wikipedia so number of times word is link to c divided by number of times word is link to any c prime ok so like here it will be ninety divided by hundred suppose they are ninety eight and two pages so ninety divided by hundred is the ah commonness for w and a one eight by hundred is commonness for w a two and two by hundred is commonness for w and a three so this is another simple measure so so you have you have seen keyphraseness and commonness ok and they are simple measures it is direct from wikipedia so now let us see one example so here is one text that is like a report of a match and what you are seeing you are seeing words the they are coloured and colours depend on the kre keyphrasenesses score that is from zero to one so dark green is keyphraseness one that means it is always linked in wikipedia so here like bulgaria national football team is roughly always linked to wikipedia is has a high keyphraseness some words like here the knock out are not always linked they have a very low keyphraseness now what about the commonness so now you will take a particular entity like here germany and you will see what are the all candidate candidates like germany germany national football team nazi germany german empire similarly for world cup it can be fifa world cup f i s alpine ski world cup two thousand nine fina swimming world cup world cup mens golf etcetera these are a various can candidates and you are computing com commonness by seeing how many times this word is actually linked to these ah these entities divided by any number of time divided by the number of times it is actually linked and this gives you the commonness so germany is linked to the germany the word germany like ninety four percent of a time germany national football team one point three nine percent of the time and so on similarly for fifa world world cup so now from there you can choose by default the word the sense with the highest commonness so like nineteen ninety eight fifa world cup will come up here fifa world cup will come up here but they will a problem with this entity right the germany will written the word germany ninety four percent time but in this case the appropriate mention is germany national football team and it will not be able to detect this so so this is the idea about keyphraseness and commonness and clearly you can see if there is a one there is one problem with this approach ok so now so is it always the best decision to use either the only the commonness for linking their particular entity and ah so what do you say so from whatever we saw in the last page is it always the best deci decision to use commonness ah it cannot be right because what you are seeing whenever a word w occurs in any context ok i will always assign it to a one by default because it has the highest commonness so that means i have never using the context in which the word w occurs by default i am assigning it to the category or link a one so that means i will always make some mistakes right there will be some pages at least weight should link to a two or a three and in those cases i will link it to a one by default so i cannot design a very good system by this approach i will there is a like there is always the chance of maching making mistakes because you are taking the the the default case and this also corresponds to the ah to the to one of the baseline that you can use in words sense disambiguation that is you take sense of a word that is most ah probable sense that is like a baseline but this will never help you in designing a very good system because you are doing it independent of the context you are always linking it to this page so now what we will see in the next lecture is that can we also use the context to improve this method instead of using commonness can we use something from the context to find out ok among the three what should be the appropriate ah link for this particular entity so so what did we see so commonness and keyphraseness are simple measures they can help you to design a good baseline that will work ah most of the times but cannot help you build in accurate system because you will always give some wrong links and we can see why because there is a default to the most probable link ok and whenever the word is used in in not so probable links it can never be correctly assigned so we need to use the context and that's what we will see in the next lecture that how do we use the context from the word to disambiguate the links thank you