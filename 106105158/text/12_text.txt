 ok so welcome back for the third week of this course so in the last week where we ended you are discussing about language modeling we talked about the basis of language modeling then we talked about what ah how to be evaluate a language model and then we came to the topic of smoothing ok so what was the simple idea we discussed what is my advanced smoothing where i add one to each individual count and accordingly i do some normalization my denominate and we added by saying there are certain simple variance possible like at k i can also do the unigram prior smoothing by instead of adding uniform weight to each word i at a weight that is depending on the probability of that word unigram probability for that word and that we were arguing that might work better than simply adding uniform count to each of the bigrams there are we had certain parameters like what what will be a best way of choosing k a and m in different models ok so simple answer would be so you ah there is no unique value that you can choose suppose you have some held out data so we can try to find out for which value of k and m you are getting the best publicity for that held out data and that can be one possible way of choosing the values of k and m ok so now today we will talk we will go beyond the advanced smoothing and talk about some other advanced a smoothing methods ok so we will start with two different a smoothing methods called good turing smoothing and kneser ney smoothing so so what is the intuition behind using a good turing smoothing so intuition is essentially i want to find out what should be the probability for the words that i have not seen in my training data so what give turing is smoothing says ok so why don't you use the estimate of the things you have seen once in your training data you estimate about the things we have not seen similarly whatever you have seen twice use that estimate that things you have you will seen once and like that ok so this this this just tries to adjust the probability mass so that the things that was seen twice are used to find the probability for the things that was seen once things that was seen once are used to estimate the probability for the things that will not seen at all ok so so the idea is that you we have to use the count of things we have seen once to find the count of things we have never seen ok so for that let us give ah let us first define what is the frequency of frequency ok we used this one of the earlier lectures also so we take this three sentences ok i am here who am i i would like from this sentences i am trying to construct the frequency of frequency so that is how many words occurs once how many word occurs twice and so on so so if you see here the word i occurs tries m occurs twice and the other four words occurs once so what will my frequency of frequency i will find out how many of words occurs once with frequency one so that is my n one so there are four words here that occur only once so n is four now you will see how many words occur twice so how many words have a frequency of two so that will i will have one here n three is one so this is my frequency of frequency ok so four words occur once one word occurs twice one word occurs tries now how by use that in good turing estimate so what is the idea so this we discussed earlier so i want to reallocate the probability mass of n grams that occur r plus one times in the training data to the n grams that occur r times ok especially you want to see the n grams that occur one once to find the probability mass for the n grams that occur zero times but you that you do that in general ok so the n grams that occur r plus one times used that for r times ok so now in particular reallocate the probability mass for n grams that were seen once for those that were never seen so now we should see formally what will be the effective probability mass n count that we will get by this good turing estimate ok so let us see suppose i have a i want to find out four words that occurred c times words that occurs c times what will be the effective adjusted count after applying good turing estimation ok so now how we defined that so what would happened by training data i will have words that occur zero times words that occur one time let it be n zero n one words that occur twice n two and so on so n two numbers that occur twice similarly now there are n c number of words that occurs c times n c plus one words that occurs c plus one times ok now i want to find i want to use good turing to estimate the the count for these words that occur c times so what it be say we said we will use this probability mass and give it to these words now how to we distribute so what is the probability mass associated with the words that occurs c plus one times ok so now suppose there are n words or nth bigrams in total ok so now what is the probability mass for the bigrams that occurred or n gram we can say in general that occurred c plus one times you see how many n grams are there there are n c plus one different n grams so what will be the probability mass for them it will be n c plus one times c plus one divided by the total number of n grams that are n that is a probability mass yes and this i am distributing to the words that occur c times now how many words are there there are n c words so what is the probability that each of them will get divided by n c yes so now so this is the probability mass for that each of them will get now what will be the effective count remember how we define the effective count c star such that if i if i divided by n merely this will give me the same probability so that will tell me c star is n c plus one times c plus one divided by n c and that's what we have written here ok so n c in general is the n grams that ah seen exactly c terms in the corpus so by using this definition i can find out what is effective count for any word that is occur n c number of times in the corpus ok so that's what we have seen so this this is the probability for the words things let us in c times c star n yes c star is my nothing but my effective n gram count ok now what about the words that will not seen at all that are the words that that or whenever i am saying words here it means the n terms in general so what about the n grams that did not occur at all matter in data so frommy definition i will use the probability mass of the words that was seen once ok so what will be the probability mass so there are by definition there are n one words that was seen once so each one of them occurs once so what is the probability mass it is n one by n yes and this probability mass i will give to the words that was seen zero times yes now you might have a question how much of this each of them individual we will get so this is the probability mass that are the word the n grams are did not occurs in the corpus to get that we will get but how much of this individually each of them will get so if you can just divided by the number of n grams that did not occur in the corpus so we will see some by some example how we can be estimate this value ok so here if my count is zero so good turing estimate for the things that for the frequency c when c zero is n one by n that we just saw ok so that is a effective probability mass that i will give to the all the words or n grams they did not occur in the training data ok so this gives me nice methods of estimating the probability for the words are did not occur in my training data now there are certain complications that you might have when you applying this good turing estimate so for a small c generally you will see that n c greater than n c plus one that is a number of words having a frequency c is greater than number of words having a frequency c plus one so remember what is what would be empiric empirical law that shares that so that is a japes law that's that is shared that there is a universal relationship so this is generally good it but it might happen when we go to very very high frequencies certain ah certain certain frequency are not observing that so this is possible ok so what do we do in general so when we go to higher and higher frequencies for larger ks or larger cs here we can replace it with some sort of best fit power law and we will do some normalizations ok so instead of doing it for each individual k i might actually just fit so the so the i have n one n two and so on and when i go to the higher values i simply sit fit some sort of power law and that is so that it is normalized and i will obtain the same probability mass that is one ok so this is power law can can be fit it in general so in in various toolkits that you might very you might use this methods they they do it for only few values of k summation few values of k it maybe k is equal to five or k is equal to ten ok in then the the formula that we were using might have to be real adjusted ok but the basic idea was is same what we have seen here ok so now suppose you apply this good turing smoothing methods for a corpus like a p neswire academic academic press news there are twenty two million words and so that's why you are seeing for different counts what is the good turing estimate that was seen this is for bigrams ok so what yours observing in the table so for the bigrams that occurred zero number of times the good turing estimate was point zero zero zero zero two seven zero ok for once point four four six for twice point one two six now so how do i actually get this number point zero zero zero zero two seven zero so we can just do that quickly so remember for the words that occur zero times what is the total probability that we are giving them getting the probability of n one divided by n ok now what is the probability given to the individual word so you multiplied by n zero and n zero is the number of bigrams that occurring zero times ok what would be the effective count effective count we will remove this n yes so it will be n one divided by n zero now what is n one so you need to find out how many bigrams occurred once yes that you can find from training data how do you find n zero that is how many bigrams did not occurred in my data now if you remember from shakespeare corpus so we ah how do we find out how many bigrams did not occur at all so we need to see what is my vocabulary size what are different sym ah different unigrams ok suppose in my vocabulary size of v that is my number of unigrams so how many bigrams are possible v square bigrams are possible so how many actually occurred suppose i know that out of this some n b bigrams occurred in my corpus so v square minus nb gives me the bigrams that did not occur at all in my corpus so this will become my n zero number of bigrams that did not occurred in my corpus so this value i obtain by finding out n one number of bigrams that occur once divided by v square minus n v and these are number of bigrams in total so that is the unique bigrams that occur in my corpus ok now what is the other objection that you might have from this table while looking at this table so for zero and one this is different but once you start seeing from two on words what is the pattern that you seen you see that most of the effective counts are the original count minus some value like point seven five ok so you see all of these a roughly original value minus point two seven to seven five so now one might are give why don't i just you see minus point zero five seven point zero seven five ok in maybe give a different value to the initial two components that is also done in practice ok so this is called the absolute discounting that should you will see next so the idea is that why don't i substract something like point seven five or some d from each bigram that occur more than say more than once or or whatever ok and i can have one or two different values of d for some initial bigrams and this is called the absolute discounting methods and you you can probably interpolated with my unigram probability that we saw in the last lecture to give it give a weight estimate yes this is called linear inter ah interpolation with the unigram probability of the absolute discounting method and that is the formula that we have written here so this is i am doing some sort of discounting from each bigram count yes plus i am interpolating it with my unigram probability with weight some lambda w i minus one you do we need a weight here so that when i saw this probability probability absolute discounting [noises] w i given w i minus one for each individual w i this will add up to one that's why i need some sort of normalization constant that will make sure that this will add up to one ok so we can keep probably some values of d d for count one and two and otherwise i can use this particular interpolation methods now that's why the idea of our other advanced smoothing model comes that was our kneser ney smoothing man methods so what is that ok so that is can we do better than using the regular unigram correct ok so so see so when before we first started with a simple ah advanced smoothing we were giving uniform weight to each ah each particular n gram uniform addition then from they moved on to unigram prior we said ok why do we give uniformly we we give a higher weight to the words that occur more often in my corpus now we have seen can be do better than even this and that's why we will see the kneser ney smoothing idea so what is the idea so know again let us go to the shannon game ok suppose at this sentence i can't see without my reading and have to filling this blank and suppose possibilities are glasses and francisco and all of us will say ok i should fill in glasses now take a scenario where reading glasses doesn't occur in my training data and reading francisco also doesn't occur in my training data now if i use the previous model what will i do the probability of filling in glasses versus francisco depend on their unigram probabilities and suppose this is my data from some yours corpus so i will find probability of francisco is higher than probability of glasses ok that can very well happened and that will that that means i will add up filling it francisco here there is not the correct word correct choice here ok so can we do something better than simply using their unigram probabilities that is the idea of kneser ney smoothing so francisco is more common than glasses in my data but can i use this idea that in my data whenever see francisco it occurs only after san in in this occurrence san francisco on the other hand whenever i see glasses it occurs some ah after mul multiple different words ok so this is the intuition that is i will see given a word what are the other words it comes with ok so here if i take word is francisco it occurs maybe with only one word yes only one word san but glasses occurs with many other words right it can occur with different different words and this so so both glasses are occurs occurs with ten different words and francisco occurs with only one different word so what is the intuition if a word occurs with many other words that means it is more likely to complete this particular bigram that we are seeing right now so reading after reading a word is more likely to occur if it has already completed many other bigram so if it is more commonly used bigrams with many many other words this will not happen with francisco because francisco is used only after san so given a new context probably san francis francisco is not good choice but glasses are occur many many other different words so after read ah completely new context glasses is a better choice than francisco so formally how this method used this intuition so in instead of using simple probability of this word unigram probability each probability continuation word so that is how likely is this word to appear as a novel continuation what do you mean by novel continuation how likely it is to complete a bigram or in n gram so how do i find out the probability so for each word to find this probability as we said we have to count the number of bigram types it completes so how many bigrams where it occurs as the last one the final word ok now for each bigram whenever it occur the first time it was like a novel continuation so that's what we mean by novel continuation probability here that how many bigrams it is complete so this will be simply it will be proportional to the number of bigrams it is completing so here how many w i minus one are there such that w occurs after that in my corpus so that is the property of continuation ok it's a same thing how what is the probability of continuation that should depend on the number of bigrams types it completes now how should i normalized it i want to find this probability for each word in the corpus yes so this is proportion to the number of bigrams types it completes so i should find out in total how many bigram types are there that each word is completing so that is effective their number of bigram types in total not totals the number of bigram types how many different bigram types are there ok so here also we are only taking the types how many different bigram types we are completing so i will normalized it by using all possible bigrams so here there says all possible bigrams because i am saying the count is greater than zero in my training data how many bigram types i have seen so this will give me the probability continuation the word the w ok so once i have found this probability i can even go back to my unigram prior based absolute discounting method and replace p w i by p continuation w i ok and that is my kneser ney smoothing model so instead of using the unigram priors for smoothing i used this kneser ney smoothing by using the continuation probability ok so if i do that a frequent word like francisco occurring only in one context of san will have a low continuation probability so this will give an model with kneser nay smoothing you see so all these are there is all same this is same now instead of p w i we are put p continuation w i that we get from the previous ah slide whatever map whatever ah formula we wrote in the previous slide that will give me this continuation p w i ok so now this one question here that we discus even the in the earlier example how do we find lambda w i minus one so what was the intuition i gave the hint was that lambda w i minus one should be such that summation over k n smoothing of w i given w i minus one for all w i in my vocabulary should be one so now again take it as a simple exercise that suppose i want to find out what will be the value of lambda w i minus one for this particular formula ok and you can assume that d less between zero to one ok for this particular case what is the value of lambda lambda w i minus one that i will get ok so answer is also on this slide so you will get lambda w i minus one of this so try to find out if i have to cons satisfy the constant of the summation of the probabilities for all the words should add up to one what is the value of lambda that i add up with ok in that will give you this value particular value ok now so other than these two models of good turing and kneser ney smoothing there are other other smoothing models at you might be able to use ok so one simple idea instead of using a new smoothing model why don't we combine various models so remember we always doing that somewhere so you computing bigram i was trying to use the probability of the unigram for unigram priors smoothing now can i generalized this idea suppose i am finding out the trigram probability trigram language model now from my data i can compute the trigram language model probabilities bigram and uni unigram using the m l e can i try to effectively combined these different probability models to find my trigrams language model ok so there are actually two very very popular ways in which this can be done one is called interpretation based methods another is called back of language models so you will just see ah some simple intuition how this can be combined together so so what we have seen as i increased my n power of my n gram model increases we also see saw that in my when i saw the perplexity values when i take a larger n gram i get lower perplexity i get better perplexity that means so that means the the the model becomes much better when i take a larger n but what is the problem that happens if i take a larger n remember why we did all this is smoothing because when i am taking larger n data becomes more and more is fast so many of the actual n grams become zero the probability become zero so at one side higher n gram is better a trigram model is better other side is very very fast so why cant i take advantage of the both both things that is i take a higher order n gram model whenever i have the data but whenever i do not have the data i try to use the low n gram model so i do that togetherness single model so that is the basic idea of these model that we will see so now general approach is can be combined multiple n gram models to get a single model that can had that can also take the advantage of larger n gram model but avoid this fastness problem ok so that is some cases might have to use less context whenever you do not have information about larger data larger context so in back off what you will do suppose i am computing a a trigram language model so w i given w i minus one w i minus two whenever this trigram occurs in my training data i will use the probability as for my m l e but if i does not if it does not occur in my training data i will back off to my bigram language model so that means whenever the count of w i minus one w i is greater than zero i will go to bi bigram suppose that is also zero when i go to unigram this is idea of back off language model so if you have good evidence you go to trigram otherwise you go to bigram otherwise you go to unigram ok on the other hand interpolation you compute all three and just interpolate than together you try to mix these together ok so so let us see what do will do in back off language models so idea is that i am computing a trigram model w i given w i minus two ah i minus one and so here this is the previous word and this is previous to previous word ok so if i do not have counts for w i minus two w i minus one w i suppose this is zero then i use probability w i given w i minus one now suppose count of w i minus one w i is also zero then you go to probability w i ok now seen but you might add up having some problems with how do i normalized the final probability distribution that i will get ok so so this is the particular formula formula of back off that you can use so back off for this trigram model w i given w i minus two w i minus one is p hat w i given w i minus two w i minus one if the count is greater than zero now what is p hat this is actually similar to my maximum likelihood estimate that you will get but it has been something has been subtracted from there so some probability mass has been shown from there so there it when it can be given to the other counts why we ne need that suppose you are computing bigram this ah trigram back off probability model so in general you are doing it after w i minus two w i minus one you are finding for various words w one w two w three ok suppose this occurs three times this occurs two times this occurs zero times so now once use the m l e method ah estimate immediately this will have a probability of three by five this i have two probability two of two by five suppose there is no other bigram and this will have a probability of zero and now you want to use back off to go to two five use probability w three given w i minus one but here itself probability mass adding up to one so how can you use the bigram probability so far that you might have to reduce some mass from these two values so that's why we are writing p hat and there are different ways you can reduce so we will take someone simple example we are a doing it by some ah simple constant you are re ah reducing some simple constants but this this can be proportional also you can multiply some ok so will you will take that is my p hat it's a reduced probability value from my m l e constant ok so now if the count is greater than zero i take p hat if the count is equal to zero then i have to use the previous i have to back off to the bigram model so i have to use probability w i given w i minus one and that's where i back off to this model but what is interest thing that you have seeing here so this is a recursive definition i am using the back off probability of w i given w i minus one and i multiplying with some constant again to ensure that the probability mass adds up to one ok so now so the recursive definition so how do we define probability w w i given w i minus one again it's p hat w i given w i minus one if the count is greater than zero and p hat w n if the count is equal to zero ok so i think it will help a if you take a simple example and try to see how do we actually use this definition to compute the back off probabilities ok so let's take an example ok so here we are taking an corpus where we are having only four words a b c and d and this is one data that is provided so what is this is data so this is it is telling how many times a occurs after a b it occurs four times on the other hand b occurs after a b zero times c occurs after a b zero times ok similarly a occurs after b five times b b occur after b three times c occur after b zero times and also the individual word words a occurs eight times b occurs nine times c occurs eight times d occurs seven times now given this data you ask to compute the prob back off probability model of w n given w n minus one where the previous word is b and previous to previous word is a ok and you have also told that you can use this simple definition for p hat that is p x minus one by eight suppose i want to use that to find out the back off probability distribution ok now so that is nothing but probability back off of word given a and b a is pervious to previous word and b is the previous word ok so let us take to do that so i want to find out the back off probability w given a b so this would be w can take a b c and d so right now from my table what are the counts i am seeing e occurs so this occurs four times this occurs zero times this occurs zero times this occurs zero times ok so assumed by definition of back off probability i can say that this probability will be p hat a given a b because the count is greater than the zero but this will be lambda times previous two words a b so this lambda a b a constant times back off probability of so i will go to the previous word back off probability of b given b this will be lambda a b times back off back off probability of c given the previous word b and this will be lambda a b back off probability of d given b so what it say is that now we have to compute the back off probability of b given b c given b and d given b ok so again we use the definition so now i am computing back off probability of w given b and w is equal to a b c and d now i see a occurs five times b occurs three times c occurs zero times d occurs zero times so i show my definition this will be p hat a given b what will be that that will be five by eight yes minus one by eight that is four by eight what will be this p hat b given b three by eight minus one by eight that is two by eight now what will be this this is this occurs zero number of times so this will be lambda of b times unigram probability of c maybe p hat c ok now what is this we can see from my data what is p hat c so c occurs a times yes and total number of so what is the total of my ah unigram count if you will add up to thirty two ok so this so p c the mle varies so it's lambda b times p c minus one by eight p c is eight by thirty two yes so this will become eight by thirty two one by four minus one by eight that will be lambda b times one by eight ok similarly for d is equal to zero i can write lambda b times p hat d that is that is lambda b times p d minus one by eight ok that is lambda b times p d here was seven by thirty two seven by thirty two minus one by eight ok so that would be lambda b times this will be seven minus four three by thirty two ok so suppose you get this values now how will you compute the the how will you go to the probability b a w given a b you have to first find what is the back off probabilities of for these two two cases c and d so far that you define the what is the value of lambda b now how do you find lambda b if you normalized this right so you will say lambda b times one by eight plus three by thirty two plus four by eight plus two by eight should give me one right this should be one for if you sum over all the words so so what this is this say lambda b times seven by thirty two is equal to one minus six five eight that is two by eight so what is lambda b so we will get ok so you will get this eight by seven so once a we substitute add up this eight by seven here that will give you this probability eight so you will find this probability and this probability also so you have the back off probability for this now put this back off probability here and we have a constant how do you find this constant again you will have two normalized it lambda a b times summation of this plus this probability should add up to one that will give you my lambda a b and also give you this probability distribution so that's why you find the back off probability distribution using this recursive definition ok and what do you linear interpolation we have different unigram bigram trigram orders i try to interpret them in different proportions ok so what we are doing here we have computed a trigram model bigram model unigram model and different proportions lambda one lambda two lambda three are given to this ok so such that they will add up to one in general you can also conditional lambdas on the context ok what is your previous an and previous to previous word the lambda might depend on that so this is the same same equation except that now lambda is a depending on the context ok and now the question might be how do i choose a held out corpus how do i choose my lambdas so that you will do as we say earlier you will have an held out corpus will find out which values are lambda gives you the highest probability and that that lambdas you can has for your smoothing ok so that's we will finish our topic of language model so all so we covered lot of sim simple ah smoothing methods and some advanced smoothing methods and then we will now go to the next topic of ah so we will start with the topic of morphology ok so that will be the topic of the next lecture ok