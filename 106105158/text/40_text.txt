 so welcome back for the fourth lecture of this week so we had already started our discussions on word sense disambiguation in the last lecture and we talked about some [app/approaches] that we use i that are either knowledge based or use some machine learning algorithm so ah in this lecture we will talk about some approaches that are either semi supervised or unsupervised so ah why do we need to talk about the ah semi supervised or supervised approaches so so in all the machine learning approaches that that that we can use for this this particular problem or any other problem what is one particular bottleneck let's take the naive bayes algorithm ok so for for applying naive bayes algorithm you want to compute the features at that is as as such easy some sort of domain knowledge will be required but you have to find out all these numbers like what is the probability of a particular sense what is the probability of a feature given a sense yes now how do you got get all these values for getting all these values you need to have some sort of label data set and what do i mean by labeled data set in this context i will have various sentences where this my ambiguous for would have occurred and somebody has labeled it that this is sense one in this sentence the word w occurs this is sense two and so on so no in some a good number of sentences somebody has labeled each word as belonging to one or the other senses and from there i can compute all these course and this has to be done for each an individual word ok and this can be really really expensive getting all this annotations them and that is true for any generic machine learning task so need to have all these annotations first and that is quite expensive that's why if you can find some sort of semi supervised approach where starting with very few labels you can generate lot of different labels and which may be which may not be hundred percent accurate but you can assume that they are mostly correct so that is also quite well appreciated that is you are trying to use some sort of semi supervised approach and that will save a lot of efforts in labeling and also you use a lot of bootstrapping here so you see one such approach for word sense disambiguation that was proposed by yarowsky so so this is also called minimally supervised word sense disambiguation and what is the motivation that for a machine o for using a supervised learning [alg/algorithm] i have to use annotations and that that are quite ah expressive inexpensive so i use bootstrapping so what is the idea i start with some small seed set and i am learning the my decision is classified from the small seed set use my decision list to label the rest of the corpus again from there find out what are the labels that are confident modify your decision list algorithm again run it on the corpus find out more labels and so on so you try to go in some sort of a loop of starting from some seed set label some sentences ext extending your seed sets of or patterns or rules again applying it getting more labels done and so on until you have somehow conversion so and you can use some sort of heuristics for doing this ah semi automatic labeling and two heuristics are very common in the case of words disambiguation one is one sense per discourse and another is one ah sense for collocation one sense for collocation we have already seen in the in the ah previous example of decision list classifier what is one one sense for discourse so idea is that suppose you are taking one particular document or one particular long context if the same word has been repeated multiple times most probably it is using the same sense so for example you are seeing the commentary or some news article about sports and you find the word bat has been used at one point and you know this category is sense one now if the same word bat occurs in some some nearby places you can probably assume that this is also being used the same sense and this is the idea so in the same discourse if your word appears multiple times i tag it with the same sense so these are my two different heuristics one sense for discourse that is a word tends to preserve its meaning across all its occurrences in a given discourse and one sense for collocation we have already seen that if it is used with the same collocation it will have one meaning and generally it is strong for nearby collocations and if the distance becomes higher it becomes weaker and weaker so now how do we use this ideas to construct a semi supervised approach for words disambiguation so idea would be let's take the same example that is i'm having the word plant it has two senses one is industrial sense another is living thing sense so i will starts with some sort of seed labels or some seed ah collocation so suppose i say that with the first sense that is industrial if the word plant occurs with manufacturing it will have only the first sense similarly if the word plant occurs with life it will have the living thing sense ok so this becomes my decision list classifier i check simply if the words plant is occurring with manufacturing all life manufacturing i say it is sense one if if life sense two ok so what will i do i'll take my whole corpus see wherever with the word plant one of these words either manufacturing or life occurs wherever the word manufacture occurs i say ok this is labeled as this is my pseudo label sentence so now i have some set of pseudo label sentences both with manufacturing and using manufacturing in life so what i would have now from my corpus i would have labeled some sentences ok with sense one with sense two so these are all the sentences where the words plant occurs with manufacturing and these are all the where the plant occurs with life ok so now the idea would be so now i am doing going in in in iterations so now i will treat the match my labeled data ok although to be ah very ah clear we can also call it my pseudo labeled data it is like i am only making an assumption that they are labeled with sense one and sense two now treat it as your label data and then apply your decision list classical algorithm to find out what are the good collocations suppose you find some three four collocations here so it can be ah different things about growth and whatever or the word car like what we saw in the previous case so we find some collocations here now use this collocations and your algorithm to build a new decision list classifier new classifier again you will use that to label new sentences and you will have new data you will get new collocations from there and you keep on repeating the system so let's see for this particular example so suppose i use that and i label some sentences with sense of ah living thing some with sense of industrial plant ok so you see here everywhere life is occurring here everywhere the word manufacturing is occurring now from these labeled or by pseudo labeled data i will try to extract some more collocations like here suppose i see that the word like animal and kingdom occur a lot with this sense but they do not occur with this sense so this becomes my collocation now now use your decision list classify algorithms previously that we have talked about to make ah so make to make my decision list ok now again run it over the coppers and then you can find suppose the word where plant and animal occurs so here you can again label it with sense of sense of living thing and from from once you have done that you can extract some more collocations from here and you keep on doing that so idea would be something like that so you have this whole corpus where the word plant occurs somewhere by using this initial seed set of life and manufacturing you have label sense a and sense b now what you will do you will treat them as label data from their capture some more collocations so something like this so you capture collocation like cell animal species microscopic for for the sense of life and equipment employee automated manufacturing for the sense of ah industrial ah plant now use that to label more sentences and it is a more sentences that you have labeled ok so all these content this might microscopic speech anyone and so on now you have a larger data again extract collocations from where build build here recently and try to label the rest of the corpus so you keep on iterating this and ideally at some point you will have you whole corpus covered now so ah so when do you stop when the error on the training data is less than a threshold so we can have some small training data to check how could your ah this bootstrapping approach is performing or when no more training data is covered by the algorithm when you see that all the examples have been covered and when whatever final decision list you get by this algorithm use that for word sense disambiguation now what is the advantage of this approach so ah so you can see that the advantage here are that it would have to put the efforts into manual labeling of the corpus so you can start with some some very small seed side set keep on applying this algorithm retroactively and obtain the final classifier now so so accuracy also turns out to be quite good but what are the different ah what maybe one problem with this approach ah so one problem of this approach might be that the whole accuracy that you get by this approach depends on how good are your initial ah initial seed set if your initial seed set is not good then you may not ah go very far in this in this approach so the the good thing is if you chose choose very good initial seeds that have lot of precondition the corpus and are also very very distinctive you see this gives starts give giving you very good precision and good recall from the very beginning so now this is a semi supervised approach and i hope the idea is clear now we will see one unsupervised approach for what watson disambiguation by unsupervised i would mean that the senses for the words are not defined a priori but they are sort of learned from the data so from the data you try to find out what are the different senses a word is used in and this becomes your ah sense definition for each word and this field is also called word sense induction as such so you are trying to induce the sense of the word by its usage in the corpus now what is the basic idea so we will talk about the algorithm the hyperlex so key i idea here is word sense induction that is instead of taking some sense that are defined by a dictionary try to extract the senses from a corpus itself so the way the world is used in a corpus use that for extracting senses of the word and these copper senses or uses will correspond to clusters of similar context for a word now let's take one example to get the intuition so here you are seeing the word space and there are many other words that are coming along with this space so the connections show here that two different words occur together in a arc occur together above some threshold number of times in a corpus ok so and as denotes that they are probably similar in the sense that they are co occurring a lot so now try to look at this this simple picture so what do you seeing the words like retail parking are connected retail square are connected feet and parking connected here in the left hand side right hand side you are seeing words like astronauts mission and ah n a s a shuttle all these are connected now by looking at these connections so what is one thing that you are seeing is that you are finding out two different clusters here ok one in the left hand side one in the right hand side one is denoting one sense of space in the sense of parking space retail space in secondary space as such ah space where astronauts will be going in and that involves n a s a and all now from the corpus i can find out that ah astronauts n a s a and shuttle n a s a connected retail parking retail square all these are connected but how will i find out that the word space has two senses and these two senses correspond to ah some particular words so if you look at the figure again you can see that for a particular sense of the word space the words will be highly connected to each other for another sense again these words are highly connected to each other but there will not be much connections between the words in this sense and words in that sense so that is if i construct this as a graph and [kin/can] use some sort of clustering algorithm to find out what are the different ah portions of partitions of this graph so one partition will belong to one sense another partition will correspond to another sense and this is the idea try to exploit how much their that the difference for the same sense the words will co occur a lot together but it will not happen for the words across to senses and there are many many different algorithms that are developed based on this idea for word sense disambig[uation] induction ok so now we will talk about a very simple approach hyperlex that uses this idea now what is this algorithm hyperlex so what it does is that for a given word that it ambiguous by using the data in my corpus it tries to identify what are the ah main hubs so each hub will correspond to one sense of this word so it tries to enter what are the main hubs and every other word in my ah co occurring graph will be connected to one or the other hubs ok so so you know everything is divided into these hubs so how do you detect these house and how do you connect the words to one of these hubs so so so in hyperlex algorithm the idea is that the different usage of the target words from highly interconnected bundles or high density components and each component will have one one of the nodes this is hub that is having a high degree then the other nodes nodes so how do you start applying this algorithm so first firstly you have to construct your co occurrence graph co occur occurrence graph we have seen already in the previous week that i find out how many times these two words occur in the corpus and i use some function of that to find out how much their ah association is strong how strong is their association and i will probably retain only those words that have a very high association so i start with connect correct ah building this graph from the corpus co occurrences now so what do you do this graph is connected around this word this ah ambiguous word like a space in the previous example so first thing i will do i arrange all the nodes in this graph in the decreasing order the of their degree so i will forget about the word space but every other word will be connecting the decreasing order of their degree so what i will ah assume that whatever the hubs are will have high degree shape so take the node that is having the highest degree among all the connections and this will be the first hub try to find out what is the neighborhood of this hub take them to the to it's to this sense cluster remove this from the graph altogether from the remaining graph find out what is the node with the highest degree make it the second hub find out its neighborhood make it the second sense cluster remove it keep on doing that so so once i have arrange the nodes in the graph g in decreasing order of degree now select the node from g that has the highest degree and this will be the hub of the first high density component now delete this hub and all it's neighbors from the graph from the remaining graph again repeat the step three and four to find out what are the hubs and what are their high density components so let us try to see this algorithm on a simple example so here what do you have you have the word bar that is the central world and i want to detect what are it's different senses so what we have done we have first try to construct the co occurrence graph so this is this is what you are seeing is a co occurrence graph that tells what is the strength of association between two words ok ah one thing here so so so i will talk about what is association measure so according to this association measure you will take a threshold and choose only those which is that that are above or below the threshold depending on how you are defining your threshold in this particular case so the number denotes what is the distance between two words so you are only retaining those words whose distance is below a threshold so your threshold is point two five so we written only those connections that are below point two five and distance can be captured by something that is inversely reverse to the association so in this case it is one minus liability of max of probability ah so it is one minus max of probability w i given w j and probability w j given w i so we take the max of that to find out what is the association would be w i w j and this will now capture the distance because this corresponds to the similarity how similar they are their condition probability if i take one minus max of that that is how much they are different what what is the distance so once we have done that for all the words all the pair of words i'll take only those that are having distance below a threshold that means they are quite near and they that is captured in the left hand side of the figure now once you have done that now you apply your algorithm that is finding out what are different hubs and taking their ah different neighborhoods so first i'll arrange all the words in decreasing order of the degree now what are the no nodes here that are having the highest degree so you find the word like iron iron has a degree of one two three and four coffee has degree of one two three and four wine also has a degree of one two three and four ok so now there are multiple words that are having the highest agree so we'll choose some preference may be either the lexical graphical order on some or some ordering on the or some travels or ordering on the graph and suppose you say from i will choose the one from the left and and you choose the word like iron here as your first hub so what is the next next step make iron as your first hub and then take all the neighbors of iron and put them with this ah with this hub so we taking the two neighbors gold and steel and putting them with this hub here ok and that becomes you first hyperlex components now how do you find the next hub you remove this hub and all it's neighbors so i remove iron gold and steel from the graph again find out what is the node with the highest degree you will find the word like coffee here so take coffee of the second hub take its neighbors so it will be chocolate cocktail and wine so these become it's neighbors it's become my second hope hub and all the component with that remove that from the graph now find out the next hub so this can be the word soap that is having a degree three you take the words soap and wax as your third hub and it's component and then finally you will have the word like pressure and dyne thats your four hub so this is my hub and these are different descendants of the hub so from your corpus you are focusing on one word k bar and you are trying to construct the co occurrence matrix finding out the association between various words building out this graph from there you are detecting what are your various hubs and the descendants and this becomes your instruction on this is your target word different hubs and their descendants and this is what you have ah obtained in a completely unsupervised manner because nobody told you how many ah senses the word bar put have or what are the different senses of bar you found it automatically by using the corpus data so so for all other words other than hub you attach them to the root hub that is closest to them and how do you find what is the closest hub you can take the distance between that node and the different route hubs ok and this distance is simply the summation over the path length what is the path length you just keep on adding the path length that is the distance between any node and the root hubs attach this node to one of the root hubs only and this is something that i talked about that how do we compute the distance between two nodes in my original co occurrence graph i take this one minus max probability of w i given w j and probability of w j given w i so now so we can see that how do we ah start from my corpus and construct different senses of a word so y y hub and ah descendants now at run time so we we are talking about this problem whats in the disambiguation so at run time i am given a ah sentence where this word is provided and i want to find out what is it's appropriate sense that is used among all the possible senses i have fine formed by this algorithm so what is the approach so let us say i have this context or the sentence where this word w i occurs and w i is my target word that has multiple senses so suppose they are it has k senses k hubs that is k senses so what do we do we associate a score vector as with each word in the context such that s k denotes what is the contribution of the ah so what is the contribution it will have to the k th hub and this is simply taken by one divide by one plus distance between the hub and the word if it is if the hub is an ancestor of the word w otherwise it' is zero and you do that for all the words in my context and sum those over so find out a simple vector that tells me which sense has how much score which has a have how much score and whichever has hub has the highest score i will choose that so to tell that again so what we will do so we will have a sentence w one to w n and i have this word w i that has s one to s k k hubs ok now i want to find out which of these k senses is used in this particular particular example particular sentence so what will i do for each word w one w two up to w n i will construct this core vector ok so [sco/score] vector has that many dimension as the number of hubs so it has k dimension so i'll construct this vector for each of the words ok so now what are the entries in this vector this entity tells me how much contribution this words will make to the first hub this contribution would be if this word is an ancest[or] if if the particular hub h of the sense one is an ancestor or this word w one it will be one divided by one plus distance of h one and w one ok so that is if the distance is high this score will be low if the distance is smaller this will give a high score on the other hand if this word if the hub is not ancestor i will put a score of like that i will put all the different values here and finally once i have all the values i will add all these so for each hub i'll add all the possible scores and i will take a final vector that is how much contributions all the words together are making for hub one hub two and hub k and i take the one that is having the maximum score among all these and that becomes my winner sense ok so ah so this is the overall idea of this approach so what we have done here we did did not start with any distance defined sense we used a corpus and defined our own senses by seeing what are the different components that occurred together so so idea is that for a for a given sense of a word different words would have a high co occurrence they will make some sort of cluster so identify different cluster size different senses now once you have these senses if you want to op use them for ward sense disambiguation at run time whenever the word is used find out from the context words which of thus ah different hubs they are closer to so whichever hub gets the high score becomes your disambiguate sense so thats ah these are some different ideas for ah approaching this problem words and disambiguation although i said earlier there are many many different ah approaches that have been proposed so in the next lecture we will briefly talk about ah an idea of word sense ah discovery that is how do you find out if the world has got a new sense in the corpus this is a relatively new ah new field and ah we will see how the ideas that we have developed in this these lectures to you how do we use them to find out if a word has got new sense in the corpus ok thank you