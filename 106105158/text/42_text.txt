 so everyone welcome back to week nine of this course so we have been talking about semantics in the last two weeks and we discussed two different approaches of semantics one was using lexical semantics and otherwise using distribution semantics and we saw how used the corpus or from a lexicon you can extract semantics you can capture the meanings of words in some sense that is whether two words are similar to each other and and many other aspects so in this week we will discuss another very interesting way of capturing semantics by using topic models and they have been a very very popular tools in n l p for whenever you have to talk about what are the concepts that are there in a particular document in a particular corpus and many other ah questions that that's around ah that are around the semantics so let us see what are these topic models so so firstly let us start the brief ah motivation that why do we actually need topic models as such ah so so when we talk about the data so the text data in general on under that so so there is a huge amount of data available right in form of whatever form you can think of there are lot of data in terms of news articles scientific articles and blog social media and everywhere now so you can think of it as some sort of information overload and you might be interested to capture only certain aspect of the data ok so we want capture certain semantics so so what is an easy way in which you can some sort of organize this data and then this can help you to search or browse through the data more much more effectively so so what other so right now what about main tools that we use for doing this search of different information either we type some query in our search engine right some human so so free text query we type and we get some reals ok and we try to browses the reals and sometimes reals are not relevant or even if they are relevant we go to the pages to which the link to and keep on browsing so that is some sort of generic behavior of how we we try to explore this sort of information but there is no easy ah interface where you can you can say ok i want to understand these topics only i want to go too deep into this topic i want to go related topics and all that that is not available with this search and ah link kind of ah behavior so the topic modeling gives you an ah alternative method of going through this huge amount of information by ah some sort of searching based on themes criteria so you can think of it as if you are having a set of documents and you know what are the themes that are running through this corpus so you know the ok this documentary about certain themes related to politics this document is about certain theme related to ah dramatic and so on different concepts now you are going to a popular thing and then you can either zoom in or zoom out ok so that is you want to go inside the theme to very very sub themes or you want to go out to a broader theme so can this be fascinated by it by some sort of modeling so so we might also want to look at how the themes in a corpus are changing over time ok how they are evolving over time this happens a lot in for example scientific article a par particular research in ah in physics might be starting with certain certain concepts and over that period of time it starting with ah it is now having new concepts ok so by by various discoveries so can we discovered that what are the themes setup already over time and you can also talk about this behavior where you have you would select the theme first and then you try to examine the documents that talk about that theme so topic modeling it is a method that gives you this facility by which you can organize all these collections by the themes that are occurring in in those documents and you can then understand search and do summarization and many other applications ok without and this is important that for doing all that you do not have to give any prior manual efforts in in labeling the data so you do not have to tell that this document concerns this topic or this document concerns that topic you do not have to do any labeling so the interesting aspect is that you give it a corpus an in an uncivilized manner it learns what is the overall structure of different topics and these document concerns which topics and so on so this is very this is some sort of very interesting aspect that made topic modeling very popular that you no need to have any prior annotation as such although there are some variations we will also talk about where you can give an annotation to get some different sort of ah topic modeling but overall in the generic picture you do not have to give any any sort of annotations so it learns on it's own from the huge amount of data so so what you do here so you discover what are the hidden themes that are pervading to the collection and the topic model itself annotates the document as for these themes so it will tell you ok these are the overall themes this document contains these themes and so on and [ noise] once you have these annotations that are given with the topic model you can use that to for validation task like organizing the collection summarizing collection searching when the user query comes by using the each annotations so many different applications we will cover some of those in this week and yes we can there is a there is a huge huge amount of literature around topic models so you can also feel free to read about it so so let's talk about one of the applications before we go into the modeling in in detail so we talk we are we are cons ah consistently talking about discovering topics from a corpus ok that is one of the most important ah application of topic models so something like that so you have a large corpus and can you discover that there are many many topics and topic look like this ok by topic i mean a set of words that occur a lot in that topic ok so so let's see one topic human genomic d n a genetic genes sequence gene molecular sequencing map information so so topic model we will try to get you ok this is one sort of topics it will not give you a label ok so although you can see this looks like a genetics topic of genetics it will not give you a label it will tell you that this is there is a topic in this in this corpus there is a theme and in this theme each words are more having more higher probability so these are the more important words in this theme and then it will tell ok there is general theme that is going through this corpus something like this evolution evolutionary species organization life origin biology and then you see a set of terms ok these are having a high probability in that theme there is no labels that topic model is giving but you can see ok it might be evolutionary biology ok then similarly another theme disease host bacteria disease so on and four theme computer models information data computers and so on so so the topic models will tell you ok in this collection you have these four themes in addition to some other themes ok so that is your predefined ah number that you will need need to give to the ah models so you can say ok i want fifty different themes in this corpus or twenty different themes in this corpus yes although there are again variations where you might not have to specify these number a priori we will briefly talk about those also but for now let us say we we tell the model we need that money themes so try to discover ok these are the important themes that i am seeing with a pervading through this corpus and these are some examples these are some examples that we are seeing here that are coming from a real corpus by doing the l d s applying the l d a model now once you have these themes then the topic model will also tell you ok this document is about having ah only these two themes out of these fifty this document is having these five themes out of these fifty in in what proportions and so on so like that you can now ah think about organizing huge your whole collection and also ah some sort of a summarization or searching through this now so why is this intuition interesting so when you look at a particular document in general you will see that yes there are actually ah some particular themes that are that are pervading this whole document so so that is you have somehow thought of making a document that concerns these themes only so here is an article from sci science magazine nineteen ninety six seeking lifes bare genetic necessities and so if you read this article so this is about using some sort of computational data analysis to determine the number of genes and organisms that needs to survive so how many genes ah genes and organism needs to survive so so this topic this article will blend some of the topics that are important to convey this message ok and these words are denoted in various colors here so we are seeing some words with pink some words with yellow somewhere to blue and and so on let's read the words in in pink so there were organism survives life organisms yellow genes genomes genes genetic sequenced genome and blue will be computer productions numbers computational computer analysis and so on so you you can clearly see three different themes one is about genetics another is about the evolution another is about ah this computational data analysis so so this this article is blen blending these three themes together ok it's can be capture that in an uncivilized manner without someone has to manually annotate this ok this is the themes in this document so if i yes you see blue is data analysis pink is evolutionary biology and yellow is genetics so this article is blending these three topics in different proportions and that is the motivations that's what ah that's what is the hypothesis that we are having about your corpus in the corpus there will be various documents there will be a ah there will be some big number of themes that are going through this corpus but when you look at a particular document it will have only a subset of these themes ok in some proportions can we ah automatically capture those by using some modeling so so yeah once you know that this article blends this topics together you can situate that in a in a collection of scientific articles you can say ok where this where this situates what are the articles it is similar to and and so on so so what is the basic idea you are not going to the mathematical details that we will cover in the next week but let us see the intuition so so this is the important idea so topic model is some sort of generative model and this just as a model that captures this idea about the collection having topics and topic document having different proportions so what it says is that so you are having a doc having some documents they are nothing but mixtures of topic and a topic it's nothing but the probability distribution over words ok so what are three themes we are talking about we are talking about a collection that is having documents d one d two up to d n here a documents in the collection then you are having some topics ok so suppose there are some t one to t k topics and use your documents in document some words will occur and you say ok these are my ah vocabulary these are my words w one to some w m ok so here three themes in the collection there are documents documents some words occur suppose you have a unique words define your vocabulary and there are some topics now what topic models say yes topics are nothing but probability distribution over words so t one would be something like a distribution ok so here what one comes the probability of zero point zero one word two comes with the probability of zero point one and so on ok similarly t k will be again a probability distribution what one comes with a probability distribution of zero point zero five word two with a zero point zero one and so on an idea is that ok probably there will be some part of the themes now so this is my topics topics are defined by probability distribution over words now what are my documents documents are again some sort of probability distribution over topics so i say d one what is d one d one is having a distribution over these k topics so i will say ok topic one occurs with the probability of zero point zero five topic two with probability zero point two topic three with zero point five and so on ok so i will say ok something like topic three and topic two are more turbulent in this document and not the other topics and same way i can situate all the documents in some sort of mixture of topics ok and this is very important to understand the topic model so what are my topics probability distribution over the words and what are my documents again mixture components or topics or you can offset probability distribution over the topics ok so this is all add up to one this will all add up to one for all the topics for all the documents ok so so what is that mean so you have a topic like genetics remember no labeling but we can see when you see that words so this is probability word genetics so if we have topic about genetics this will have words about genetics with high probability and if we have a topical of evolutionary biology it will have a topic about evolutionary biological of high probability ok so this will be making two different themes in this connection and and the model is a generative model and as we understand generative model so it will work like ok from we will first generate the topics yes we first define the topics then when we have the topics you will now start building the documents you say ok document will have some proportion of these topics and then i will write the words in the in the documents so topic search in that the first and then the documents as per the generative model ok so so this picture will make themes more clearer so so what so we are having seen some nice colors here so on the left you are seeing some topics right that's all you are talking about we are having a set of topics any topic is a probability distribution over the words in the collection so like here this topic has the word gene with the probability of point zero four d n a with point zero two genetic with point zero one life with point zero two this is different topic brain neuron nerve right data computer number different probabilities you see seen their different themes now once we have this corpus wide themes so so each topic is a distribution over words do you understand that now what is a document so you are seeing a collection here and there are a lot of documents right and we are being shown one document so each document is a mixture of corpus wide topics so these are my corpus wide topics now i will take a particular mixture of these topics so suppose one of my mixture is here i take this topic red yellow and blue i will take these three topics only maybe others away with a very very small fraction and this defines the topic distribution of my doc document now so i have now seen said that my document contains these topics with some proportions now how do i generate the words in the documents right that is important i need to generate the words so how are these words generated that's again interesting so you have this you think of it as a multinomial distribution and from this distribution you sample a topic ok suppose the first sample is this pink that pink topic that is about a organism life evolve and so on ok these sample is topic now from the same topic you have to generate a word how do you generate a word again this topic is what a probability distribution over words think of it as a multinomial distribution against a sample a word from here and that's what you will generate in the document and this we will keep on going for generating all the words in the document in the in the document so you will say ok next word i will again sample a topic i get this yellow topic about gene d n d n a genetics i use this topic to generate a word by sampling from a multi ah multinomial distribution and that's what are you keep on repeating this is for this document again for next document i will select a mixture of topics ok and then once i have the mixture i will again keep on selecting the words that will go in the document so now so so this is a statistical model also we called it a generative model so what is this reflect so it reflects to two important things what are those all the document in the collection share the same set of topics right we have an underlying set of topics all documents are sharing the same set although the proportions are different ok so each document exhibits those topics in different proportions that is one important fact and then what is the other other fact each word in each document is drawn from one of the topics where the selected topic is chosen from the per document distribution over topics ok so is that clear so for each document i have a collection of top i have a distribution about topics so i say ok this topic is probably point two point five point one and so on now each word in the document is sampled from this ah distribution how you sample a topic first and then as per the topics probability distribution you sample a word ok this this we will covered in the generative model in more details but that is the intuition that we showed also from the picture see there are two important facts about l d f now so if we think about the example article from science magazine that we were seeing so the distribution over topics would place a probability on genetics data analytics and evolutionary biology ok and each word will be drawn from one of these topics so what will happen when you are doing the influencing you will say ok these this document contains these three topics and how do we generate a word you will sample a topic from here and as for the topics probability distribution sample a word and keep on generating the words so so this is the genetic model but is that what you also see when we have a real corpus we apply l d a and do an influencing so that is what is being shown so for this collection of science papers so ah in proper model was trained with hundred topics and this is what you see so these are the hundred topics and for this particular document some topics have a high probability so this topic have probability point roughly point four point one five point one two and the next one having point zero five so if you see the top four topics in this in this particular document and try to see the the the most probable words in these topics so that's what you see so this topic has human genome d n a genetics and so on since our topics that we were showing earlier and you see this was obtained without doing any manual labeling so you you find out ok this this document contains these four different themes important themes and this is you can also do some labeling later on manually this is not done by l d a but get this is also not very very important so what is important is that i l d a can help you to obtain this sort of ah distribution this in a very uncivilized manner so you can find out what are the overall themes and for a document what are the most important themes so now so we talked about ok what is the generative model of l d a but what is the main problem of l d a ok so we are saying ok we will generate the topic first and then we will generate the documents right by sampling a distribution of topics each each for each word i will take a topic it's distribution i will take a word and so on ok but that's not how we write the documents right ah so so how will that we used so so it has to be used in a manner that i am observing some data i have the generative model and now i am trying to estimate the parameters of this genetic model by using my observations that is what parameters will maximize the likelihood of seemed observation so this is like reversing this whole process of ah l d a that we are talking about ok so so so we know the documents object in a collection and if the documents but i do not do the topic structure so i do not know what are my topics so whatever distribution of words within each topic i do not know for each document what will be the distribution of topics and i also do not know for each ah for each word in a document what will be the topic assignment i do not know any of this a priori so this is all my hidden structure so center problem l d a is to reverses in to process and use the observed documents to infer the hidden structure so what is the hidden structure can you infer that by seeing only the object documents and this may also called as some sort of reversing the genetic process and that is a center problem of l d a so so this is this initial introduction lecture was about to give to give you the intuition but now that you have some intuition of what l d a is what topic model is we will next going to details about what is the mathematical model of l d a ok and how do you solve this problem of ah reversing the generative process ok so that will be recovering in more details in the next lecture thank you