 so welcome to the third lecture of this week so so we are talking about lexis semantics and there we delt with what are the various relations we can establish between the word form so we were talking specific ah in traditional now that are lexemes different items in my lexicon and we also talked about word net that what are the different relations we can establish between various lexemes word net and what are the different standard forms like using sins at and what is the hierarchical award net how do you use the define word similarity and so on and finally we ended up the saying that there is one in head problem engaging word net that is when you get a new sentence you do not know what is the particular sense of the word that is being use in the sentence so you cannot apply the methods vary directly so so thats fair so we deal with this problem that given a sentence for each word find out what is the sense in the word net that is being used here and this problem is called versions disambiguation so very classical problem in n l p and let lot of research has happened in over last few decades so we will ah talk about what are the most important methods for dealing with this problem and we will talk about both simple approaches and machine learning based approaches and also some unsupervised approaches so we will cover that in the next two lectures starting today so this is our topic that is word sense disambiguation so let me just define the problem again so we have seen in this week that there are many words that are having several meanings and then we can call them different senses of the same word so now example can be of the word bass so you can use bass or base depending on that they you want to use that in the context of fish or music now the problem is whenever you are given this word i will be talking about music or fish you know ok so lets see these two sentences here the first sentence is an electric guitar and best bass players stand off to one side not really part of the scene just as a sort of nod to gringo expectation perhaps this is one sentence and the second sentence is talking about some trip bass in in lake mead were too skinny so now the same word is being used in both the sentences and i have to find out whether the word is being used the sense of fish or music now once you see the sentences how do you find out that what is the meaning of of the of this word in the sentence you will try to look around the context in what context this word is being talked about accordingly you will chose one sense over and another one now this whole the field of words has disambiguation so this deals with this problem that so i have to find out the sense of the word depending the context know how can i do that computational so so so we can define the task of the disambiguation that is to determine which of the senses of an ambiguous word is invoked in a particular use of a word so we say we are doing this problem only for the ambiguous words whenever it has more than one senses in a particular use of that word what is the sense that has been used now and and as you can see we will deal with this problem by looking at the context of the words use so now how exactly we do that there are many different algorithms that handle this problem so there are approaches based on their knowledge base approaches that use different overlap based methods for handling that then there are some machine learning based approaches they use supervised approaches unsupervised approaches and semi supervised approaches and also there are some hybrid approaches and in the literature there are a lot of different methods for solving this problem now what we will do we will talk about some very basic methods so that you have the intuition with you that what are the different ah features what are different methods you can apply for word sense disambiguation so now starting with the knowledge base approaches so there are some sort of overlap based approaches they try to use some sort of overlap now how do they find is overlap they would require a machine readable dictionary ok so like word net is my machine readable dictionary you can have some other sort of the torus also that are in a machine readable form so by machine readable i mean so all the entries and all the information is such that you can easily access that and you can make use of that now so what approaches will do they will try to find the overlap between the features of different senses of an ambiguous word that is they will call it sense bag and the features of the word in its context and call it as the context bag and they will try to measure the overlap so now just to explain this idea let us see i am having a sentence s that contains some words w one to w n and the word w i can be used in two senses w i prime and w i double prime its sense one sense two ok now in the sentence this word w i would be used only for one sense and that is what we are assuming this will be true in most of the cases unless the speaker himself wants to imply two different meanings of the same words so we will say in general for this word w i you would have only one sense so now i want to find out depending this context whether sense one should be taken off sense to should be taken so idea would be for each sense i will construct the sense bag ok and we will see how do we construct sense bag so i will have a so this i will convert that to a sense bag sense bag one and this i will convert to sense bag two now looking at the context that is you can think of all the words that are coming in the sentence other than this word i will construct a context bag now i will try to find out what is the overlap between context bag in sense bag one and what is the overlap here ok and i will take the answer as the as the one sense that is having the highest overlap and this is to explain that simply i have various senses lets context the sense bag context that the context bag take the overlap take the sense with the maximum overlap now to construct this sense bags and context bag you will use various features that are provided in the dictionary that you have so features could be like what are the sense definitions that i am having what are the example sentences that are provided different hyponyms hyponyms and so on and any other criteria that you can use from the dictionary and you will use that to construct both the sense bags and the context bag take the one with does maximum overlap now lets take a simple example or in actual algorithm that that does that so we have lesks algorithm so we talked about this algorithm in the previous lecture also for a different case when you wanted to find out what is some ready between two different senses now suppose i want to use this this less algorithm for the purpose of versus disambiguation so what it does so it constructs the sense bag context bag and how are they constructed the sense bag contains the words in the definition of the candidate sense of the candidate part so i have a word that it ambiguous can have multiple senses for each of the senses which i will construct one sense bag and the sense bag is constructed by using the definitions that i have in my dictionary and how do i construct the context bag i take all the words in my context take each of the senses of the context word and then take their definition all these together become my context bag so i have a single context bag and i have multiple sense bag for different senses of the word and then i take the overlap and this overlap is taken by using the leska algorithm similar to what we saw in the previous lecture that if there is a match of n a particular n gram i a d score of n square so now lets take this case so i have a sentence on burning coals we get ash now hear the word ash it ambiguous and i want to find out what is the correct sense of this word that is usually sentence and suppose for simplicity we are using only the word coal as my context so what am i going to do i will find out i will make different sense bag for the word ash whatever senses are recorded in my word net and i will construct the context bag by using all the definitions of the word coal ok and then i will try to measure the overlap so suppose we do that so here is the case from by taking the dictionary definitions from word net so ash has three senses so one is corresponds to some trees second corresponds to some solid residue and third is to converting to convert that into ash and we want to find out what which of the three senses is used here so what we will do we will take the word coal that is the only context for we are using here but in general you can use any number of context words i take all its sense definitions that are defined combine them together to make a context bag so this whole thing together becomes my contacts bag now i measure it similarity with each of the senses ok and if you do that we find the sense to three of the words are matching here and sense to becomes a winner so here what you using here you are also using the is stemming so this is optional you might use stemming you may not use stemming so this is the generic idea of leska algorithm ok i hope this is clear if i want to use multiple words i can repeat the same thing i suppose i am using burning coal and get all the three words i will take all their definitions and combine them together into a single context bag and then measure the similarity of each of senses with this whole context bag ok so now lets take another scenario where we have a different kind of source so what is this source now for each word we are told what are the categories these senses its senses belong to so i have some finite set of categories this can be like finance category location categories sports category and so on and each word has been recorded as it has a sense in these this category so now the problem would be when i am given a sentence a word can can have senses in multiple categories find out what is the a probably category in this sentence so how do we use the this framework here so what i will do so for each sense of the target word if find out what is the source category to which this belongs ok now calculate the score for each senses by using context words no idea would be the context words again would be having the researches all categories so for each context but i will find out what is the source category over i will see by using the context word which thesaurus category getting the highest count so here so what we will be doing a context word will add score of one of the sense if the thesaurus category of the word matches that of the sense so lets take this sentence the money in this bank fetches interest of eight percent per annum and suppose my ambiguous word here is bank has two senses one in the case of safe finance that is for the money bank that is also used in the sentence another could be river bank also also can be used as location or something ok and i take each context words money interest and annum fetch each again would have their own dissolves category so what we will be doing i will take all these words money interest fetch annum i know bank have two different senses finance and location so for each of these context words i will put a one if the match any of this category so money message finance so i will put a one here interest message finance again i will put a one here fetch does not meet meet any of these senses so both are zero and a message finance again plus one and then i will add the score of both the senses and i in here i say that a finance the score is three for locations score is zero so i will take the sense of bank is finance in this particular case so these are simple approach it by just taking the dictionary definition thesaurus category now so so there is one problem in both these approaches that is work for disambiguating the sense of a particular word we are not disambiguating the sense of the other context words we are taking all the possible senses of them into a single context bag but suppose you want to solve this problem where in a sing in the same sentence there are multiple ambiguous words and you want to disambiguate each of them together then can we do that and this can be done by making each of the fact that although each word might have multiple senses but the particular senses that are being used in this [sen/sentence] this sentence there will be somehow connected to each other so we want to exploit this and for this we can use some sort of grab based method similar to what we do in the case of ah pagerank algorithm so let me just quickly explain this idea and we will actually deal with this pagerank algorithm in detail when we talk about summation application but i will try to give you the intuition so this is the problem that suppose i have the sentence the church bells no longer ring on sundays ok and i can say there are four words bell rings church in sunday and i want to disambiguate the senses of these words and also suppose that from the dictionary i know what are there sense definitions so here the word church has three senses bell has three senses ring has three senses and sunday has one senses and i want to disambiguate the first three words together so how do i do that so idea would be so that is treated as a graph based problem where for each word i first write down water [diff/different] different senses of this word so i have made a vortex for each of the sense of these words so there are three three words is for a bell three for ring three for church and one for sunday so i have now ten words in this graph now what will be the idea i will now try to connect the different words together by seeing what is the overlap among different sense definition so i will now try to correct connect some weighted edges between different sense definitions by using [las/laska] lesks method so the [lask/laska] is by similar by simple for finding similarity between two sentences i use that find out what is the similarity between any two sense definitions and add a weight between these two and this will give me some sort of matrix like that so i have all the sense definitions connected now one thing one important thing is that for the same word i will not have to connect its own sense definition so there will not be any edge between s three and s two for the same word bell but they will be edges between different words so as three of bell an s two of ring would be connected by what is the similarity between these two sense definition so idea is that now we want to somehow find out that particular combination here so one sense for each of the three words such that their overall similarity is the highest so they are having the highest similarity and one particular method that can use for that is using it a pagerank kind of algorithm so idea would be if there are multiple if there are appropriate sense definitions that are similar to each other if i use the pagrank algorithm they would have very high ranking school because they will all contribute to each other so what do we do so for this graph once we have all the word decease we have all the edge bit also we treated as a problematic fear finding the page rank for each vertex of this graph and to give you simple intuition so now what would we have so we have ah ten nodes the graph and i try to connect them by different numbers these numbers depend on there lesks similarity now once i have this if i want to compute the pagerank scores for each node ok and now what is the algorithm for computing paging is score if you take it the simple idea that is ah i suppose so this pagerank it completed in a creative manner so we start with some initial random is course this will be some sort of probable distribution and i use this equation v equal to v a to find out what will be the pagerank score ok and we denotes the pagerank scores and and how do we actually come up with this score start with some initial v zero and then keep on multiplying a a square a cube until it converge and that becomes your pagerank score ok so we will talk about this algorithm in detail later and for now you can just quickly have a look at what what is the pagerank algorithm but the idea would be once we we do all that you will find one pagerank score for each node and the nodes that are having a lot of connections with other nodes and with high in degree connection will be given a higher pagerank score so if suppose this node is connected to multiple sense definitions it is connected to this sense definition this sense definition and this sense definition that means this might be one of the important senses in this for this particular word and idea would be to find out for each word one sense that is having the highest connection and this would be so someone that will get a high pagerank a score so i will give pagerank the score to each of the ten node and then for each word i will pick the one that is having the highest among this set so i will take one from each of the set and here i anyway choose this one and this becomes my final disambiguated sense so once i have added all these weighted adages i will apply the graph based ranking algorithm so this can be pagerank algorithm and now once we get all this course i will for each word i will choose the one with the highest score an idea here is the sign the purpose sense that is connected to multiple other sense which for multiple words would get a higher score and that is the intuition that i want to choose the particular sense that is having a similar definition has many other senses and this i do for each of the word so here suppose s one in for bell one s three for ring and s two for church are selected and they are my final disambiguated senses for these three words ok and this is my overall algorithm now we can also use some machine learning based methods and one simple idea would be to use a naive bayes algorithm so naive bayes algorithm we were we are using for doing classification here what is the classification task for a given word there are multiple senses from the sentence find out what is the appropriate sense so this is this method has to be word specifics and for each word you might have as many classes as the number of sense of this word so so problem here is find out the sense of the word that gives the maximum probability as given f and set of features that i will extract from the context around this word so this for a multiple senses i want to find out the sense that is having this score at maximum now because naīve bayes genetic model that means the sense comes on the off first and then the features are are generated from their different features so how do we compute this particular probability this would be nothing but orgmax s probability f given s probability s divided probabilities f and because this common for all the sense this is orgmax over s p s p f given s is now p s nothing but the prior probability of the sense that is if a particular sense of the word is more commonly used that get a high prior and p f given s h what is the probability of different features being observed or generated by this sense so naīve bays model what we do we make this assumption that each of these features are conditional independent of each other given s given the sense s so suppose there are features f one two f n so i was write it as orgmax s p s i is equal to one to n probability f i given s ok and this i will do for each sense and i will i will take the one that is giving you the maximum score now what is important here is what are different features that you will be choosing for an naīve bayes method and these features have to come from the context around the word so what what are different things i can use in the context i can use water different part of speech that are being used what are different words and so on let us see for this task what are the important features so this we have already seen some for this task i can use the part of speech of surrounding words and what are different semantic and syntactic features i can use co occurrence vector that is what are the different other words this sense is used with and then i can use this collocation vector vector that is water in general the next word next next word previous word previous to previous word and their part of speech tags when the word is used in this particular sense so i will construct this this side of feature vector by by using this all these examples and the two parameters of a model that is the prior probability of the sense and the probability of features given the sense can be computed from my corpus by using simply maximum likelihood estimate so how do we compute probability of the sense s i number of times the sensitive divide by number of times this word is used improbability of a particular feature to the senses sense number of times this feature is observe with the sense divide by number of times all other features are or number of times senses used so these are simple formula for mle and you use that and plug into the algorithm and you have your naive bayes models thirty for for use and you can use some other approaches like decision list algorithm so what is the idea so here so you will use so this hypothesis of one sense per collocation now what is this one sense for collocation hypothesis for a given word idea is that with the particular collocation it is use only in one sense for example take the word like bat we saw there are two two different sense one is like it creates cricket bat and what another it like a flying mammal now suppose you take this collocation cricket bat ok so the word cricket covering before bat with this collocation the word bat will always used in one sense only not the other sense this is the idea can i collocations to find out with this collocation this word to be with all in this sense and with the other collocations it will be used in another other senses and this is called the one collocation percents property sorry one sense per collocation property and so how do i start about getting these set of collocations for a for a ambiguous word so initially i can try out all the possible set of nearby words that that occurs with this word more more often and once i have done that for each such collocation i can compute what is the probability of a particular sense being used for this collocation with respect to the other sense now take the simple case where the word has only two sense sense a and sense b so for a given collocation i will find out what is the probability of sense a given the collocation and what is the probability of sense b given this collocation now question is once i found both these numbers what will be a function that will tell me how good this collocation is now this collocation would be good if one of these probabilities is high another probabilities quite low so collocation indicates sense a if this probability divide by this probability is high sense b if the inverse of that is high and this simple measure we used to come find out what collocations for a word are more important than others so so i come so this and i take a log of this and this is called a log likelihood ratio log of probability of sense a given the collocation i divide by probability of sense b k given collocation i and then hire log likelihood means more evidence so what i will do for different collocations that i have extend for the word these can be some simple words that occurs a lot with this word in different context i will compute this log likelihood this course and then i am i will arrange them in their decreasing order ok and this will give me the decision list so now so here is an example suppose you have some training data and here the ambiguous word is plant ok near trying to extra various collocations that can help me to find out whether the sense used is a a is in the sense of plant life and with the sense is b in the sense of manufacturing so suppose i i i run my algorithm and i find out that the collocation plant growth its having a very high likelihood that is ten point one two and this this is for the sense a what does that mean log of probability sense a given plant growth divide by probability sense be given plant growth will become ten point one two and this is the highest among all the collocations this comes out on top n a c sense is a suppose similarly the second collocation is if the word car occurs in plus minus k words around this ambiguous word plant then the sense would be b and this as a log likelihood of nine point six eight and so on ok and with each likelihood i have a sense now once i have that i can use it directly as my decision list classifier so what i will do at runtime whenever i am given the sentence i find out if this collocation is present plant growth if this is present senses a if not whether the word car occurs in plus minus k words around plant if so senses being if not if plant height occurs senses a and so on so so when i get a sentence at one time like plucking flowers affects plant growth i will take this sentence and run it through this decision list classifier and accordingly wherever i find a match i immediately provide the sense and i stop now one thing you have to be careful here with these numbers so so remember the formula we have a probable determine the denominator and it can also be zero in some cases so you might have to use some sort of smoothing you can use add one a smoothing or some other smoothing method for that so once you do that you can come up with this decision list classifier and at runtime given a sentence you can easily compute what is a sense that should be used here here is another example of decision list classifier like you are discriminating with me bass and base the fish and music sense so it can be something like that suppose this is how you are different collocations are ordered h for a log likelihood so we make a finishing tree if the word fish occurs in plus minus k words if yes sense is fish fish if no if the collocation stripe bass occurs if yes fish if no if the word guitar occurs is directly coming from the ordered list of log likelihood and this you can run through a new sentence to find out the appropriate sense of the word bus so we saw some of the algorithms that that that by using knowledge based approaches and machine learning approaches now in the next lecture we will also talk to talk about some other approaches that are either semi supervisor unsupervised but in general there are many many different algorithms that you can apply on this task we only providing you very brief ideas on some of those so thank you soon so i will see you in the next lecture