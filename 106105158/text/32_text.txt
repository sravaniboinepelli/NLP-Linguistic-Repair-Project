 so welcome everyone to the seventh week of the course so where that we are now moving into the second half so in the first six weeks we had talked about basic tasks on in any language how do we pre process the language how do we deal about its morphology and syntax now we will be going to some advance topics so in the next three weeks starting this this week we will talking about semantics how do we capture semantics so again semantics is very huge topic we will focus only on very very basic and interesting notions that can you help you build some important applications and also read some research papers in this field later in the in the in the last three weeks of the course we will going to various applications so today we are starting our discussion on semantics so and we will be taking about two notions distribution semantics and then semantics before going into that let us see what do i mean by semantics so in this lecture today we will we will give the introduction to the topic of distributional semantics so as such when i say semantics what comes to your mind in general syntax talks about arrangement ok how are that different words are arrange in the language in when while talk about semantics it immediately we move from arrangement to some sort of meaning domain so i want to find out what is the meaning of different words how do they combine together to form the meaning of the of the larger unit that can be the sentence that is in general the the main the main field so if i have to give a definition i can say that semantics is the study of meaning that is what is the relation between symbols and what do they denote so here suppose i have the sentence john told mary that the train moved out of the station at three o clock ok it's a sentence in the national language contains multiple words we know how to tokenize them and how to find the morphological tax and may be also the syntax and different representations now what do i mean by saying that now i want to go to semantics a semantics i will like to know what all these words themselves mean separately so what do i mean by john told mary etc so these are all symbols now what do they denote and when they combine together in a sentence what meaning are they putting so this is as i said very very ah vast field lot of different research has happened and is happening we will focus on mainly the word meaning so how do we say what is the meaning of a word what are different notions so in general so when i talk about distribution semantics or any other model that we will cover in this course we we are trying to find out how in general computers can produce such semantic representations so machines can interpret some sort of semantics and thats why the i idea of computational comes in here so so computational semantics in general is the study of how we can automate this process of building these ah semantic representation and also reasoning with them ok so when we talk about the methods ah in general there are two different methods one are based on formal semantics that is how do i construct various mathematical models that can tell me what is the relation between various expressions in the language and also relate them to whatever there is is there in the word for example so if you have heard about read predicate logic thats what is done in formal semantics so if i have a sentence like this john chases a bat and you want to produce a mathematical structure that denotes the meaning of the sentence so i will put them in very very very very logical form like i will say so [vocalized noise] so for startup i i will say there is an x where x is a bat and john chases the bat so john chase becomes a predicate and i will say john ah chases x an x i have already defined as a bat and like that i will define how do i convert my natural language expression to some sort of this logical form and then i will also have rules on how do i infer from this ah logical expressions how do i reduce one expression into another one and then so on and this is again a field of formal semantics and that is something we will not cover in this course what is something apart from formal semantics that we can see and there where the data a lot of data that we have can help us and this is where the field of distribution semantics comes in that is can i study the statistical patterns of human word usage to extract semantics so can i see how humans are using different words in their language to find out what is their semantics and this is what will be the topic of this week what is the type of distributional semantics how they capture that and how can i use that for certain meaningful applications so so this field of distributional semantics is mainly built upon this hypothesis of this hypothesis of distribution distributional hypothesis that is prevent from many many years what is the distributional hypothesis let us see some famous quotes about this so as earlier as in ninety fifty three wittgenstein said that the meaning of a word is its usage in language so that is you can know what is the meaning of a word if you see how it is being used in the language ok along similar lines in ninety fifty seven firth said you know the word by the company it keeps now what do i mean by the company of a word what are the other words it occurs with in my corpus on my language and that tell me that tells me about the word now going one step further so so here so what these different quotes are mentioning that the word meaning whatever it might be i do not care about what exactly is the meaning but whatever it is it should be reflected in the linguistic distributions that is the way the word has been used in the language that will tell me the word meaning so now zellig harris in nineteen sixty eight that gave this famous code that took this idea to one step ahead that is words that occur in the same context tend to have similar meanings so now i can talk about two words having similar or different meanings if i can somehow measure their context and capture that and i will say two words are having similar meanings if the context in which they occur are very very similar so so the idea here is two words if they are semantically similar they tend to have similar distribution patterns so now thats what we will be doing in this week how do we capture distribution patterns of words and use that to find out similarity across words let see some other quotes from zellig harris so he also said that if linguist have to deal with meaning it can only do so through distributional analysis and that we said in very early ah seventies and eighties and and if you see the the field right now so whatever the main methods or semantics are there they are built upon distributions so with the idea of so with the recent idea of and all and this is what we will also capture in our ah in this week of this course another very similar sort of code from zellig harris from his distributional structures so if we consider what are morphemes a and b to be more different in meaning than a and c then we will often find that the distributions of a and b are more different than the distributions of a and complaint in other words difference in meaning correlates with difference of distribution so what he saying suppose i take three different words or morphemes a b and c and suppose there is some where i can capture the distribution so that is how they are occurring in the languages so i have distribution for a b and c now what he is saying if a and b the distributions are more different then that of a and c then what you will find so what we are saying a and b are more different than a and c if the distributions are more different in the meaning also you will find they are more different ok so you can think of these as so i take some examples like car as a and b c can be automobile they are quite close and b can be something like a book ok so here a car and automobile are quite similar and car and book are more different so they are very very different so if this this is what i see in meaning i will also see something similar in distributions so what i will find these two distributions are quite similar and these two would be quite different ok we will see in this week that how do we capture these distributions and how do we compare that but this is the basic idea now what is another important thing here so distributions that we are capturing whatever semantics it is allowing me to to handle its all differential not referential now what is the difference between that two terms differential and referential so again if i would look at the same example of these three words car book and automobile so what do i mean by saying so the semantic sign capturing is only differential but referential so i cannot say what is the meaning of car i am not defining the meaning of car at sort of this concept representation ok there is there is something i am not doing so what i am doing what how similar or how different the two meanings are so i am saying a and c how similar their meanings are or how different their meanings are thats why this is differential sort of understanding of semantics so how different or how similar two different meanings are i am not talk talking about some referential meaning in distributional semantics so now ah if we look at so this was the linguistic prospective is there some cognitive prospective also with distributional semantics so so there we have this idea that what is the representation of the word and it is said to be some sort of abstract cognible structure that i am storing in my in my brain or somewhere we do not know but that is something i gather as i keep on hearing this word or looking or finding this word in more and more context as i find this word in more and more context i tend to build some sort of representation about this word this is the cognitive prospective and what is some sort of evidence so so for example how when you encounter a new word that you have never heard before even if we do not know its meaning you may guess something about this word what it might be ok so lets take an example ah like i am saying this word wampimuk ok i have never heard this word before now i am seeing this word or hearing this word in this context he filled the wampimuk with the substance passed it around and we all drunk some ok so just by looking at the word that are occurring around this word and seeing what are the words might occur in place of wampimuk i can say that this might be some sort of a container some sort of a glass or something so which can be used for filling up the substances and and passing around so so as i keep on hearing a word or seeing a word in more and more context i tend to build some sort of meaning structure about about that word now suppose i have i have heard this word in a very different context like this we find a little wampimuk sleeping behind the tree immediately i will have a different ah interpretation i will say ok may be wampimuk some is some sort of a small animal ok so this is some sort of ah intuition behind the cognitive prospective and why we might think the meaning in terms of the distributions in the language now to capture these distribution and then and and find semantics from there the models that are used are called distributional semantic models and also there is some term called dsm for them and what are these so these are various computational models that build contextual semantic representations from my corpus data so now what is important here is that i am already given some sort of and if more data generally the better so i am given some corpus data on how different words are used in language and from there i am trying to come up with some ah some semantic models distribution semantic model and this will capture the differential aspects of meaning among words so so dsms are models for semantic representation and where i capture a semantic content by using a vector so for each word we will try to build a different vector that will capture the semantics and these vectors are obtained via the statistics analysis of the linguistic linguistic contexts of the word now this word occurs in various context will will help to find out its vector representation and there are some alter alternative names for this semantics like corpus based semantics statistical semantics geometrical models of meaning vector semantics and word space models and and there might be some other prevalent names also but we capture the same idea that can i use the distribution of the words to find out the meaning or at least capture which words are similar than other which pair of words are similar than other pair of words so now so again so this distribution semantics when i talk about what do you mean by the two terms distribution semantics so when i say distribution they are the vectors denoting different different words and their vectors in some multi [vocalized noise] high dimensional or in low dimensional space and this space is the semantic space so i have distributions that are vectors denoting the words in that multidimensional semantic space and semantics space has dimensions which correspond to various possible context and this context can be gathered from a corpus so what i am doing every word i am denoting in this semantics space and this is my distribution semantics and this semantics space composed of various dimensions that are my context in which i am trying to represent a given word so let us let let us take this an example so when i talk about symbol vector space model so that all of you would already be aware of so this is my two dimensional plane and its x y plane and i can denote different objects so here there are two points that i am denoting a and b by their coordinates so what are ah so their projection in on x axis and y axis that tells me the coordinates and i can now capture how similar how near to words two objects are in this space analogous to that now let us think of a semantics space where dimensions are not x and y axis but they are various context and can i denote all my words in those dimensions so here suppose that my dimensions are two words eat and drive ok two different words and i am trying to denote three words car dog and cat in these dimensions and what you are seen here so this might denote how often a word occurs with this word on and this word the projection will denote that so you see cat comes quite often with the word eat similarly dog car comes a lot often with drive but not so much with eat so you see immediately when you try to put these word in this semantic space will capture some similarity that cat and dog are probably similar much more similar than cat and car and dog and car and this is the this is the idea can i use different context as my dimensions and represents all my words in those dimensions to give a meaning representation in general here i have shown you only for the dimension that corresponds to two words but in general you all have to use any number of dimensions so you can use any number of words that you want so so you might have a representation like that so i can say cat is a is a any other object or a word that has a weight of point eight in the dimension of dog point seven dimension of eat point zero one dimension of joke so depending on how often the word cat occurs with all these words ok and and this i can do for different different words and then i can capture how similar they are and this is a very very powerful technique although its looks very simple it it works quite well in many many applications that that we will see so now lets take an example and how do we start constructing this a vector space or word space model and try to see can we compute the similarity across words so here i am given a small data set that has six different sentences in so like an automobile is a wheeled motor vehicle used for transporting passengers and so on there are six different sentences are given and i have to build a vector space model or a distributional semantic model now to build the model i need to start with what are the words i want to represent and these are my target words so here i want to represent four word automobile car soccer and football so now once i know what are the words i want to represent the next question i need to ask is i need to represent in in what dimensions so this becomes my context here also denoted by term vocabulary what are the dimensions in which i will be denoting all these four words so here i have one two three four five six seven dimensions now so what will be the alg informal algorithm for constructing the deman the distributional semantic space or the word space you start by picking up the words that you are interested in that means the words for which you want to find the distributions and these are called your target words so here i start with four target words then you define what are the context words also so here you found seven context words next question would be ok when do i say that this word occurs with another word should i say that when it occurs in the same sentence same paragraph same document or within plus minus two words ok when do i say that so so that is called the context window so i can define a context window to be ah may be five words around the word or it can be a sentence paragraph and whatever word so so i need to define a context window that is how many words should i consider surrounding a target word and it can be defined in terms of a document paragraph sentences and so on now a simple method would be once you defined the context window you know what are your target words what are your context words so when you are defining a distributions of the target words find out how often they occur with the various context words within the context window and just write down the number this occurs with this word in this context window two times three times zero times and and so on and you define the vectors for different target words and this is this can also be called your as your co occurrence matrix between the target words and the context words now once you have built the co occurrence matrix you can think of each row of this matrix as your vector for that denotes this individual word so if we try to apply this algorithm on the previous example that we have seen so what we will have we will have four words as the target words seven word as my context words and i will have a matrix of dimension four cross seven and each element will denote how often this target word occurred with this context word within my context window let us see the whole sentence here in the context window so you can try doing that with the example that that we had seen and try to find out what are the different number of times each individual word occurs with another word and you will find something like that so this you can call as your co occurrence matrix or distribution matrix composed of targets and contexts so what do we see here automobile occurs with wheel transport passenger football occurs with passenger tournament london goal and match and [vocalized noise] similarly for car and soccer so now this is my ah representation of target words in this semantic space now suppose i take only two dimensions of this semantic space so here i had one two three four five six seven dimensions suppose i see only two dimensions so here i am seeing goal and transport and i am projecting all the four words in this dimensions so what representation i will see so i will see that soccer occurs with goal and goal not be transport football occurs with goal not be transport automobile occurs with transport not with goal and car occurs with transport not with goal immediately you can see some separation that is soccer and football are coming out with more similar automobile and car are coming out with [vocalized noise] excuse me are being more similar by just looking at these two dimensions ok and that is some very interesting aspect that the distributional semantic models try to capture now suppose i go back to my vector representation so where i have all the seven dimensions and i want to find out which two words are similar and to what degree so one simple thing i can do is to take the dot products ok so that will tell me if they are having weights in similar dimensions if they having weights in different dimensions the dot products will be zero or close to zero but if they are having weights in same dimensions their dot product will be high suppose we are not doing any normalization i can just take simple dot products and find out which two objects or which two words are more similar than other words so if you try to do that in this example so what did you would you find automobile and car so dot product would be one plus two three plus one four and everything else is zero similarly automobile soccer you will find zero automobile football one car soccer one car football two and soccer football five ok so from these simple dot products computations so what what is something that you see so you see that immediately you can capture the idea that here automobile and car are very similar they are having a very high dot product four similarly soccer and football are also very similar they are having a dot product of five on the other hand automobile soccer are not very not much similar car and soccer are hm not much similar and so on so you can find out some sort of differential aspect of my meaning that is which two words have similar meanings and which two words are different meanings just by looking at the simple distribution and as you keep on getting more and more data you will you will be able to capture more and more information about how similar and how different two meanings of words are this is a very very basic ah basic idea that that i had tried to give in this lecture now we will see what are the different formal ways in which we can construct these models how do we count what are different ways in which we can denote the entries of this matrix and how do we use that to compute ah similarity across two different words ok so this we will cover in more detail in the next lecture but i hope that the main idea of using these distribution models is clear thank you