 so welcome back for the fourth week of the course so in the last week we had discussed a lot about part of speech tagging and what are the various algorithms as such one can use to solve this problem and we started with one particular classifier that is h m m model for doing part of speech tagging so we had gone through the basics of h m m and what are the different probabilities then one need to use to be able to come up with the the ah best sequence given a new sentence so what do you mean i mean by sequence given new sentence i want to find out what are the part of speech tags for each of the word in the sentence so there are multiple there are many many possibilities and you need to find out what is the best probable tag sequence so we formulated this problem as noisy in the noise channel model framework and then we came up with the hidden markov model by using certain assumptions so in the last class so we had also discussed a bit about what we will be doing at run time when we are given a sentence so let me start from there and we will see a particular algorithm on the viterbi decoding to find out the actual tag sequences in a very very efficient manner and then later on i will go to the parameter learning part so we had talked about it in the in the last week also but we will devote sometime over that in this week so so we had seen this figure in the last week itself so we have the sentence promised to back the bill and i want to obtain the part of speech tag sequence for the sentence so how do i start i first start by finding out what are the probable part of speech tags for each of the individual words in the sentence so further word here promised the two possible tags are v b d and v b n it can be past tense or the participle for the [wor/word] word to there is only one part of speech tag possible for the word back there are four part of speech tags possible we have seen that in one of the lectures for the word the there are two part of speech tags possible for the word bill there are two word of speech tags possible so i start by enumerating all the possibilities for each of the individual words in the sentence now i want to find out what is the actual part of speech tag sequence for this word for this sentence now looking at this picture how many different possibilities can you see ok so if you start from any of the part of speech tag for the word promised and go to any of the participant tag for the word bill any particular path denotes a possible tag sequence so if you try to count the number of possibilities ok so it will be two times one times four times two times two ok it will be roughly thirty two possible part of speech tag sequences and among the study to i have to find out what is the most probable tag sequence now how should i do that so one knife method would be that i enumerate all the possible sequences so all the thirty two sequences i compute the probability for each sequence so now you know how to compute the probability by using the h m m kind of model so we did one simple example in the last class and find out which of the tag sequence has the highest probability that will be a naive method you might even do that for if you have say only thirty two or fifty possible text sequences but think about the sentences that might have ten to fifteen words and some words might have five to six different tags so you will see that it will grown exponentially ok so enumerating all the sequences and computing the probabilities is not an efficient solution so you might want to do something more efficient so that you can find out what is the most probable tag sequence in a very very ah efficient manner ok so what should be in good algorithm so so let me take this this figure this example to to give you the intuition on what kind of algorithm would be used for finding the actual tag sequence so to do it efficiently ok so the idea here is that if you are enumerated all the thirty two possibilities you are doing some computations again and again ok so can you try to reduce the time by doing it in a dynamic programming approach where you first store computations for the smaller paths and use that for the larger paths so we have seen one particular example of dining programming in in the case of added distance so we will try to use a similar concept here for for decoding the actual tag sequence and this will be called viterbi decoding ok so looking at the picture so how you can save the computation time ok so let's take this sentence here now let's let me take two different paths so one path that is there in the actual sequence s v b d t o v b d t and n m and let me take another path that is v b d t u then j j then d t and then n n ok so there are two paths that we start with the same tag and end with the same tag ok so so so in v b d to v b n n p sorry d t n n and v b d t o j j d t n n ok so now they are two different tag sequences ok and i will i want to find out probability of both of these to find out which one is better now what is the idea of viterbi decoding the idea is that so you are ending with the same sequence here d t and n m ok so let us look at only till that point tell d t because after d t the computation will be the probabilities will be the same ok so this is one way of saving computation ok in both of these the composition of d t will be the same so you can save on that but what is more important here so when you arrive at the tech d t in one case you are coming from v b another case you are coming from j j ok so there are two different ways you are coming to d t in these two sequences let us suppose that at this point ok i can store for the word or further tech d t what is the best way of arriving at d t is it via this path or is it via this path so i suppose i can find out at this point what is the best way of arriving a d t is it this path of that path if i can store if i can somehow find that the later computations will so for the two paths the later computations will be the same ok so then i do not need to compute these probabilities again for the next sequence so if i can store at this point that the best way is coming from v b and not from j j so then i am doing it for the next next steps that are going by d t i do not need to explore this path so this is the idea suppose i write them as steps so step one two three four and five at each step for each part of speech tag if i can store what is the best previous tag from which it is it should come so what which previous tag will maximize the probability of arriving at this particular tag then the computation of shared for the same path and i do not need to repeat the same computation again again so in this particular example once i can store at step four for d t the best way is coming from v b and not from j j when i go further i need not worry about this path at all i can simply continue with this path ok because if this probability is p one this is p two the next probability will be shared it will be times probability of n n given d t and probability of bill given n m ok that will be shared across the two paths so if i know at this point p two is higher than p one i can probably fo i can forget this path for going via d t and this is the idea this i will do for each of the speech step so similarly for step three for each of the four part four part of speech tags like for v b i will so so here it is easy because for each part of speech tag it can come only via t o ok so it it is important for this case because for d t it can come via four previous tags so i needed store which of the previous tags it is coming from which has the highest probability similarly fine and p which of the four previous tags has the highest probability so that is what i am going to do a d c step for each part of speech take i will store what is the best probability of arriving at this part of speech ok so let us look at the formulation so intuition is that so as i have explained we are recording what is the optimal path at each state for a given step ok what we are storing what is the cheapest cost of arriving at a particular ah state that means that has the highest probability in the cheapest cost now once we have found out what are the tags at different points we also want to write down the sequence so i also need to store a batteries so this is the highest probability so what is the particular part of speech tag that gives the highest this highest probability ok so i want to store this delta j s what that is the cheapest cost or the highest probability to step j at step s and also the backtrace from that state to the best predecessor ok so for the previous example that we were seen for d t the best predecessor was v b and not j ok so now so this i am doing at any step as for state j how do i use that to compute for the next step ok so let us again look look at it so suppose i have stored delta j s ok so here s is three and j is my state it can be v b j j n n and r b so i have stored for all these part four different part of speech tags what is the cheapest cost of arriving at this point what does that mean i i know at this point which path is better v b d t o v b or v b n t o v b ok so i know that for all these four different part of speech tags so i store the step for this step for this state what is the optimal cost other what is the most probable path so this cost i have or this probability i have now how do i use this probability is to compute for the next step say want to compute delta j plus one s how will i compute that so now delta g s would have taken care of the word back already so what are the things that i added for going from so i am sorry here this should be stepped yes so this is delta j s plus one ok so this is a stap s plus one i want to find out probability delta j so i know the probabilities are step s now what is the additional information the transition from this is state to this state and the emission probability of this word given is the state so i can compute it using delta j s plus one is delta i s times the probability of this tag sequence that is probability of tag t j given t i this is the probability of tag sequence transition and the emission probability that is probability of word edge step s plus one given t j ok so this i am doing for some ith state but there are four p there can be some n number of states so i will find out the max of those among all the states at the previous step i have delta s i multiplied with it the transition probability and the emission probability at at step s plus one ok and the max gives me what is the best path at this is state j at step as for this one ok so now suppose out of these four i found out this is the best path so we will also like to store what is the best predecessor ok and this is nothing but orgmax of this function ok so i call that size s plus one that is orgmax of the same function ok and that is what i am also storing and that's all this will continue so what will happen at the end i am at a step five ok and i know what is the probability so let me write down delta n n at five and delta v b at five i know both these probabilities what is the best way of reaching at this state what is the best probability of reaching at this state at this fine i can just take the maximum of these two and this will give me the the best probability of any possible part of speech tag sequence and then i can use the backtraces from here to obtain the actual part of speech tag sequence and this is my vita by decoding algorithm so if we see that formally so yes so i am i am storing at each step s what is the cheapest cause of the best probability of reaching there also the predecessor and using that i am also computing the probabilities for the next step ok that is so that is similar to what we have seen here the probability at the previous step transition probability and emission probability then i take the max over all the previous possible states also show the predecessor for the next step and how do we end once we have gone through the sentence end i will have the probabilities for all the states possible at the end the ending word i will take the maximum of those n backtrace from there and that will give me the optimal automat tag sequence for this particular sentence ok so i hope this algorithm is clear so let us take a simple example so that you can become much more familiar with that how do you use this video decoding algorithm finding problem so what do you see in this in this problem i have a sentence the light book ok and you are given some sort of emission probabilities first that the word the can come from the tag determiner or noun so they are two possibilities determiner noun the word light can come from noun adjective and verb ok possibility by using vita b decoded algorithm so how do we start so i have three steps one two three ok so i want to store at each step for each state ok so let me call it delta one at step one similarly delta two as step one ok so what is the this probability so this will be what is the probability that the tag determiner occurs in the start of the sentence multiply with the probability of emitting the from the tag determiner ok so let us see how these probabilities given in this case probability of determine occurring in the start of the sentence so in this question in the la in the last ah sentence it is written assume that all the tags that have the same probability stop in the beginning of the sentence so probabilities are given implicitly ok so now the question is it how many tags would you assume so here you can take any assumption so in this question you are shown only four tags determiner noun verb an adjective so assume there are only four part of speech tags and each one has equal probability of occurring at the start of the sentence so what will the probability of determine occurring at the start of sentence it will be one by four ok similarly for noun one by four times probability of the given determiner and that is given here h point three ok i am probability of the given noun is given as point one so this is your delta two one and delta one one so you can also further write them oh point zero two five and this will be point zero seven five so better write them as two point five into ten to power minus two seven point five into ten to the minus two ok so you have delta one one del two one now you go to the next step ok so now here you want to find out delta one two in step two what is the best probability of reaching noun noun noun you can reach via either here or here ok so you need to compute both the possibilities and take the max over that so let us compute this probabilities ok so what will be that so first will be delta one one times probability of getting noun from determine times probability of light given noun s and second one will be delta two one times probability noun given noun times probability light given noun ok so let us compute these two probabilities so you will take the max of these ok so let us compute compute these probabilities so this would be so let me take it here seven point five into ten to the minus two probability of noun given to determiner so that is that we can see from the table given to us noun given determiner point five and light given noun each three into ten to power minus three ok and this gives me three point seven five into three eleven point twenty five into ten to the minus five and the second one becomes two point five into ten to power minus two times probability of n given n this is point two into probably at given and that will be three into ten to the minus three ok and that is point five one point five one point five into ten to power minus five ok and from these two you can say that this one is higher than the next one so this is what you will take so you can store delta one two h eleven point two five into ten to power minus five ok and you will also store the backtrace that is coming from this path ok same thing you will do for adjective and verb adjective again there will be two possibilities coming from determiner and noun for work there will be two possibilities again you compute the probabilities using the formula and store what is the best probability so it will be delta three at step two delta two h step ok so this is what you will store also you will store the backtrace once you have done that now we'll go to the final step so where you will store delta one step three one means noun here and delta two at the step three again you will compute that by taking all the three possibilities times the transition and all so this will be max over three quantity this will be max over three quantities again you will find out the two values delta one three and delta two three and how will you actually determine the finest sequence you will take the max of this suppose max comes out to be this one then you will go to the predecessor ok suppose the predecessor was this one you will go to hear suppose predecessor of as this one and so on so you will find out this sequence assuming this is the these are the best fervent ok but i hope the idea is clear that how do you actually do this computation so i would encourage that you you complete this particular exercise on your own now coming to the the point of learning the parameters so what are the parameters do we need in the in the so we need three different parameters ok so we need the probability of starting the sentence ok that is let me call this in the in the in the language of h m m the pi that is what is the probability that a particular tag or estate start the sequence ok so i want this probability pi i for all posh tags ok then i need the probability of transition from one part of speech at another ok so this my transition matrix i need all this a i j transiting from probability t j given t i s and i need my emission probabilities that is probability of the word given a tag so you can use it using a matrix b so i need all these three probabilities to actually use this viterbi decoding algorithm at runtime if i do not have these probabilities i cannot teach you viterbi decoding so the question is so these are my parameters of the h m so how do we actually find out these parameters of my h m so here i am saying there are two different scenarios that you might have ok so first scenario is where a label data set is available what do you mean by a label data set you have a set of sentences available to you for each sentence you also know what is the actual part of speech tags sequence so what is the part of speech tag for individual world somebody is manually labeled it for you so this is a label data set this one scenario second scenario is you only have the corpus but you do not have any data where the sentences are labeled with the part of speech categories ok so i have the tooks is a clear so scenario one where you have a label data set a scenario to only corpus but no labels so what i am saying for these two scenarios you can find the parameters in different manner so suppose you have scenario one you have a label data set so what you can do for finding the parameters of h m m so this is something we also took them the in the last week so for example if the label data set i want to find a parameter like them the transition probabilities probability of tags a given tag i how will you find that the label data set you will have all the possible tag sequences that actually occurred in the corpus now from that you will find out number of times t i n t j occur together in the corpus divided by number of times only t i occurs so this was what we said as maximum likelihood estimator you can use that to find this probability same with all the other probabilities pi i as well as p you can find using maximum likelihood estimate now problem comes when you have the corpus but no labels ok so now there are no labels so how would you actually find out probability of t j given t i when in the in the data there is no label on which word got a tag t i or which one got it at t j so you cannot compute it directly from the from from any labels so what is the method that you will use for learning the parameters when the corpus is available but labels are not available and that is what we will see in the next lecture ok so we will be using baum welch algorithm to estimate the parameters of the hidden markov model when the labels are not available ok so this is similar to expectation maximization algorithm and you will see what is the addition for this algorithm and how we actually apply this ok so hope in this lecture you understood how to apply with viterbi decoding when the parameters of h m m are given ok and so you can do that for and in the next lecture we will discuss how do we run the parameters when the labels are not available thank you