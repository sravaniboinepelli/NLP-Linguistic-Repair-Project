 so hello everyone welcome to the third lecture of this week so we are talking about distribution semantics and we had already discussed what is a generic framework of distribution semantics how do you build models by taking different sorts of targets and context and how do you fill up all the antigen of matrix now in this lecture we'll talk about some of the some applications for distribution semantic models also how do i compare similarity across two words and then we'll move towards some a structured model of distribution semantics so starting with some application so so let's talk about this problem of term mismatch that we see in the case of information retrieval so what is the problem so so information retrieval the user is giving a query that is in terms of certain words that he feels are describing his information in it and so he is trying to ma so the system is trying to match this this set of keywords to all the documents that are there in the depositary and by by doing this matching they are trying to find out what are the potential document that can be returned to the user now what is one problem here so it might happen that the particular concept that the user has in mind the user expressing by using certain words but the documents has have the same concept using a very very different word so so they are the the words are similar on the semantic label but on the surface label they are two different keywords or two different words so this is called a term mismatch problem and one of the key issues in information retrieval is how do we solve this term mismatch problem how do i find out that the user typed this particular term but he might also be interested in the other term that is semantically similar but on the surface form it is different so let's see an example so suppose ah there is a user query insurance insurance cover which pays for long term care and it might happen that the rol relevant document that contains the answer to the user query may have words like medicare premiums insurers etcetera that are not directly provided in the user query now one question is how do i find out that these documents that contain some similar words are also relevant to this user query so for this task one can use issue semantic models for query expansion so query expansion is one of the techniques that is used for this term mismatch problem so idea is that i the user has given a query with certain words can i try to find out some other words that are semantically similar to the user query words if i find out some words then i append those words to the user query and this is called the expended query query expansion process and now i give this expended query to to my search engine and then retrieve the documents so now what we will see in in brief can we use our distribution semantic models for this problem of query expansion so so what is the idea so once user gives a query we reformulate it using the terms is a relevant or that are related to the terms that are already there in the ah in the query find out some related terms and try to use that to improve the retrieval performance so so how do we find out the related terms so firstly i have the query terms that the user is giving find out what are the distributional vectors and take a function combination of all the distributional vectors and obtained the expanded query so suppose that the query has three terms find out what are the related terms to all this three terms in the distributional sense and make a linear combination of all such ah possibilities and give the expanded query to the user so how will it look like so suppose the user gives a query like catastrophic health insurance so what you are doing so in your distribution vector suppose you are finding out what are the terms that are related to each of the three terms catastrophic health and insurance ok so what will happen so user has given these three terms and suppose you are using words at con as context so the different words were there w i w n and for each word so you are computing what is the co occurrence so it can be say p m i value what is the p m i between catastrophic and w one so like that you are computing for all the words so now what you might do you might say ok what are the words that are associate with all the three words so that are having high p m i values so one way is you just add all the p m i values p m i q w i for all words q in my query ok so for each word w why i i complete a score like that what is the p m i of that query one of the query word with this word w i and add your all the query words and then i can sort it and maybe i can normalize it with respect to the highest term so what will happen at the end of that if i can i can sort these course and find out what are the words w i w i prime etcetera that are very much related to the query terms and all these we have already covered how do a compute this course once we this course i can compute this symbol formula and find out what are the terms in a decreasing order so this one part way but you can have any other sort of functional combination so this is a simple addition but you can have multiplication and other sort of function combinations also so suppose we we do that so what happens to the actual query so this is my ac actual query catastrophic health insurance and if i try to use this technique to find out some other related terms i get terms like this so i have terms like surtax h c f a medicare and h m o s medicaid h m o beneficiaries premiums and so on now so these terms are taken by ah from a news corpus that is u s news corpus so they are certain terms that are relevant to the u s ah medical ah system so what are the so what you are seeing in this in the expected terms so some terms are like broad explanation terms so that means if you look at may be some ah ah or some other thesaurus you can find out such kind of ah similar terms so you will say ok medicare beneficiaries premiums or some broad explanation terms so they are ready to the query in addition you also get some very very specific domains domain term like here h c f a that is u s domain term for health care financing administration similarly h m o for health maintenance organization h h s so these are very very domain specific terms so if you know that you query in in a cert is coming from a certain domain this approach can be very very helpful to also find out terms that can ex used that can be used to explain the query and not also a domain specific ok so let's take another example so these are so i'm showing here some trec topics so trec is a text t o conference so they organize various competitions for information retrieval so they are actual queries from the trec ah data so you have a query like ocean remote sensing and when you use this method you can find some other terms like radiometer landstat ionosphere c n e s altimeter nasda meteorology and so on and ah again as we seen saw in the earlier query there are some broad explanation terms like radiometer lanstat and ionosphere that are connected to the query and they are some domain specific terms like c n e s and nasda here so this is one very nice ah application of distribution sem semantic models you have some ex existing query you want to match it to some document why don't you expand this query by using some related terms and this is the idea and this we can use in general for any rid matching task you are having do different text data and you are trying to match these so try to find out if there are somehow related on the semantic label using distribution semantic models and use this idea to find out if they are similar or not now so ah so once we have computed the distributional semantic model so i have the vectors for different words ok so different target words i have the vectors now how do i compute the similarity between different words so it depends on what is your representation that you are using for computing the ah semantics or the what is the vector representation if it is the binary vector you will use different sort of similarity methods than if it is a ah real valued vectors or if it is a probability distribution you will use a different sort of similarity matrix so as is the framework allows you to use any of these ah vector representations so let us see if you use if you are using any of these what kind of similarity methods you can use to compute similarity between two words so let us say my ah so have words x and y and they are denoted using some binary vectors so as such any sort of distribution you can also convert into binary vectors so how do i com ah compare ah how do i find similarity between two word two words where the representation is binary vectors so for binary vectors we have some standard methods like using dice coefficient ok so what is that ah two times intersection of x and y divide by length of x plus length of y and what do i mean by this so x and y are binary vectors suppose x and y are binary vectors and let us say ah the size of the my vector the dimension are say since it is a nineteen dimensions ok and my x is one one one one in the first ten dimension and zero zero in the rest nine dimensions on the y is zero in the first nine dimension and one in the last ten dimensions ok so now i have two vectors x and y and i want to compute the similarity between these two vec vectors suppose i'm using dice coefficient so the formula is two times x intersection y divided by length x plus length y so this is the length of x intersection y now what is a x intersection y so that is only one element ok so x intersection y has one element that is one so this will be simply one two times one length of x how many ones are there ok so we'll not take the length of the vector because otherwise it will be the same for everything everything will have the same size so length of x means how many entries are one so here it will be ten for y also it is ten ten plus ten so this will a point one ok so now this is ok if you have the binary ah values suppose you do not have the binary values so suppose my values are like my x could be point three point five point seven point zero one and so on so if you want to use these measures so one simple way could be convert them to binary and now converting would mean you would put a threshold that if the value is below this threshold then you put it to zero above this threshold you'll put it to one so suppose here threshold is point zero five so what you'll do this will go to zero this will go to one this will go to one this will go to one and here you'll check whether less than zero point zero five then go to zero if greater than then go to one like that so you can convert any such vector into binary representation and then use this dice coefficient now are there some other methods of computing similarity also so we have jaccard coefficient and overlap coefficient so jaccard coefficient is x intersection y divided by x union y now what is the difference between jaccard and dice in what scenario jaccard will gave give a different value than dice coefficient so let's try out the same example so i'm using the jaccard as x intersection y divided by x union y ok so in this case what is x intersection y this will remain as one only one entry is one and what is x union y now x union y will contain all the elements this will be nineteen so what you are seeing for the same two vectors dice give the value of point one and jaccard gives one by nineteen that is closely point zero five so this give much smaller value ok so why is that so you are seeing here if there are very small number of shared entries jaccard further penalizes ok so that's what you are seeing here there are very only one entry common among twenty so jaccard is giving further penalty now what will overlap do so overlap is x intersection y divided by minimum of x y so can you think of a scenario where overlap it can becomes one but say jaccard will not become one so this will happen when ah one of x and y is completely incoud included in the ah other so so let us say let us take a different case ah suppose my x is one one one one one zero zero zero zero zero and y is one one one one one one one one zero zero ok so in this case what would be the jaccard it will be x intersection y divided by x union y so this will be five divided by eight and what will be overlap this will be x intersection y divided by minimum of x and y ok so this is again five and what you mean of a x and y again five this will become one so that is if if one of the vector is completely subsumed by another vector overlap is one ok and that will not happen in jaccard dice coefficient and similarly if there are a small number of shared entries jaccard will give us smaller value so that means depending on what kind of ah ah what kind of similarity want to use you you can have either dice jaccard overlap suppose in your task you want to find out if one of the words is completely subsumed by another you will use overlap and not jaccard ok but if you want to see some other criteria you can choose one of the three methods so here so what you have seen jaccard coefficient penalizes small number of shared entries while overlap coefficient uses the concept of inclusion where the one of the entries completely included in the other one now this is a vectors of binary vectors suppose they are ah real number values so like x and y so the say the sa n number of real number values so then you can use simple cosine similarity or euclidean distance ok so you can use cosine similarity and euclidean distance now what is the difference between the two if the my vectors are not normalized cosine similarity euclidean distance are different ok but if they are normalized they will be the same and they will give the same sort of ranking and that you can do a very simple exercise if they are normalized they will give me the same sort of ranking between the ah similarity of vectors now on the other hand suppose my distributions are probability distributions so i am devot denoting different vector size probability distribution my space of the context vectors so then how do i compute the similarity i'll use different measures that are used for computing similarity or distance in the case of probability distribution so what are the common measures so you can use k l k l divergence is so that is sigma i p i log p i by q i so this so k l divergence is asymmetric ok so if you use divergence between p and q and q and q and p they will come out they may come out to be different so that's why there is a symmetric divergence also that is information radius so p and p plus q by two and q and q plus p p by two fo find the k l divergence between these and add this that is information radius and also you can use is a very simple formula like l one norm so you have p i q i find out the l l one norm so what is the difference between p i minus q i sum over all i so that is you have different sort of representation and you can u use different similarity value similarity matrix now so let us talk about what are the ah different what are some other sort of distribution semantic models that all that also try to use some specific instructions in the sentences and we try to motivate them using this example so that is what is the difference between an attributional similarity task and a relational similarity task so what is ash attributional similarity so that is i'm given two words like do dog and wolf and i want to find out how similar they are so similarity between dog and wolf will depend on how much their attributes are similar that that's why it is called attributional similarity and by using distributional semantics how do you capture that what are the other words they are co occurring with are they co occurring with similar sort of words if they are co occurring with similar sort of words they will have a high attribution similarity and what is relation similarity so that is slightly different that is now i'm talking about pairs so that is i have two pairs a b and c d are they relationally similar that is how many co similar relations that they have so example would be like one pair is dog and bark second is cat and meow so now dog and bark do they share similar sort of relation as cat and meow so this is simple different type of task ok and so this that's why first one is called attribution similarity how much the attribution similar among the two words second is called relation similarity i have the pair the relation between this pair does that hold also for the other pair ok and this also gives tries to many analogy testing task so a is to b as c is to what so we'll see how do we extend our distributional seman similarity or semantics models to also capture all these cases now for that we will talk about a different sort of matrix and this will be called pair pattern matrix so till now you are talking about target context matrix so let us see if you can use a similar idea for building a pair pattern matrix so what do i mean by this so here the row vectors will correspond to various pairs of words like mason stone and carpenter wood and the column would be various patterns in that in the sentences these words occur with so like here x cuts y x works with y etcetera and then you compute the similarity of rows to find similar pairs of words so now so what what do i mean by this so till now what we were doing we had words like dog cat and i was finding out these representation in various contexts so i'm finding out vectors for dog vector for cat and trying to match these ok and this was my simple attributional similarity so now what am i doing i'm trying to compute a similarity between pairs so now my rows are different pairs so like the pairs can be say dog bark cat meow or it can be like here mason stone carpenter wood so like that they can be various pairs now these are my rows now what do the columns denote now columns would be various patterns so pattern could be like x cuts y x works with y and so on these are various patterns now what are x and y you can think of this as my patterns x y sorry pairs x y now what ha how will i fill this matrix simple way would be i have x and y x can take value like mason and y can take stone for a given pair now i will go through my coppers and see what are the various patterns in which these words co occur with so suppose there is a sentence mason works with or ah mason works with stone or carpenter works with wood let's take a sentence carpenter works with wood so here so i have pattern x works with wood x fits for carpenter y fits for wood so i'll say ok there is a plus one here similarly there will be all sorts of pattern here in which x and y can occur and i'll fill which pair occurs with what patterns so now this is my pair pattern matrix pair pattern matrix and then i can once i have this matrix i can compute which pairs occur in similar patterns and then they are called ah relationally similar they have similar relation at the other pair so so then so we can talk about the extended distributional hypothesis this that much given by lin and pantel so what is the idea patterns that co occur with similar pairs tend to have similar meanings so here we are talking about in terms of our columns so here suppose what they are saying if a pattern p one and pattern p two if they co occur with similar sort of pairs then they are giving similar sort of meanings ok and and therefore i can use this matrix to compute semantic similarity of patterns also so i can find out the semantic similarity of the pairs also the patterns so suppose i'm given a pattern like x solves y and i want to find out what are other similar ah patterns as x solves y how will i do that i will first enumerate all the possible patterns that can occur with with x and y then i find out all the possible pairs how many times they co occur with various patterns so i'll fill this matrix and then i'll find out for this pattern like x solves y what are some other patterns that us that are having similar pairs as x solves y ok and what are patterns like x is solved by y sorry y is solved by x ok suppose it occurs with similar pairs as x solves y i'll say that this and this are same and that will be the idea its like here y is solved by x y is resolved in x and x resolves y so what do you find all these patterns occur with similar pairs so they can be also called similar now so now what is one thing so for dealing with this pair pattern matrix words will not be my basic context units so how do i capture and represent this sort of information like x solves y and y resolved by y x how do i capture this information so for that i'll need a formalism that can capture semantic relations and also ah various syntactic information can be captured and for that we will go back to our dependency based formalism to capture this kind of information what is the idea so let us say ah i have a sentence like the teacher eats a red apple so i can first formulate a dependency graph for this sentence now i can use this dependency relations to say that only some sort of dependency relations are interesting and others are not interesting ok so for example i can say that eat is not a legitimate context for red ok although eats and red co occur with the very small distance context window i can say that ok the relation det object an and a modifier together do not form a very nice context so i'will not use this co occurrence so i can be selective in choosing what kind of co occurrence information i will use and what kind of co occurrence information i will not use and i may also give them different sort of weights so for example i can say that the object relation connecting eat an apple will be different than the modified relation connecting red and apple ok so this relation det object and a modifier at can be different relations so till now what was happening i was only seeing if this word co occurrence with this word or not so now we have started talking about in different terms this word co occurs with another word in this context ok so with using a det object relation with using an adjective modifier relation and so on now so so from the parser i can get all these relations so how do i further use those so ah so we'll say that to qualify as a context a word must be linked by some interesting lexicon syntactic relation so wha what do i mean by lexicon syntactic relation a good dependency path should adjust between the two words so in in simple terms i can only use of use a single edge between two words ok but in general you can also talk about ah larger path larger len length of edge between the two words so let's take it simply for the simple paths of length one between two words so what will i do so let us see i have a sentence the virus affects the body's defense system and you get the dependency parse now from the dependency parse i can extract the various triplets like system det object affects body possessive system so these are the various ah topples i can extract from a dependency graph so i will have things like this system det object effects and so on now how do i use those for my ah structured model so let us try to ah have a look so we will have words like system d o b j and affects yes and i have many such pairs many such topples now how do i use those for my distributional semantics so there are actually many ways you can do this so one simple way would be just forget dependency information so that means you will have system affects and so on so that is you going back to your earlier relation where you are not using the instruction so this you can again convert into that kind of model the word system is here and what is the co occurrence with the word affects and you will have one and so on there is one way so where you forget the dependency information but that's not what you wanted right you wanted this information for some reason so another option could be i combine dependency information with the context so my context is now structured so that is i'll have system and my context is det object of the verbs affects ok so these are my context now so then i can represent system in this context det object of affects det object of other verbs and subject of these verbs and so on ok so now you see immediately my dimensions are different from here here i had only words here i had verbs and some relation together but i'm going back to the same sort of matrix format and still i cannot use this dog is to bark and cat is to meow i cannot do it here now what is some other sort of representation from here that you can gather so other could be i combine these two in my rows this become my target and this come become my context so that would be third would be so like system and affects coming my target and det object is my context so now we can talk about my pair matrix so i have systems and affects these are my pairs and pattern right now only dependency relation det dependency relation det object and subject and so on ok and this is the general framework pair pattern now here we are talking about only very simple paths of length one what is the det relation between these two words you are free to choose a higher order path it can be ok there is a path of length det object n subject and so on and use that at your patterns here ok so so that can help you to capture all these things like x solves y so you can also use what are the different words that occur in between these two words if there are some other words occurring that can be your one of the pattern so here you have complete freedom of choosing what are your pattern so these are simple dependency graph for for simplicity but you can choose any other patterns that you would like these patterns come from may be dependency graph come from the word co occurrence how what are the words that are occurring between these words and so on and then you can compute similarity between various pairs ok so now quickly so we have this data and i can use that to get this information or this the other sort of information or the third sort sort of the information that we saw now let us see one simple application like how do we use that to find out selection preferences of the verbs now what do i mean by this selection preferences now different verbs for their different argument like object subject they prefer a particular type of noun and this information is useful in many task like dependency parsing so for eat i want to know that eat will prefer only certain ah sort of nouns as objects ok you can eat only verys very ah specific things so like so how do we compute this selection preferences for verbs so we have seen that that from a parsed corpus i can ah compute these vectors like virus and system i can put them in this space how many times car occurs as object of carry so you are carrying car you are buying car you are driving car you are eating car and you are storing car and flying car and so on ok so what are the number of times and this is some normalized values vegetables how many times are they carried bought eaten stored and so on ok so this you can compute from the corpus once you have the parse so this gives you some sort of representation that what kind of objects come into that can be used ah ah what kind of words that can come as object of the verb buy what kind of words will come as object of the word verb drive and so on ok but suppose you want to build a prototype that is in general what kind of words can come as object of the verb eat from the corpus here one simple thing i can do i can find out which words has have a high value so you know vegetables can be eaten biscuits can be eaten so they have a high value but suppose i want two hundred prototype for a new wor the words that is not occur in the corpus how likely it is to come as a object of the verb eat so what will i do so what is the simple method so i would say ok let me find out what are the words that are having a high weight in this dimension so you know vegetables biscuits fruits etcetera will ga will have a high weight in this dimension now my hypothesis will be words that are similar to these words like vegetable biscuits can also be eat now how do i find out words that are similar to vegetables biscuits and so on so for that i'll build a prototype that is ok what are these words so these are words that can be carried bought stored right so that means any other words set can also be carried bought stored might also be ah eaten ok so then i will find out all the other words that have high weights in these dimensions other than eat because whatever is having high dimension eat i can easily capture here but what are the other words that are having high dimension in all these high weights in all these other dimensions that also can become my prototype so what will we do suppose i want to compute the selection preferences of the nouns as object of the verb eat i'll take some top n words like vegetables biscuit that have high weight in this dimension of object eat then i take the complete vectors so that is of all these top nouns so this will words like that can be consumed bought carried stored etcetera now this becomes my object prototype now given any noun try to match it with this with this object prototype and you will see that how likely it is to come as a object of the verb eat and that is the generic method of finding selection preferences of an word noun for any ah word noun to come as a object ah or subject for verb so we talked about ah how do you extend your distribution semantic method for using a structured models ok a lot of work again a lot of research has gone to this si this domain so you have already touched the basic and i hope if you if you need this idea you on your own you can you can think about how do you use different sort of interestings context interesting targets and and solve various problems so in the next lecture we will talk about another very import important interesting idea that is what are word vectors so and word embedding and how do we obtain them from the corpus and and what are different tasks they can be used on ok so i'll see you in the next lecture thank you