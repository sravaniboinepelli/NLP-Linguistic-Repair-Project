 welcome back for the second lecture of this week so in the last lecture we started our discussions on distribution semantics and we took a very simple example in the very naive method we constructed that distribution matrix and try to compute the similarity across words now you will be see this this concept much more formally that how do we construct such models and what are the different applications that that we can use them all we will see some some examples so firstly let me just go back a step and and see why do we need to talk about the distributional semantics so if i talk about a vector space model without distributional semantics what would happen so there words would we treated as various atomic symbols so what do i mean by that suppose think of your semantic space as corresponding to various various different words that you see in your vocabulary so if your vocabulary is of size v so contains word with index one two up to v and i want to give a representation to this these words in the vocabulary some representation so what is the representation that i can give without any ah looking at any distributions so i will see that say that ok each one is a different dimension dimension one dimension two up to dimension v ok and how do i denote a word in these dimensions so suppose i have the i th word so i will say so let me denote it the i th word this will be a vector in v dimensions where only the i th element is one everything else is zero ok so like that i can give a different representation to each of the v words right where only one element corresponding to the index will be one everything else will be zero and this is also called the one hot encoding only one of the entry is one everything else is zero now so there is also my semantic space where all the v dimensions correspond to different v different words now what is the difference here i am not capturing distribution i am just saying this word is the i th dimension nothing else now can i do semantics with that or can i capture the meaning of the words with that each word has a different vector now suppose i want to find out if wi and wj are more similar than wi and wk so what is the similarity between wiwj and wiwk so what will happen if i use this encoding so i will find out because here the i th element will be one everything else will be zero here j th element will be one everything else will be zero so if i try to take a dot product i will get a zero in both the cases even if you find that these two words are looking similar than these two words and that is one problem with using on the one hot encoding so suppose i take two words here like motel and hotel and suppose motel occur at this index it will has one everything as zero hotel occurs this syntax this is one everything as zero so if i take to take try to take the dot product of these two i will get a zero and if i if i take hotel and book that also will give me (Refer Time: 04 :00) a dot product of zero so this will not capture that hotels and motels are much more similar than hotels and book now what do we do in distribution semantics so we use this idea that you know a word by the company it keeps so instead of putting only one one and everything else zero try to model the distributions so suppose i take a word like banking i will not denote it by a simply one at its syntax but i will see what are the other words that come its context so if we talk it is occurring in in the corpus with government dept problems turning crises europe regulations replace etcetera then these words will be use to represent banking so now what will happen so instead of my one hot encoding i will now go to a distribution representation where i will say for the word like banking how many times suppose this is my dimensions corresponding to government so how many times it occurs with government say two times similarly here it is my ah dimension corresponding to say regulation how many times it is [vocalized noise] occurring with regulation so this is i am doing for bank now i can do similar thing for something else like economy and so on what they will find probably the word banking economy have quite similar distributions than the word like banking and something like play yes so instead of having a on hot encoding i am having a distributed representation and thats what is being done by these distribution semantic models so now how would i build a distribution semantic models step by step so you will take a corpus that is the ah that is something that you have require that contains a lot of data the the various usage of different words in the sentences now you will do some pre processes to define what are your target words and what are your context words context can be context word or something else we will see some examples now once you select what are your targets and context from your data and what is the next task next you will count how many times the target co occurs with various contextand you will give some weights to these contexts it can be the simply number of times they co occur or some function applied over this this number and we will see again see some examples for that now once you have given the weights you build the matrix distribution matrix now some optional step is if you want to reduce the dimensions of this matrix because the dimensions in general can be very very high if you are taking talking about a large corpus you might have millions of words as your context so do you want to further reduce the dimensions then once you have the representation in that reduce dimension you can also try to capture the similarity across words so you compute the vector distances on the reduced matrix and this is the overall pipeline for constructing a distribution semantic model now here there are many design choices that you might have to make for example we said that we will define what are my target words what are my context words now it can be i there can be many many variations so symbol is could be my targets are words and contexts are documents and i am saying how many names this word occurs in this documents and further i can give various weights so this is one matrix type another could be word cross word how many times this word occurs with another word ok then again keep on going so how many times this word occurs with that word in certain such proximity or this adjective occurs with this modified noun and word with dependency relation noun and verb with its argument and so on so i can have various different sort of matrix types initially we will only focus on the ah first and second that is my word is the target and document is the context or word is a target and word is the context now once i have this matrix what are the other design choices then how do i weight the elements should i use the probabilities to weight them should i use length normalization tfidf pmi positive pmi or positive pmi bit discounting what is the method that i should use for for weighting the the entries in my ah matrix ok then if i am doing [vocalized noise] dimensionality reduction there are the many methods lsa plsa lda is also one sort of dimension reduction method pca so on then once even i have done that how do i compare two different vectors once i have built the all the vectors in this manner should i use the euclidean distance cosine similarity or dice coefficient jaccard coefficient etcetera so they are lot of design choices that you might have to make and each individual on might make a different sort oF distribution semantics model but the underlining idea is the same that you want to capture a distributions from a large corpus so now and then there might be different questions that you might be going to answer that once you have built the distribution matrix how do different rows in the matrix let to each other and how do different columns in the matrix let to each other so so what we have seen a number of parameters that we need to fix like what type of context should i use what weighting schemes are use [vocalized noise] yes and what similarity measures should i use ok and different models might be a different settings of these parameters n now lets starts with the simplest case where words are the targets and documents are the context now here is a simple example so you areseeing some words as a as a rows and documents as columns and what do the entries denote that against occurs in the document d four one times in d seven three times d eight two times d nine three times document it occurs it does not occurs ok and thats how you are represent giving a representation to the word against these doing without any weighting just simply the row counts and this you can be very easily find for any different any word so in information table also this is done for each word how many times it occurs in various talkings on the other hand suppose your context are the word then you are trying to show how many times a word occurs with another word so in this particular case so the word against occurs with age ninety times age agent thirty nine times and so on and this diagnoses tells how many times the word agent occurs ok so it is being computed ah by ah as if co occurring with itself so so here so again you will you will define a context window it can be a sentence or something and you will find out how many times the word air occurs with against and age occurs with age and so on this you can do for all the words or a specific set of word that you have already chosen for your analysis this is simply the count now ah if i am using my words as contexts document contexts are easy because i know what is the context window size this is the whole document so i will say whether this words occurs in the document or not or how many times it occurs now suppose i am using words at constant now now the question comes in what should be the size of my context window should i choose only the co occurrence within two words occurring or within a paragraph within the whole document and this is the [vocalized noise] design choice that you can make depending on are you trying to measure ah similarity on based on some very close co occurrences or you are alloying even a some very very distant ah co occurrences in the same document so so what wsa the parameters here what is a size of my widow ok so how far am i looking it and what is the shape of my window is a rectangular triangular or something else let me just briefly let what do i mean by the shape of the window so that is suppose i am having a word w ok and it has some context so i am trying to see how many times this word occurs with these words around the context so suppose this is the previous word previous to previous word next word next to next word ok so my size is am i looking at only two words around it or more than that three four and so on this is my size so this is the size and what do i mean by shape so we said it can be triangular ok so triangular will be something like this was is rectangular what is the difference in triangular what i will do as i keep on moving away from my target word i reduce the ah the count i will say ok this count i will treat as one this i will treat as point five so this is the strength is decrease as you go away from the target word similarly this can be one this can be point five and you can as such were give it any shape like exponential or whatever ok in that that way you can capture even much far the co occurrences in that you know what you will what you will do you will say ok i will count each co occurrence size same same way so each has a weight of one or whatever you give so this is the idea of window size and window shape now what the so so so let us take one example ah from some these article so this is my passage the suspected communist rebels on fourth july nineteen eighty nine killed and so on ok this is my passage and i want to capture the co occurrence over there are so so what i will do i will first define what is my window size suppose i have a window of size five i want to use window of size five where i am taking two words either side of my target word so what will the window look like take the word like rebels here i will find out its co occurrence only with two words on the left and two words on the right so here suspected and communist come here on and four come here similarly when rebels occur here says and communist come here have and killed come here so these are my context words in the window so i will only define rebel rebels coming with these words everything else i will not i will count as zero so this is my unfiltered window ok i am not filtering any words i can also use a filtered window where i can filter the words so something like that so i will take two words either side of the target word but now these are filtered so that they do not contain the stop words so here on the left i have the two words suspect and communist but on the right i remove the words on four and a number like nineteen eighty nine and i am taking the words july and killed ok similarly here i have says and communist in my window but not have have is like a stop word so i have remove that and then killed is there up to sixty five is not there and soldiers so you can take either the filtered window or an unfiltered window now how do i weight the context so again let us take the two cases when my documents are context was is when my words are context so when my documents are [vocalized noise] context how should i weight the occurrence of a word in my document so so what it should depend on so in how many times a word is occurring in the document what it should depend on or what the weight should depend on so i am denoting a word in documents and i am going to give a weight suppose it occurs five times in this documents ok and another word occurs seven times and now i am convert that into some weight so what should thethis weight depend on so what can be the the the different ah parameters on which it can depend firstly if a wordoccurs more number of times it should get a high weight ok so this weight should the proportional to number of times it occurs and we call it the term frequency how many times a word occurs in a document what else if the document is very very long so you might have take that window count so suppose i have a document d one of size hundred and i have a document d two of size thousand ok and i know that word one occurs in d one with five times and it occurs in d two ten times i cannot simply say that in document two word one has a high weight because document two itself is very very long so the weight should be inversely proportion to the length of the document ok as the length of the document increases the weight should also reduce and what can be the other measure another measure is very very important also you can you can ah even know that if you have ah heard about ah some information debate topics so this is called the inverse document frequency so that is idf inverse document frequency and what is the idea so idea is if a word occurs in many many documents through my corpus was such if a words occurs in only selective documents whom should i give a more weight ok so that is suppose my word w one occurs in this document five times and w two occurs seven times that is ok but suppose i also find that w one occurs only in three documents overall and w two occurs in fifty documents what can i say about the nature of the these two words so one thing i can say is that w two is probably a very generic term and w one is a very very specific term ok this becomes a generic term and this is a specific term so idea is that specific term might have more information about the document than a generic term because generic term might be occurring in many many different documents so i want to give a high weight to w one because it occurs in very few documents and it has occurred here so that means i will give a weight proportional to one divide by number of documents that it occurs in ok so i can call it a document frequency if the document frequency is high i give it a small weight so these are three different parameters or factors on which my function bit will ah depend on how many times a word appears in the document then what is the different number of words that occur in a document and how many different documents a word occurs in and as we saw its weight should be proportion to the number of times that occur in the documents should be inversely proportion to the other two factors and there are various different induction functions that that have been proposed that take into account all these ideas and one very common measure that is used a tfidf that is i give a weight of proportional to the frequency fij times log of n by nj ok so nj is the document frequency so if it is higher i give it a smaller weight and where tis the document coming in to picture document length so because once i compute the tf idf for each individual term the document i take the l two norm so if there are lot of terms the document the individual weights will be reduced ok so this is one very ah commonly used ah induction function called tf idf i see what are the number of times that word occurs in the document and how many different documents that occurs in so one thing they have many variations that have been proposed for this function this is the simplest function that has been know for tf idf and finally we have to take the l two norm this is something that we should not forget that i have to [vocalized noise] finally normalize all the different values in a single document so that some of the squares adds up to one so now suppose i take words as my context instead of documents then what is the interesting weighting function i can use lets take an example so here my target words is word is dog in both the cases context word is small and domesticated two different words as my context and what you been shown here so target word has the same frequency yes but the context words in one case is small as if it sorry small has a frequency of four ninety thousand five eighty and domesticated has a frequency of nine hundred and eighty so you can clearly see here see here that the word small is very very common and domesticated is very specific term now suppose i find out the co occurrence the dog are occurs with small eight fifty five times and dog occurs with domesticated twenty nine times now my question is what number i should used denote how much dog co occurs with the word like ah small and domesticated so if i do not give any weights what will happen here i will say dog occurs a lot with the small eight fifty five times and dog occurs very rarely with domesticated twenty nine times ok but that is not capture the whole picture so what is the whole picture domesticate is the very very rare word it occurs only in nine hundred documents out of which in thirty documents occurs with dog a small occurs in four ninety thousand documents out of which only in eight fifty five documents it occurs with dog so can i use the idea that if a word is rare word its co occurrence with the target word should have a higher weights than if the word is very very common word ok so that is can i use the frequency of the individual words also when i give the weights to their co occurrence and thats what we do why using various association measures that i used to give various weights to the context and the idea is that the less frequent target and context elements are the higher the weight you give to their co occurrence count ok so in this case what will happen so because my context element is less frequent here in domesticated their ah occurrence with dog should be given a high weights so co occurrence with the frequent context element small will be less informative here than the co occurrence with the rarer word domesticated and there are various measures that captures that like mutual information is very very popular measure and also log likelihood ratio etcetera they try to capture this so how common and rare or rare the words that among which i am finding co occurrence are and how many times they co occurred together so they try to use both of these into a single function so what is my what is the function for point wise mutual information for two different words w one w two i will find out in mutual information what is the probability that they co occurred together in the corpus i will divide it by what is the probability that they would have co occurred together had they been they been occurring independently ok so this mutual information tells me how much information do i gain by seeing how many times they are co occurring that i that i wouldnt have obtain by their random co occurrences so so thats the formula thats the formula so what is their probability of of they co occurrence in the corpus divided by what is the probability of a co occurrence in the corpus if they were independent now how do we capture the probability of their occurring of they occurring independently in the corpus so that would be i will say ok the probability with which w one occurs and among those times w two could have occurred again with this probability so it will be the probability of occurrence of w one times probability of occurrence of w two so so this is a function that i can use probability with which w one w two co occur divide by probability with which w one occurs times probability with which w two occurs and i will take a log of for that and how do i capture probability of occurrence in a in the corpus for two words number of times they co occurred together is a simple co occurrence count divided by n so n here would denote the different ah number of context that you can see or in in or it can be also the number of different tokens that you have in your data similarly probability corpus of the word w is very simple this is the unigram model number of times this word occurs divided by a num all total different ah number of token that you have seen so so in pmi what would happen in in sudden cases the value might also go to ah negative values so what we what is some variations there so you can use up only the positive values and this is called positive pmi ppmi so where only those ah values that are greater than zero are taking into consideration everything else is converted to zero now there is one problem in this mutual information approach that there is a biased words some in frequent evens so remember why why we came this idea of mutual information we wanted to give high weight to the evens that are in frequent ok if two words are rare we do wanted to give their association a high high weights but what happens if two evens two words are very very rare they get they might get some high weight ok so some one particular cases suppose think about the scenario with wi and wj have the same occurrence in the corpus as w i wjwj together ok now what to do the probability or the mutual information in this case so pmi between wi wj is log of probability wi wj divided by probability w i times probability wj and suppose all three are equal so what would happen this would cancel so this will be log one by p wi so what it is saying if these three are equal pmi will be high if pwi is low and immediately that that gives a bias in in the favor of events that are smaller though or words that occur very very rarely ok so you can think of two scenarios where all this three probabilities are one by hundred or versus all these three probabilities are one by ten thousands so either we i would like the association to be the same in both the cases but what would happen in this formula the association become high when the probability is low and this is not desired ok so this will create a problem when their ah individual probabilities are very very low this this immediately will become pmi will become very very high and there are another ah another case where you can see that if the word wj occurs only once in the corpus and it also occurs that time in with wi so what would happen so pwiwj and pwj will become similar here also because what i am i saying wj occurs once also with wi ok so pwj will be one by n and pwiwj will also be one by n so if i put that here it will again come out to be this formula and that means it will the pmi will depend on wi if wi is frequent pmi will be low if it is rare it will become high i mean this is not desire so to evaluate this problem so they are two different things that you can do one is you can ah probably start by the moving all the words that are having very very low occurrences so all the words that are occurring less than two or three times you can remove from your data all together and huh the words that are occurring more than that this problem will be not ah not that bad but if you want took also take into considerations the very very infrequent words so what you might have to do you might have to ah takes into consideration some sort of ah bias so what is the idea so there is this very important discounting factors that are proposed by pantel and lin so what it says that you multiply the pmi value with this discounting factor that is fij divided by the fij plus one times minimum of fi fj divided by minimum of fi fj plus one so what would happen now so you will multiply this with pmi so is a frequency of the individual events are very very small this factor will become smaller so like thats take two cases here one where fi is equal to fj is equal to fij is one where they are even second fi is equal to fj is equal to fij is equal to ten not so rare ok so in the first case what will be the discounting factors one divided by one plus one times one divided by one plus one this will be one by four in this case it will be one divided by ten plus one times one divided by ten plus one this will be sorry ten divided by that so this will hundred divided by hundred twenty one so discounting will be much more higher here so you will divide the pmi by point two five here you will or multiply by point two five here you multiply by roughly point eight and this takes care of the problems that we had talked about in with infrequent events so whenever there are infrequent pmi will become very high so if you use this discounting it will again come back to a reasonable value and this will not create a problem when the evens are not so rare so here is one example of what kind of vector do you get by taking this pmi ok so on the left hand side you have so so what is done here you taking a large corpus and finding out for individual words what are the other other vectors that i having high pmi values and then you are doing some normalization so that all the pmi value is on this course for a given word add up to one so what do you see here if i take a word like petroleum you find words like oil gas crude barrels exploration that are coming out to be having very high pmi values with drug you find words like trafficking cocaine narcotics insurance you find a words like insurers premiums lloyds with forest you find the words like timber trees land and robots robotics you find word like robots automation and so on and all this is coming out just by putting this pmi function over this corpus where different words and sentences are ah are put together ok i am computing this function and finding out for a word what are similar words and you can see that it is capturing very nicely what are the other very similar words to that ah to s starting word like robotics you find words like robots and automation but the word like forest you find immediately words like trees etcetera and this can give you nice intuition that you can use that very nicely to capture which two words are similar which two words are very very different and in in some of application you can make use of that so i will start from here in the ah so for the for the next lecture and we will see how we can use that for some very interesting application like ah term information i will define the problem and see how do you use that thank you