 welcome back so so we talking about distributions semantics so in in this fourth lecture in the fifth lecture of this week we will talk about another very interesting idea that is word embeddings that come from distribution semantics ok so what are word embeddings ok so when i am talking about word embeddings so they are like word vectors so ah so the word vectors are nothing but simple vectors of vectors of weight ok so you are so we talked about these factors so its so you are having some dimensions and they are representation words in this set of dimensions on their various weights so now we also talked about this one hot encoding that is among the n dimensions only one dimension is one everything as a zero and these dimension might correspond to the index of the particular word that i wanted to be set now and we saw that if i if we are using one hot encoding i cannot ca ca compute the similarity among words so if i motel and hotel there will have one at different places and if I compute similarity if i do and it will be zero so this is not ah conducive to for ah semantic similarity between words so no let's take a simple example so have a vocabulary that contains only five words king queen man woman and child so the so i have a I have vectors in five dimension so i can encode queen like this so queen has a weight of one in the second dimension that correspond to the index of queen and it has weight zero and other dimensions and i cannot compute the how similar king and queen are how similar queen and child are by using this method so we cannot make any ah meaningful comparison we can only find if these two words are same so that is not very interesting so what happens in the distributional representation or so we also called it word embeddings that any word w i in the corpus is given a distributed representation ok by in embeddings so i have a fix dimension like d dimensional vector and I represent each word in these d dimensions and i have give them various weights and these weights are to be learnt by some method and we will talk about what will be the method by which i will learn this weights so idea is that each word in my vocabulary i will try to represent them in some fix dimensions d so now so so this is one idea so like i have the word linguistics and i can denote this word in some fix dimension here d is eight so there are eight dimension so they it has different weights in different dimensions and similarly all the words will be represented similarly they have so different weights in these a dimensions so so what is my distributional representation so take a vector with several hundred dimensions so it can be hundred fifty three hundred thousand now each word is represented by a distribution of weights across these elements so each vector is nothing but a distribution of weights across these dimensions so what will happen so instead of a one to one mapping between an element in the vector and a word you have a ah distributed representation of each words and by using that you can cap capture similarity so whether two words are ah similar or not if they have similar weights in ah many dimensions then they will be similar and that's how i can capture similarity between words ok and yeah so we we can say that each elements in my vector or each dimension might contributes to the definition of multiple words so what the dimensions might indicate ok this is just in illustration ah this is not ah a literal meaning ok so thats what you can ah thats what can you can help you to understand what are these dimensions so suppose all the dimensions in my distributional representation i can label by some hypothetical ah labels ok so my algorithm may not have some labels some very ah good labels like that it may not be possible to do that even manually but this is simply for understanding that assume that so there are d dimensions assumed that you can assign a label to these some topic some concept ok like here my dimensions can be like royalty masculering famining age so on these are my dimensions and assume that the weights that each for is have in this dimensions correspond to how much that word is closer to that concept so for example the word king word how much it is closer concept of royalty so this king is very close to concept of royalty it has a high weight in this dimension point nine nine it is very close to masculering so it is has a high weight point nine nine but very small weight in famining age suppose that a high weight means a ah it's elder so it's point seven similarly for queen what will happen royalty will get a high weight yes but masculering and famining will be just opposite and age can be again ah three point six now take women and princes women will have very low weight in the in the case of royalty high weight in the case so famining and princes will have a high weight in the case of royalty and high weight in the case of famining and very low weight in the case of age and this is simple simply for illustration so idea is that i am trying to represent all my words instead of fixed dimensions and these dimensions are latent they met correspond to certain concepts a combination of concepts and so on that may not no that the algorithm also does not assign but we would assume that that these dimensions correspond to some concepts and now each word can be return as a distribution among these concepts ok and then i can measure to words based on how much similar they are on various concepts this is the idea ah main question is how do i capture this representation that how do i represent different words in this fixed dimensions so now we can see that so such a vector is representing the meaning in some abstract manor so now what is my ah dimension size so my dimension d can be if we starting from fifty up to thousand ok and the by ah focus is that the word that are similar in meaning they should have similar embeddings or similar representation now if you know about ah s v d singularly de decomposition that is also some sort of embedding method that converts the vectors in some low dimensions so i will i will i will encourage that you read about singular that valid decompression also another another name for that is latest symmetry indexing but i will just give you very briefly what is the idea of s v d so we talked about ah this co occurrence matrices ok so this is my matrix a so these are the target words these are contest words ok and this matrix has certain entries now what is one problem is matrix this might be very high dimensional so each word might have a say five hundred k dimensional representation so so what did what this is co occurrence with different words and this words can be age number of words can be age age five hundred k and even more in some cases so now i want to give a low dimension representation a distribution representation so what will the idea so i will use the theorem that any matrix say can be written as u sigma v transport ok and there are certain properties of these u sigma and v but this sigma is a diagonal matrix and the n this are singular values ok second here about it what is this matrix particular decomposition for singular values now once you have this matrix in this format this is same matrix so idea is that you take a k rank approximation so that means you have singular values they are is in the decreasing order and you take only the top case singular values and you do that for all this u sigma and v so we have taking only top k and t so only the first k entries of you corresponding to top k singular values that is u k sigma k v k transports ok and this is my k rank approximation of my matrix a ok so now this u will denote my low dimensional representation so earlier you might be in the same ah dimensions you can be again in ah five hundred k dimension ok five hundred thousand but suppose you you you are trying to pick only hundred you will take only ah first hundred dimensions from here ok and this will be a low dimensional representation for u v and so on so s v d is also one sort of ah embedding so each word you can embedding some ah k dimensions by taking the top case singular values only so but we are not talking about s v d here we will talk about the word vector method for computing this representation so now before i discuses what are the different task sequence do with these word vectors oh ah sorry how do you compute these word vectors let us see what why they are found to a very interested in in in this domain and what are the different task they have been used in so what is we found that these representations are capturing some meaningful syntactic and semantic regularities in a very very simple manor so what is that so that is we can use the vector offsets to talk about ah relation between words and how how much two words ah are similar compare to the other pair of words so for example i want to capture the singular plural relationship between word like car and cars boy and boys bag and bags and so on and suppose i do not know what are singular and plural words so can i capture that using word representation so what does we will found suppose that for each vector x i u ah for each word i you are vector x i so we can take the vector offsets and they will be coming out to be similar for ah singular to plural pair so that is if i try to compute x apple minus x apples that is coming out to be very close to x car minus x cars similarly to x family minus x families and so on that is i compute the vectors of these and if i take the vector offset apple minus apples has similar offsets as car minus cars as family minus families and this is very very interesting this is not something for which this vectors for trained for but they are capturing this ah regularity very ah nicely so this would be like and and and because they capturing such regularities they can be used for various reasoning task like enology task a is to b as c is to what so like man is to woman as uncle is to and you will answer aunt but can my model answer that man i have given man woman pair and for the next pair the first word is uncle what be the next word can you find out aunt and that is what the word vectors have you found to be very useful in they can predict these ah enology sort of task and how do we do that this just use the simple vector offset method so let us see one example so here so this is the idea so i have my vectors denoted in a two dimensional plane so how do you come convert any say hundred dimensional vectors to it two dimension representation you can use principle component analysis p c a or some other methods to project them into some lower dimension so that is been done here so two dimension representation so what you are seeing here so that outside would be woman and man that is similar to what has been observed in uncle and aunt and king and queen and this is a very nice regularity similarly here for singular plural king to kings and queen to queens they having similar offsets so we can use that for enology testing so what is the enology testing task you are given a pair with a with a certain relation like france and paris what is the relation so paris is the capital of france now we are given various examples and we have to find out which of this examples attribute the same relation so that is whether itally rome has its relation of capital and country japan tokiyo and florida tallai tallahassee which of these has the same relation so can my vectol vector representation ah capture this so a given one example can you find out the other example and so so similarly for big bigger can you find out this small ah smaller cold colder quick quicker and miami florida can you find out other examples so on and how do they do that so i have this task a is to b as c to what so example is man is to woman x king is to sorry ah man is to woman as king is to what and how will they do that so simple idea is take the vector offsets between woman and man and add it to king ok so find out woman minus man add it to king but how do you find out the word queen in generate may not be then exact match so how do you find out what words are coming closer so this the idea so i have a is to b as c to what ok so so what will happen in my vector representation a b and c i want to find out what is the word d so what will i do i have the vectors of each of the words so find out the offsets w b minus w a added to see now i am trying to find out which vectors or similar to this ok i write it this and were what are the vectors are similar to this one and so i will just find out ah similarity of that to all the words w i in my corpus and i can also normalize it w b minus w a plus w c ok so that is a normalize all this is not very important and we take the argmax over all w i in my corpus so all words are vectors so i find out which words are coming closer in this is space so what is the closer words to by when i add this offsets to this word and that is my answer ok so so for example if you do this here so will find that the word queen comes up and this has we shown in many different cases like country and capital vectors china beijing russia moscow japan tokyo and the what you are seeing here the offsets between the vectors are very very regular in all this cases and this is just coming out from the word vectors and more questions like news papers so new york new york times baltimore and balitmore sun san jose san joshe mercur mercury news and so on various n b a teams detroit detroit pistons an and so on airlines austria austrian airlines belgium brussels airlines and company executives what do you seeing it is capturing a generic relation also in journal any relation that that that can hold between two words so if if you are finding out two words with one relation you can find out some many other words that i having the same relation by simple this vector offsets method so find out vector offset between pairs is it similar to the vecs vector offsets of my initial example so this is the problem we also tackled in the case of ah instruction models of distribution semantics that is we have making the pier pattern matrix and then capturing the co occurrences in the in the case of word vectors even you even though you did not is start by piers and battles even the though you you found out only with the word embeddings word vectors this helped further in doing this task also and this was very ah one of the very interesting aspects similarly here we can do element element wise addition so suppose you have the embedding for a german you have embedding for airlines and if you add these two embeddings and find out words that are coming closer to to the new vector now and you find out some very interesting words coming up like here check plus currency and you find out word that very close vietnam plus capital again words setup very close german plus airlines you find lufthansa airline and lufthansa a carrier so on russian plus river you find words like moscow french plus actress and you find some transact message and this was again some interesting accept so you can have embedding of two words and you add these gather if find something that is some sort of composition of these term so not a generic method so this was coming out in some cases may not be true for all the cases but even coming out in some cases for what in interesting observation so now how do we capture these word vectors how do we compute this word vectors now basic idea is we will again go back to the co occurrences we will trying to use co occurrences but instead of counting the co occurrences from a coppers what we will say i am given a sentence a word is there and the contest is there try to predict the contest from the word or the word from the contest and if you not able to predict try to update your weights and this will be the idea ok it start with some initial vectors using those vectors try to predict from the contest what to do the targets of on the target what will be the contest if it is not machines update your vectors and so all the codes ah as well as the learned vectors are available here so we can actually download these and and and try to use them for many of your task also but you will discuss how what are this word vectors in how to they so in general there are two different variation of this models so that tried to capture these word embeddings and they are called c b o w for continues back of words model and ah skip gram models and let me just quickly explain what are these and we will discuss in little in the next lecture so in continuous back of words model what you are doing so you're taking the contest so you are focusing on the current word w t we taking the previous words so it can be actually any number of previous words and to next words and your trying to use those to predict the center word so using the contest pred predict the target word or the center word in the skip gram model what you are doing you are using the center word and trying to the context words so there are two different ways of learning is embeddings so idea would be i will start with some initial vectors now trying using the context vectors i will try to predict what is my center vector if i am not finding the match i will update my r different vectors send for send the as well as the context and I will keep on doing that until i am able to predict this some high confident or i am i am converging at certain point my vectors are not changing and these vectors that I am learning by this method will be my word vectors ok and i do that slightly different in both continuous back of words model and scripts come model so in the next lecture what we will do we will discuss in detail how do we learn this vectors ok thank you