 so welcome back for the final lecture of this week so we are talking about different l d a variants and what are their applications and we talked about three different variations like code rate topic models dynamo topic models and sewage topic models and we saw how they can model different assumptions we can also be used to pair the ah response as as a observation inside the model to to make nicer topic models so we now see in this lecture we will also see some two other variants so one is called relational topic models another is ah some sort of nonparametric bayesian models and you will see what sort of applications they can be used for so ah what is the idea of relational topic models see so in so when you are talking about data so many a times this is simple text data so where they are not related to each other sometimes this is like a network also so where different observations or different data points are connected together by some sort of graphic structure so for example think about the scientific articles so as such you can think of them as separate different different articles ok this is one document another document different observations in fit a topic model there or you can think of this as these are documents that are also connected what is the connection between the scientific documents with a citation network one paper might cite another paper ok so if it will be citing another paper there is a link between them so you are seeing these observations are also connected similarly think about the web page one web page might give a hyperlink to another web page so again these ob these are observations that are connected then you can talk about friends then they are connected friends in the social network profile with the profile is kind of the document and the connection is like the ah so and you are trying to model the connection so now what we are seeing here that can be adapt our l d a model where such that it can take care of not only the content but also also of the also of this link that is also contained in connection both can this we can this be handled by the topic models and that's where you have this variation called a relational topic models or r t m s so relational topic models try to take care of this variation that i have the content as well as a connection now now so before going to the model how how we will try to do that suppose we did not have this relational topic models what we are trying to to predict here i have this whole set of observations now which two observations will be connected to each other so or let us say i am talking about the scientific articles when i knew a person is writing a new article what are the papers he will cite how will i model this so so i will think of it as a learning problem i will say ok i have a dataset i know there are lot of papers and some papers writing another papers ok so i will say ok i have observations document d one d two d three and so on d n i know d three is citing d two i know d n is citing d three d n a citing d one so on i have these observations right and i want to find out when a new able d n plus one comes in what will it cite how will i solve this problem is suppose using l d a what i will do i will first in l d a over this corpus and i will find out ok this topic distributions theta d one theta d two up to theta d n ok you find that out then i try to model ok given that this is theta d two theta d three will they link together or whether d three will link to d two how will i model this again you can use a regression here right so you can say ok given theta d three and theta d two predict the link yes or no so i have some positive examples where ever there is link and negative examples there were where ever there is no link at run time when i get a new new document using l d a i get theta d n plus one now i try to combine it with d one to d n find out which one it is more likely to linked that is one way it can be handled now does that remind your something so that will remind if the now last topic that we discussed that was supervised l d a remember there also we had the same problem we said ok each document has a response so each document might have a response r one r two and so on so we said ok one way could be take theta d one and run a regression from theta d one to r one theta d two to r two that was one possibility idea plus regression but we said we can do something smarter than that that is we take it as an observation within my model and we say ok with each document with z you are also sampling your response variable with some eta n sigma now can we use the same idea in this relational topic model also so the ah right now the difference is that we are working with the pairs ok so what we can do here we can say that i had document d one i have document d two now this will have some z this will have some z now can i model whether they will link to each other given their individual topics and this can now be in observation inside my model ok so this is idea this is a simple and direct extension of this over the topic ah l d a that can i now use pairs and model the response as my another observation sorry model the connection as my another observation so so this is how it is done so you are having the same topic models sorry same model for so beta k the topic probabilities over over different observations but now you look at different pairs so let us look at these pairs ah document d i and d z so they have theta n theta z variable so what you are modeling given ah the z i n and z j n their individual topic probabilities whether they will link to each other so there it is per pair this is a binary link variable and this is a indirect analogous to what we written in the case sewage topic models is eta times this into this ok plus some variance and so this can also allow prediction about new and unlinked data so how is it acted so what what are the examples so how it will be used to something like this suppose i have a top paper markov chain monte carlo convergence diagnostics a comparative review so and i want to find out what are the other papers it will link to or i can say ok suppose i am reading this paper what are the other paper that are relevant to it like a recommendation problem so i can use this idea let what is the other other papers that it will link to win my r t m model so here is some examples that this is what your r t m model gives and these are actually important papers that this document actually linked to so r t m in general can allow the predictions that given ah a document with new words what what are the document will link to ok this is one thing that can be done also suppose you give a link that this document gives links to another document what will be the approximate distribution of the document that also can be found using my r t m model so links given the new words of the document and words given the links of a new document both can be handled so so coming back to the model so this formulation what it ensures is that the same latent topic assignment is used to generate the content of the document and it also generate their linked structure so ah you are not separately learning the linked structure you are learning it at the same time when you are lot learning your topic distributions so that is interesting in this r t m model so from this z i n and z j n you are trying to predict what will be your y i j ok now here is one problem ah so so this is your simple model so this is your simp ah this is what your lda model is draw your topic proportions then for each word draw the topic assignment and the draw the word ok now for the relational part for each pair document d and d prime what you are doing you are trying to sample the variable y whether they will link or not and this depends on the z d and z d prime and what is the function it's a psi of ah given z d z d prime and this is model like this this is ah as an exponential function over eta transpose a hadamard product over z d and z d prime now what do i mean by that so remember ah in the sewage l d a module also we did eta transpose z bar right and this gave me a scalar two vectors you are multiplying this ah giving you a scalar you have the same dimensions right now here also you have eta transpose same dimensional has the number of topics and you are multiplying it with z d bar hadamard product with z d prime bar now what is what are z d and z z d prime z d is what are the topic probability is doc document d ok and this is in document d prime again they are vectors had not probe probe probe is nothing but element wise wise product see you multiply this element with this element this element with this element and so on and then you add it and multiply with eta transpose so this will again give me a scalar ok and plus you might add some sort of ah bias or valence this is a bias here so z d is nothing but the topics of probabilities in each document use it for the pair and and get a hadamard product of those and plus with bias term gives you ok whether the document is likely to ah ah these two pairs are likely to link or not right eta transpose and this total thing and in in the paper they also use the sigmoid function apart from the exponential function and link function models each per pair binary variable as a logistic regression and this is by eta eta and mu ok so ah so this is your relational topic model what is your what is one problem with this model so for each pair of documents you have to define their response that is whether they are linking together or not so why d one d two is it one or zero then in my document is giving a link to another document you can always say to one but whenever document is not giving a link to another document it is not necessary that they are not related at all it may be that he is not giving the link or the link might occur in future there are many other things depending on the data points so one thing is that whenever there is a link you take it as one whenever there is no link you take it as an unobserved variables so i do not know if there is a link or not so that also helps you in ah being computationally efficient it it avoids getting you so many different pairs of document right so so that is in especially in the case of social networks whenever there is an absence of link you cannot take it as if ah y d one d two is equal to zero for example also you can see from your like facebook profile so you are taking the profile as your documents and you are finding out if this person will become a friend of another person so even if they have not friends at this time it might happen that they will be become friends in future so absence of the link cannot be taken for that their response should be zero so it can be taken as in an unobserved variable that is a better solution so so this is done and and then so they tested whether this words better than the model of l d a plus regression this was the person that we saw that you take the topics from document d one document d two and learn a regression over there that is one option another is you take the pair which are linking as an observed variable and also use it for learning your topics inside the model itself and that they found gives much better performance than l d a plus regression model for this task given any document what is the document it were linked so this is a other examples like for this document competitive environments evolved better solutions for complex task documents like coevolving high level representations were coming out to be it at ah first position in the r t m model but we were not coming out from l d a plus regression model so this was something interesting at the so from this model now finally we will wrap up this week by some small some simple discussion on nonparametric bayesian models so so what do we see in the case of l d a models so in l d a what we are having we are defining that this corpus consists of certain topics ok and then each document i find out what are the topic assignment is what i find out what is the topic assignment for that word so what is one problem here i have to tell a priori how many topics that are there in the data but that may not be an easy parameter to always give right you may not know should i use ten topics twenty topics hundred topics in this data so can my model itself find out what will be the number of topics as such and that's where we come to this topic of nonparametric bayesian so this so this parameter goes away you do not have this parameter of number of topics and anymore ok so in many data analysis settings you will not know what is the number of topics a priori so in bayesian nonparametric models what we assume is that there is an so we are not fixing number of parameters but that also means ok my um data can have any number of param ah of parameters that means any number of topics so i have to a start with the assumption that there are infinite number of topics ok and my data can take any finite number of them so it can take hundred five hundred thirty whatever it means so in generally i have an infinite number of topics so that is so that's what is interesting about it it assumes that there are there is an infinite number of latent clusters or topics but only a finite number of them is used to generate the observed data but you do not give it as an input to your model how many such questions we need this is what model on it's own comes up when it sees the observations so the posterior provides the distribution over the number of clusters the assignment of data to clusters and the parameters of each cluster all these are different different parameters of hilarity model that can be learned from the bayesian nonparametric models so we will see so there are actually many ah different variations variations of this bayesian nonparametric models also known as hierarchical dirichlet processes because this is a ah very wide topic we will not cover this topic in detail in this course we will just give you some intuition that what how do you come up with this infinite topics right in your setting so and how does it model any finite number of topics in the data even if you are starting with infinite topics so we will talk about one particular setting that is a chinese restaurant process that can be used for modeling the ah or grouping your observation into some finite number of groups the number is not told a priori so what is this process is quite interesting so so the chinese restaurant process so so we are talking about the restaurant so we have saying that suppose this is a restaurant where there are infinite number of tables ok when you see the here here the term infinite immediately what comes to your mind ok these are the latent clusters so i have infinite tables in that restaurant and then there are customers that are coming up so we are like observations so we are fitting the observation to top to different ah clusters now the model says ok ah when the customers are coming in the restaurant how are the seated on the tables now one thing that it says is that the tables are of infinite capacity so they can they can hand each table can take as many customer as as you want so there is no limit on the number of customers that can sit on a table ok so now how do we assign customers to the tables in this process and that's where so it's a says ok so i have some tables infinite number of tables so customer one is comes in and sits at the first table ok table one fine so customer one is no problem now now the customer two comes in so idea is that customers who two now sits on table one with the probability one divided by one plus alpha and sits on a new table with a probability alpha divided by one plus alpha ok here c is add adds up to one ok fine so it can sit at either table one or table two now when a new customer comes in certain points of c n it can sit of suppose this table this table this table is occupied it can sit either at the occupied tables or add a new table so what they say it can sit on any of the occ occupied tables with a probability proportional to number of customers already there ok an on a new table with the probability proportional to alpha so that is so remember that's what we did here this is proportional to one this is proportional to alpha and then we normalize we get one divided by one plus alpha alpha divided by one plus alpha i have given any given a point of time you will have certain configuration different tables will have different number of customers so new customer comes in it can sit of any other table with a probability proportional to the number of customers already there or it can choose a new table and that's how you can fill in any number of customers on any number of tables so you will see at the end at the end of this process you can have any number of tables right so random process you might have five tables filled ten tables filled fifteen tables filled so you can have any number of clusters this is now just does not have to be defined a priori right we ah from your ah sampling you can choose any number of topics or clusters now one thing what will be the effect of the parameter alpha suppose your alpha is large versus alpha is a small if your alpha is large what will happen if alpha is large when a customer comes in it has a there is a large probability that will choose a new table ok so that means with large alpha you can get large number of topics so is it gives you some idea ok if you want more number of topics and each topic having small number of observations then you will choose a large alpha but if you choose a smaller alpha you probably by you will probably choose a small number of topics because ah if the customer will have a higher probability of sitting at any of the tables that are previously occupied so that will be the effect of these alpha so so coming back so first customer enters and sits at the first table second customer enters and sits at the first table with the probability of one by one plus alpha and the or the next unoccupied table with the probability alpha divided by one plus alpha and when the another customer arrives she sits at any of the occupied tables with a probability proportional to the number of previous customers already seated on the table and at the next unoccupied table to the probability proportional to alpha and that's how i can distribute all the customers in a restaurant and we saw that if i have a large value of alpha i will have more occupied tables and fewer customers per table right so we saw that now so so ok it's looks interesting if i have a document i can model so these tables as my topics and these words these customers as my words and i can say ok which words are send to what topics but how do i model my whole corpus you see i can model my one this can be thought of as my one document d one i know ok what are the topics and what are the customers but suppose i have a corpus right that so i i generally i wanted for my corpus so corpus means i will have multiple such documents now one thing would be i i fill it independently for each document but if i do that there is no correlation between the assignment here in d one and d two ok so it might happened d one i have first table some words are filling in here like government table two some words like a sports something ok and document d two the word is sports might start coming here and there will be no correlation between topics here in this document topics in this document ok so i cannot run this chinese restaurant process independently for different documents that way i will not be able to ah have a correlation ah between these so what is actually done so we saw that chinese restaurant process it helps in obtaining a random partition as the sequence of customers sitting at tables in a restaurant so we get a random partition of words into various topics now we also sit tables can be thought of as topics and customers as word in the restaurant or restaurant can be thought of as document however it cannot model the entire corpus for that we can extend c r p to a set of restaurants so that it can be extended to defining each document as a separate restaurant ok and this is called chinese restaurant franchise so now we are going from chinese restaurant process to chinese restaurant franchise so so this franchise has a lot of restaurant and that's why we will try to ah connect the different topics so let us see how do we connect the different topics so again so this is what is similar to c r p so customer in the jth restaurant sits at tables in the same manner as in c r p ok so customer comes in a in the restaurant any of the restaurant now it saw it sees which tables already occupied how many customers are there the probability proportional to the number of customers is sits at that table and the probability proportional to alpha it sits at the next unoccupied table so that remains the same as c r p now where is this ah connection among topics coming and that is coming in from the nice idea about the franchise that is let us say that they just franchise wide menu so now you have a new term of menu there is a certain dishes that are there in all the all the restaurants and that is franchise wide so whenever a customer comes at a given table it orders a dish from that menu now whoever comes in next we will have the same dish whoever customer comes in next we will share the same same dish now this dish is shared throughout the restaurants so now this dish at a table exercise the topic of that ah of the document so you can have so now the dishes are same across the across the different restaurants and that's why you can model it the same topics so so how do we achieve coupling among the restaurants so this is achieved by a franchise wide menu so so what we say that the first customer to sit at a table in a restaurant chooses a dish from the menu and all the customers was sitting sitting at that table will share the same menu and this is our chosen with a probability proportional to the number of tables which already have that that serve that dish so if this dishes serve at many different tables across the franchise then this will have a higher probability so that we are also giving you prior probability on choosing a bulk condition on a given table so it will look something like that so what you are seeing you are seeing three different restaurants in this franchise one two and three now this psi on one tells for the restaurant one what is the dish at table one so this is five one this is from the global menu of dishes five one five two and so on it is a global menu the second table has five two third table has five one that means more than one tables can order the same dish that is possible in that ah model so why because you see when a customer comes in it will first choose the table right so it can either to choose one of the table that is occupied on your a right table then when you choose the table it chooses one of the dishes so that means it can the same dishes dish can be chosen at more than one tables ok so that means this table also the same dish was chosen and so on next second restaurant first table choose dish five three second table five one third third table five three and so on ok so it has certain different are choices and and then the third restaurant first table choose five and second table choose five two and so on now you also seeing the customer assignments so in this first ah restaurant customers are coming in theta one one theta one ah two theta one three and so on theta one one sits a table one theta one two sits a table two theta one three at table one one four table one one eight at table one five six here seven here and so on so there are certain assignments that are given to these ah customers similarly here theta two one two two sit here and two three two four two six sit here now similar to l l d a what we will have from a ah when a customer chooses a ah dish it will see what the other customers have chosen ok that's where they will be coupling that the words will be assigned similar sort of topics across the different restaurants so so you are seeing each restaurant is modeled by a rectangle each customer are seated at the tables and each each table you are serving a dish from a glob global menu so this is a table indicator ok so by modeling this now you will know that what are the assignments of your different words to different topics in all these documents ok in interesting thing is that you do not did not have to tell how many topics do you need a priori what is the model can itself learn from the observation so so what we have seen we have seemed ok so you have an l d a model you can use it for very nice applications like ah finding out similarity between documents words but suppose you want to use some rich assumption like topics are collated changing over time so you can modify that model that's why it's it has a lot of ah then the model is very very strong it can use ah many different variations then you say ok ok i this is unsupervised model but suppose i want to model some responses with that that is what is the rating of this text or how many likes i can take it as an observation variable inside response variable and learn my topics according that it is sewage topic model i can go further and say ok which two documents are connected together that i can again measure the link as an observation ah so link can be as a response or observation link two pair of document documents this was very relational topic models and then we said ok suppose we do not want to use any parameters how many how many topics etcetera then you go to bayesian non parametrics and they are again there a lot of different models we talked about in very briefly about chinese restaurant process and chinese restaurant franchise but they are the other variations also that can be used and depending on your application we can choose one of the other models and they are tools available that will allow you to model any of this so that brings us to the end of this ninth week where we discussed a lot about topic models and also about semantics token semantics from the next thing we will start talking about different applications so we will talk about entity linking information extraction in the next week and there will go about other applications in the subsequent weeks so thank you and see you in the next week