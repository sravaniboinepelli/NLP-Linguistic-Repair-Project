 so hello everyone welcome back friend for this lecture five of week three so so in the last lecture we we started with the problem on part of speech tagging ok we defined the problem image ah given a text that can be a sentence ok so you need to find out what is the actual ah part of speech category for each of the individual word so there may be many various ambiguities but you need to resolve the ambiguities and find out the unique part of each tag for each of the words and we said that you can solve it using rule based methods or some probabilistic methods we also discuss that the the methods can be ah generative or discriminative and then they they differ in the in the philosophy of the model so in general generative model the class comes first and it is assumed that the words are generated the the data is generated from the class and in the discriminative model you directly find out the probability of the class given the data so we will in this lecture we will start with the ah generative model that is hidden markov model and see how it can be used for solving the pub task of part of speech tag so this a probabilistic model so so starting with what is my problem so i have some n words w one two w n in my corpus that i observed in and i need to find out the participation text for each of these words so suppose that you have to find out the sequence capital t so that assigns t one to t n for all of these n words now each of these t i's that a participant text can belong to ah the my my actual say suppose i am using the university of when we tag set so it can take any of those forty five value age so there are many many different values that these this sequence can take i need to find out the actual ah and ambiguous tag sequence given this ah word sequence as my input so now how do i start solving this problem as per the ah as per this ah probabilistic model so so so the idea is that among all the possible sequences of part of speech tags that this word sequence can take i need to find the one that has the maximum probability so i can write it like that so this is i have to find out t hat that gives you the maximum probability argmax overall possibility probability t given w so i need to find out the purpose sequence that gives the highest probability now so as we have already said t is nothing but t one to t n and w is nothing but w to w i so i can write it as argmax t one to t n given w one two w n ok this is what i will be doing so in generative model remember what is the idea idea is that the class or the the tags come first and then my words are generated from there so i cannot find out the probability of tag given the word but from the model i can find out the probability of word given the tag so i need to invert this the direction of the probabilities here so instead of finding t given w i need to find i need to use w going so what is the theorem popular theorem that i can use i can apply bayes theorem so instead of directly opening it here i can try same argmax t p w given t p t given divided by p w ok and p w it's common for all these sequences the probability of the sequence so this is again it does not matter so that will give me argmax over t p w given t and p t ok so this because an using generating model so first my class that is my t comes and then my sequence is generated w that's why i have to take it in this format probability w t p t now i can try and open this so this will be so let me just take this particular thing and this will be the probability w one to w n given t one to t n probability t one to t n ok so now so i can again use chain rule to write that ok so that we can see in the slide ok so so i can i can you write it as using the chain rule so this will be nothing but probability so multiplication over all i is equal to one to n w i given w one to w i minus one t one to t n and probability t i given t one to t i minus one ok this is simply by using the chain rule i can write it like that so now so it is very difficult to get the estimates for all these probabilities that we are thinking it so that means i need to do certain simplifications over this formula so what are the simplifications that we do so so one simplification that we do here is that so in this formula we are saying that the probability of the word the current word so you see we have a sequence of words w one to w n and correspondingly we have a sequence of x t one to t n whatever w i what i am saying the probability of this word depends on all the previous words and all the tags so instead of that because the generative model i might say that this probably depends only on the current deck ok this is simplification that i can make so this i can simplify as probability w i given t i ok so this is one simplification then here we are saying that the probability of t i the tag t i depends on all the previous tags so again i might simplify it by using some markov assumption that it depends on either the only the previous tag or previous to previous tag so if i take only the bigram assumption so i will set t i depends on t i minus one so in terms of this model i will say that t i only depends on t i minus one ok and here i will simplify it using probability t i k one t i minus one and this is my bigram assumption ok so now if i if i make the simplification what is the model that we actually see so this is the formula that we came up with so i want a a tag sequence that has that gives the highest probability for this particular form and we make some simplification that is the probability of a word appearing depends only on it's part of speech tag remember this is genetic model so the word is generated from the part of speech tag so first the part of speech tags are generated and then the tags are giving each an individual word so we are making this assumption that the word is generated only from it's own part of speech tag ok then so this gives me the first ah simplification second one i will say that the probability of attack depends only on it's forward previous tags so if i make the bigram assumption it will depend only on the previous tag ok so this will give me this function so together the same the simplification will give me this formula so i want to find it tag sequence that gives me the maximum probability for this particular formula ok so now once we have come to this formula so what is this model actually ok so first let us see that it can be easily compute these probabilities probability w i given t i and probability t i given t i minus one so how will you actually compute these probabilities so one way is that you are given a corpus where you know all the words you also know what are their part of speech tags somebody has manually annotated each this this data file now given this data can you compete this probabilities so for example computing probability t i given t i minus one if you want to find out how many times this tag t i comes after the tag t i minus one so you will compute it by using the maximum likelihood estimate that is the number of times t i minus one and t i come together in the corpus divided by the number of times the word t i minus one the tag t i minus one comes ok so if you see here in the slide so p t i given t i minus one can be found by this by using these counts count of the two texts together divided by the count of the previous tag only so if i want to compute probability n n given d t i will say number of times d t occurs followed by c n n n divided by number of times d t occurs and this if i have the numbers i can compute this probability what is the other probability of the compute i have to compute probability of word given the tag ok so so the complete probability where i given the tag i will find out again how many times this word occurs with this tag divided by how many rimes that tag occurs so this again from my corpus so if i have the numbers i want to compute probability is given v w z i will find out how many times the word each occurs with the direct v w z divided by the number of times the tag v w z actually occurs in my corpus so here if i have the numbers i can compute these probabilities so all the probabilities that are required for this model can easily be computed if i have a data where i know the words and the part of speech tags for each and individual words so now let us see how we can use that for some disambiguation it's not the complete model just to give you some idea so once we have this how we can use that for some disambiguation so i have a sentence a part of a sentence here secretariat is expected to race tomorrow ok and the ambiguity here is in the word race so whether the other word races in noun n n or v b you see everything as the same here now what are the probabilities as from my model that differ in the two interpretations so if you see the first interpretation what is the second interpretation what are the probabilities that are defined in the first one you find if the probability of the tag v b given t o versus in the second you find probability of n n given t o then in the first when you find probability of n r given v b and second you find probability of n r given n n in the first when you find probability of race given v b and second you find probability of race given n n and because in your model you are multiplying all the probabilities that means you will multiply all the prob all these three probabilities in the integration one and all three in the interpretation two so whichever multiplication gives you the highest value will decide what is the actual ah part of speech tag of race that should be used here it should be v b or n n so if you have the corpus you know all the probabilities you will multiply you will find the number and this will tell me whether i should prefer interpretation one or interpretation two ok suppose i take some numbers so here so difference is because of these probabilities suppose from my corpus i find some numbers so i will i will see here is that ah the the um the possibility here is that the word race should be a verb not as a noun and if you see the sentence to race tomorrow the race should be race the word race should be used as a verb not as a noun although ah noun is a more common category count part of speech tag for race then verb but because if the context it is more likely to have race edge verb in this case ok so that is how we can disambiguate in one part in this simplistic case but in general for the whole sentence even if you do not know any of the tags we can try to use this model to find out what is the actual sequence of tag that should be used so now so coming back to this question what is this model ok so what are you seeing here so you are seen so you have a sentence so that is the words that's what you are observing and then there are certain tags that assigned to each of the individual words so in this model you have a probability of of going from one tag to another tag and then from a tag you get a word so can you find can you think of what is the model it corresponds to what is the actual model so if you have come across hidden markov model where you have the states and from one state you transit to another state and so on and in each state that is hidden you can emit the observation ok so the word here is observation so this is nothing but the hidden markov model so what are hidden markov models so just to give the idea in brief so here you have the tag transition probabilities t i given t i minus one you have the emission probabilities so that is word observation probabilities probability w i given t i and so this is so using this whatever we are describing is a hidden markov model ok now to to tell you what is hidden markvo model so let me just quickly tell you what is a markov model and how a should markvo model different from a markov model ok so what is a markov model ok so this markov model is best explained using this simple example so in markov model what happens you again have a states but the states are also your observations suppose you are studies the weather the weather on the current day and the weather can be sunny rainy or foggy these are the three different kinds of weathers that can happen on a given day now what what you are you will get you will get you will have these three states you will know given that today is sunny what is the probability that tomorrow will be sunny or foggy or rainy so this is state transition probabilities you will obtain ok so so suppose q n is a variable that denotes the variable on the n th day and using this model we can find out the probability of q n the weather on the n th day given all the previous days back and here if you use the first row mark markov model so so we say this will depend only on the previous days back q n will depend on you n q n minus one so let's take one simple example so here so the you are seeing the state transition so so that says that if today's weather is ah sunny then tomorrow will be sunny with the probability of point eight and if today is sunny tomorrow will be foggy with a probability point one five so you can see that from the edges that go from one state to another state also from the table that is shown in in this so now once you are given this probabilities you can do certain you can do certain computations for example suppose you have to find out given that today the weather is sunny what is the probability that tomorrow is sunny and day after is rainy ok so let's use the variable and i want to find out probability q n plus two day after is rainy q n plus two is to rainy and tomorrow is sunny q n plus one it's sunny given q n sunny so how would you compute that so if you simply is the channel you will say that is nothing but probability q n plus one is equal to sunny given q n is equal to sunny times probability q n plus two is equal to rainy given q n is equal to sunny q n plus one is equal to sunny ok and now because you are using a first row markov assumption so this will be equivalent to so this you will compute using probability q n plus two is equal to rainy given q n plus one is equal to sunny now so you have to compute this probability and this probability and that you can obtain from the state transition graph so if you go to the previous slide you will find out probability sunny given sunny is point eight m probability rainy given sunny is zero point zero five ok so once you multiply this you will get the answer is point zero four so here so you will get the answer is zero point zero four so that gives you the probability that tomorrow will be rainy and sorry tomorrow will be sunny day after tomorrow will be rainy given that today is sunny so that is how you use the markov model now this is the markvo model so what you see here you have the states so in this case the weather so on a days is a solidus state and transitions are happening among the states but what is your observation that is also state so you also observing the weather that is shear state you are observing that now remember the example of participate text what are we observing we observing the words then when i give you a text i you only observe the words but what is hidden that is the text the text are hidden so that's where the hidden markov models are different from markov model the states there are not observed they are hidden variables and what is observed is different so you have the words are being objective and from the state you can emit the words so this is the idea so the generative model would be so you are starting from some state state one you keep on transiting to other states as three and so on and v is a state you also emit a word emission one emission two and so on and these emissions are nothing but your observations these are your observations so you need to find out the underline sequence of a states given the sequence of observations so now so yeah so this is the difference we have seen for markov mode chains the output symbols are the same as the states so the word sunny is ah both a state and the observable so what happens in part of speech tagging words are the output symbols but the hidden states are part of speech tag ok so so hidden markov model is nothing but an extension of markov of chain so in which what happens that ah the output symbols that you are having are not the same as the states so states are different from the output and we actually do not know what should we are in until we try to use our model so what are the elements of an hidden markov model so so what do you need from hidden markov model so you need a set of states yes you need the probability of transiting from one state to another state in your the probability of emission given a state which words will be emitted with what probability ok and you might also need what is the beginning of a state and ah and so on so if a if you try to correspond a hidden markov model with the with part of speech is tagging so we need set of a states in our part of speech taking case so the tags are the states we need an output alphabet that is then what are emissions so the words are the emissions with the initial state so in our case that is the beginning of the sentence we need the transition probabilities that is given a previous tag what will be the next tag and we need the emission probabilities that is given this tag what will be the word so now so once we have this we can also give a graphical representation to our ah hidden markov model for part of speech tagging so what is happening see here we start state so here by start steady shown from the start state you have a probability of transiting to any of the other tags so what does that mean you can start the sentence with some particular tags so this probably should depend on what is the tag that is more likely to start the sentence and once you have this tag what is the next likely tag and so on so this is your state transition graph so this is what is shown in the slide so when we are tagging the sentence you are actually walking on this a state graph from one state we are going to another state and so on and they are very transition probabilities that you can have over this state graph and this you can model by using probability of a state t n given t n minus one now what does is missing here with each now within the hidden markov model with each a state you also have the word emission so from history you also want to know what is the probability that you will output or emit a particular word ok so this is what is additionally at ah ah needed here so in the graphical representation now with these states like to i should also the probability of different words emitting from that state so here all the words is in the vocabulary are are written and given the the particular state what is the probability of ah out putting or emitting that particular word in the vocabulary so that you need to define for all the very possible ah states in your graph so now once we have both these what is my problem so so i can define so if you remember i can define all these probabilities once you give me a corpus that contains a set of words and ah the their corresponding part of speech tags if you give me that i can find out all these probabilities so i can define these graph once here once i have a corpus what is my problem at runtime i am only given a an observation that is i am only given a sentence that is sequence of words i have to find out what is the corresponding part of speech tag sequence that should be ah that should be used for this word sequence ok so that is suppose i am given a ah sentence like this is the part of the sentence promised to back the bill ok there are five words here so my problem is to find out what is the sequence of x that would be used for this sentence so how would i approach this problem given the state transition graph that i already have now for each word so so one thing to simplify this i will do is that for each for i will i will find out what are all the possible part of speech text a word can take ok so we will be talked about this said that some words can take multiple part of speech tags ok but but none of the words went beyond seven part of speech tags ok and mostly the words were having if they were ambiguous they were having two or three part of space tags so for a word if i can identify what are the part of speech tags so i can only use that to have my set of possibilities so here for example i will say the word promised can take only v b d or v b n past tense of participial it can be only one of those i have to dis dis disambiguate among the two the word to can have only one part of speech tag back can have four tags the can have two tags and bill can have two tags ok these are all the possibilities now each four can take any of these possibilities so now can you just quickly see how many possibilities are there so if you see here i have two times four eight times two sixteen times two thirty two possibilities of the part of speech tag sequences now i have to find out among these which is the most likely sequence of participation tags that is being used so now so so how will i solve this problem one neither might be i enumerate all the possibilities and for each possibility i compute the probability separately so i have thirty two possibilities and i compute thirty two different ah so for all the possibility i compute the full probability that is one one possibility but firstly this is not very efficient and think of some sentences which might have ah say fifteen twenty volts this this will just go exponentially with the number of words ok so this is not a good solution so i need to have a solution where ah it does not grow exponentially with the size of the sentence ok so so what will be a good algorithm for for doing that so ideally i want to come up with the actual part of speech tag sequence like here it will be v b d t o v b d t n n from all the possibilities and how will i do that and that's what we will be discussing in the next lecture that so this is the viterbi algorithm how do i apply in my hmm model this will be algorithm to come up with this part of speech tags sequence in efficient manner ok instead of doing something naive implementation that is ah exponential and that is actually not feasible for for doing it at for for a large corpus so that will be the focus in the next lecture thank you