 so welcome to the final lecture of this week so in the last lecture we had talked about what is the approach of using maximum spanning tree for dependency parsing so there we only covered that if we have a sentence and we have some way of constructing the original graph multi diagram where there all possible connection we can usually algorithm to find out the m s t ok in thats what we covered now important point is how do we find out the aspects ok and thats what we will be covering here so how do we treated as a learning problem where we given some data where we know these are sentences and they are corresponding dependency graph how do we use that to establish the aspects for a new sentence and you understand now that once you can do that part rest part is already defined by the algorithm so your main task is how do you effectively construct the aspects now now just one thing what do i mean by the aspect aspect is something that will depend on the two nodes that you are connecting and what is the relation with which we are connecting them so this has to be some sort of a feature representation vector over the nodes and the label and you have to define what are your features and once you define for any possible connection you can find out this vector find this vector is a separate task it does not require any learning the way we will pick a learning involved each features will also have a weight assigned that will help you to determine the aspects so feature vectors when you multiple the weight you will get the aspects and this weight vector for the feature is something you can learn from your data so that is what weight vector with this feature set help me to choose the optimal or the best a spanning tree that corresponds to dependency graph that i already have in my data if i have some weights that are not allowing me to choose the best graph i should a bit my weight and this is the idea that we will see in this lecture so we will talking about the learning part here so the first thing that we should see here is the arc weights that we are defining can be written as some feature vector over i j k yes the node i the node j and the relation k and a weight vector and i multiply by weighted feature i get the arc weight now we have to see how to define this feature weight features this is again analogues somehow to what we did in some of the previous ah classes and how do we learn this weight vector so here arc weights are nothing but a linear combinations of features of the arc and a corresponding weight vector now what we are different arc features you can take ok so it will dependent on again what are the words i am connecting so let us take a sentence like john saw mary mcguire yesterday with his telescope and i want to find out the head weights between first saw and with with the relation pp ok so this is my i this is my j and this is the relation pp so here by the multi diagram arc means for k you can have many variations you can have pp you can have n subject direct object indirect object and non modifier etcetera we have the various possible variations so what what would this feature dependent on so feature would dependent on probably what are my words themselves they are some of the most important features if my word is saw and my word first word is saw second word is with and the relation is whatever pp here ok so head is saw and dependent is with that is taken in my one primarily feature so every case i will have this feature i will have an answer one or zero then i can have a feature composed from the part of speech tags of these words so i also know what are part of speech tags and so the feature can be like what is the part of speech for the head part so head part of speech is verb and what is the part of speech of dependent word that is preposition and that would be one of the most important features that i can use because many a times the relations are defined between various grammatical categories like for subject it will be between verb and a noun so if you know that the first the head word is verb and the dependent is noun there is a good chance they may have a subject relation although there are other relation possible so part of speech is one very important feature then what else you can also use the part of speech of the words that are coming around this head and dependent word in between like in between part of speech is noun that means there is a noun that occurs in between these two words yes in between part of speech is adverb there is adverb that is occurring dependent pos right is pronoun for the dependent the right hand side the part of speech is pronoun and head pos left is noun to the head to the left of the head the part of speech is noun so like that you can have again have many different features on the neighboring words and the words in between so you can use the identity of the words or the part of speech and then you might also want to use the orientation whether the head is towards light left or towards the right so that can be captured by arc direction this is to the right so arc direction is right you can also see the distance how many words are intervening between the head and dependent so in this case there are three words that are intervening ok so like that you can you can take various different ah question or various different characteristic of head and dependent surrounding words and the labels and construct all your features so this gives you the set of arc features f i j k and now given any possible ah arc in your graph you can construct this feature vector yes it will be answer to all these questions is the arc distance three or not so answer will be one or zero so like that you can construct this feature vector but what is your aspect that is the feature vector multiplied by the weight vector so next thing is how do we use this weight vector or how do we obtain this weight vector so as such you can use any features over the arc i j k and input x so now how do you obtain my weights for that i have to define what is my ah inference problem so this we have already seen for a graph the weight of a graph will be defined by summation over the weights of all its edges and the m s t problem is find out the graph with the maximum weight ok among all the possible spanning trees that i can obtain from the starting graph take the one that is having the maximum weight now this w i j k can be written as w times the feature vector f i j k and w can go outside of this equation so this will be in other words w times summation over all the feature vectors and take the arg max over that and suppose i call this as the feature of my graph the feature vector for my graph is nothing but the summation over feature vectors of all the edges and this is my problem arg max w times f g over all the possible graphs or directed spanning tree g that i can construct from my initial graph now how do use this inference problem to learn my bits so this will be my learning criteria and this will be very similar to what we did in the previous ah example that the previous method also so what is the idea you have a gold standard training data ok so you are given a training data of a sentence and its gold standard dependency graph so there are some capital t sentence that is now start with some initial weights they can be zero or they can be some arbitrary weights ok and lets start with the ah the first iteration now i am iterating over all the sentences some capital n number of times like we did in the previous case in each iteration i go to each of the individual tree so one to up to capital t each sentence instance now what do i do so i am i am in the learning part i do not know what are my optimal weights so how does learning work so i am given a sentence x t and its dependency graph g t i am already given this i know what are my features yes i know what are my features i do not know what are my weights so how do i proceed so lets take a particular instance for my learning so what will happen i am given this sentence x t so i will apply my algorithm as if i am doing it at run time so i will make the root node i will make all the possible connections somehow ok we can have any possible connections thats not matter so now you start by making all the possible connections that we discussed this is my initial graph you can that graph g x that has all the possible connections ok this also all the possible connections now at this point how will you proceed for proceeding you need to know what are the edge weights here so now how do you obtain the edge weights so this edge it will be nothing but w times f i j k i is the ith word here j is the jth word here and k is the relation now how do you obtain f i j k this already given to you so you know what are the features so features will be some question like ith words is root and jth word is something like saw it will be either one or zero yes so your feature vector will be some zero some ones and this might be different for different edges now what else you do not know your weight but you will have initial weights or some intermediate weights you will use those weights multiply by the feature vector and you will get some number and that is your w i j k yes and you will put back this number it can be say ten or thirty whatever suppose you get ten thirty twenty fifteen and so on you will filling all the values using the same method now once you have filled in all these values what will be the next step so next step should be apply chu liu edmonds algorithm yes and obtained a dependency graph and in other words obtained a mst ok so suppose you obtain a graph like root here and so there are three words so something like this suppose lets take a simple case and like this this a one of the dependent one way of writing dependency graph ok and suppose this is what you obtain from your algorithm now where is the learning here learning is if the dependency graph that you obtain is if this is not the same as your gold standard dependency graph that means your weights are not correct at this point you might have to modify your weights and how do you modify your weights very simple like you did in the previous method also you would say w new or w old plus you want to go towards the actual graphs and away from the so call it g prime that you obtain from by your algorithm so call it g prime that you obtain by your algorithm g t is the gold standard graph so you want to go in this direction and away from here so it will be simply feature of g t minus feature of g prime ok and what is f g t this is nothing but the summation over features of all the edges of g t similarly here summation over features of all the edges of my g prime and that will give you your new weight vector of course there can be some learning weights and all but this is just to give you the intuition and thats all you will have a new weight again you will start with this new weight for a new sentence see whether you obtaining the g t like show graph by this algorithm if not you will keep on updating your weights ok and this will sometime at some point converge and some of one of your iteration you will have your weights roughly stabilized and thats where you stop so now lets so hope this idea is clear lets take a simple example to further see that how do you apply this algorithm for learning the weights so let me just quickly go through this algorithm again so i am repeating it for some n number of iterations for each tree let us find out g prime as the argmax over weight at the particular instance and f g prime ok so that is i apply my ah chu liu edmonds algorithm find out the tree with the maximum weight if g prime is not the same as my gold standard graph that means i have to update my parameters and how do i do that at this i plus one th time my weights are initial weights plus f g t minus f g prime this i am doing only if i am not obtaining the correct graph i continue further and at some point it will converge and thats where i will return the weights and i will assume at this point i have the ideal weights so that whenever you give me a new sentence i can use these weights to compute all the arc arc weights yes that arc weights are nothing but weights multiplied by the feature vector and then i can i can apply chu liu edmonds algorithm to compute the dependency graph so let us see some example for learning the weight vectors so we are using the same ah sentence like the previous ah case so we are using the sentence john saw mary ok and suppose that this occurs in the training set now for simplicity we are not worrying about what are the relations between two words i am saying that there is only one relation rare so my features will only dependent on the ith word and jth word that will make things easier so here what i am showing let us take some seven different features and some weights that have been initialized or some intermediate weights and suppose you are you are un counting this sentence in your training data how will you do the weight update so here let us see what are my seven features defined over any word i and j for a arc from w i to w j so features are the ith word has a part of speech of noun and jth word has a part of speech of noun ith word is part of speech of verb jth word is part of speech of noun and so on ith word is root jth word is verb and for this simple sentence you already know right so you will have [niose] root john ok let me write it like that saw mary you know this is a noun this is a verb and this is a noun and this is root so now your task is suppose you are given for these seven features the weights are three twenty fifteen twelve one ten twenty before the start of the iteration now you have to determine the weights after iteration over this example now what do i mean by doing that same as i said in the in the last slide so what i will do now i will first find out all the possible edge weights ok lets try to find some edge weights what is the edge weight from root to john ok so what is this edge weight this is weight vector multiplied by the feature vector between this root and john this might w i this might w j now what is this f root to john this will be a vector of size seven each value to be one and zero and this will dependent on the answer of my question so let us see what are my features word w i has a part of speech of noun now w i is a root so it does not have any part of speech this will be zero w i has part of speech of verb again zero w i is root ok lets see the next one w j is verb so here w j is john that is noun so this is also zero root and j is noun this will be one root and w j occurs at the end of the sentence know john occurs in the john occurs in the start of the sentence this is also zero w i occurs before w j in the sentence roots does not occurs in the sentence so this will be zero also w i is the part of speech of noun so it is not true for root so we see out of seven features only fourth one is one everything has a zero so the feature vector here is zero zero zero one zero zero zero now how do i get the weight the weight vector is already given to me that is three twenty fifteen twelve one ten twenty so i multiple this weight vector to the feature vector and i obtain the weight of this arc that is twelve similarly i compute the weight of the arc of from root to saw so let us take some other case like john to mary how do i compute the arc weight again this is lets find out the feature for john to mary ok so first one first question again w i is the part of speech of noun w j is noun now john and mary both are nouns so this should be one w i is verb and w j is noun should be zero w i root none of the so none of these features will be one because all the required to be root so the all three are zero so two three four five are zero six w i occurs before w j so john occurs before mary in the sentence there should be one seventh feature w i had part of speech of noun w j is verb that is zero so only one and six are one everything else are zero so it will be one zero zero zero zero one zero and multiplied with the weight vector so similarly first element and sixth element three plus ten so thirteen so here the weight will be thirteen like that you have to find the feature vector for each edge multiplied by the weight vector and obtain these values you will said get some values here x y z etcetera now once you obtain all these values what you will do apply chu liu edmonds so once you have all the weights you can very easily obtain chu liu edmonds and you will obtain what is your dependency graph now what will happen suppose you find a graph like this root john saw mary suppose this is your output after applying chu liu edmonds now what you will do so you will this is your g prime by applying chu liu edmonds so your first thing you will check is that what is your g t so your g t is root remember you will first have connection from root to saw saw to john and mary this is my g t are they same they are not the same so i will update my weights and how do i update my weights should be w new is w old plus f g t minus f g prime now what is f g t f g t is the feature of this graph that is this feature vector plus this feature vector plus this feature vector three feature vectors combined and f g prime is this feature vector plus this feature vector plus this feature vector and these are all some ones and zeros so weights will be nothing but these weights plus summation over all these three elements of features minus all these three elements ok and that will give you the new set of weights and then you will continue with this new set of weights for the next example if you have to do so so this is in a nutshell what we will be doing but again i will encourage you to go through these example fully find out all the weight vectors yourself see whether you are getting the same tree as the gold standard if not how will you update your weights ok in this will be your next exercise so here we are not going to very much detail in in how we are going to learn it by various learning rates and all just an intuition that how do you post this as an learning problem ok so in the in the last module of using parsing or transition parsing and this module of using ah mst based parsing we have seen that how we can solve a problem dependency parsing in a detailed different manner so we treated as a some sort of algorithmic way i was starting from some initial configuration going to some final configuration that resembles a dependency graph and in the in between for example how are we taking transitions we will determining by using various training data or how are we taking the edge weights we are learning by training data and this would help you to also think about in terms of some other problems that how can i pose them in machine learning way and i was also saying there are some other they are some other or i would say even many other methods of for doing dependency parsing so we will not be covering everything in this course but now whatever we have covered will help you to take any new approach and understand that ok so this also ends our discussion on main discussion on syntax so we had talked about starting from word order information language models to part of speech and morphological information to various syntax in in the sense of constituency parsing and dependency parsing so from the next week we will start our discussions on semantics so we will first see what is called different methods by which being capture the meaning the semantics so we will have distribution semantics and semantics for that so thank you i will see you in the next week