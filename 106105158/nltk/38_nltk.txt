yes welcome back to the second lecture of this weak so in the last lecture we started with lexical semantics and we defined the relation between lexical entities and we talked about ah polysemy hyponym hyponymy meronymy and and so on and and in this lecture we'll see resource wordnet and how it captures all these relations so wordnet you can you can get a lot of information of wordnet on this website so its an its an effort started at princeton and there are lot of versions that that have come up and you can you can find out the latest version of wordnet and also download that and you can use that also in once you download you can use that in your command term [ inal ] - terminal also so what is wordnet wordnet as such is a hierarchically organized lexical database ok and its completely machine readable so you can you can use that in in different applications you can call wordnet get get the recent information from there so as such on this website you will find the wordnet for english but there are other so there are many many other versions for other languages also also built so there are lot of wordnets available for european languages and a lot of effort has happened in the last decade for building wordnets for indian languages and that you can also ah download many any many of those versions are also available for download for free so for that you can look at indo wordnet ah website so ah so there you will also find out what sort of synsets or concepts in one language are relate to some other concepts in other language so this information would be available in this ah in this indo wordnet and and euro wordnet websites so now div now we will focus only on the the english wordnet part in this in this ah lecture but the methods would be easily applicable to other wor [ d ] - wordnets that are having shared by single structure so if we talk about english wordnet so there are mai [ nly ] - mainly four different part of speech words that are ah and there and what we are seeing here how many different synsets are there for each part of speech so there are roughly eighteen thousand ss synsets for nouns thirteen thousand plus synsets for verb and eighteen thoudsand plus synsets for adjective and three thousand plus for adverbs now so so one important thing is that we when we talk about wordnet we talk in terms of synset now what is the idea of a synset in wordnet so in synset what will happen so there is different words that have similar meanings will be stored but a single word might have multiple meanings also so how are how are both of these things taken care together so so lets take an example so ah so example is particular synset corresponding to chump that is a noun and that is a person who is a believable and easy to take advantage of and suppose chump the first synset this programme meaning so word has have might have multiple synsets so chump first synsets is this meaning but there might be other words also that have the sense and so this word its nine sense denotes this particular meaning so i have to say that the word mark its nineth sense is this meaning the word fool its second sense is this meaning the word goal the first sense is this meaning and similarly fall guy and so on so what i will do here each word might be represented by multiple synsets so a word might have sense one sense two sense three and so on and what i will do if suppose the third sense of word one and second sense of word two shared the same meaning i will put them in a single synset and that is the idea of synset different words that share a same meaning and by words here i will mean the lexim ok or a particular sense and for wordnet this list might denote what is the meaning of the word chump or you will also find some gross information of the wordnet so now this also this figures some so [ rt ] - some sort of explains what is the relation between the word form or the lemma and the synsets so what you are seeing here so there is many to many relation so that is for example the word note it can go to note and to end bullenthri so denotes two different synsets in one sense it goes to note synsets another sense it goes to bill synset but this bill synset will contain note and also contains bill and some other words ok so what you are seeing here many different lemmas that share the same sense can come together in a single synset and the same semma same lemma for different meanings can go to different synsets and that is captured by giving some unique identifies to different two different synsets of the same lemma so i will start talking about now lime one lime two lime three instead of just saying lime ok and each of these three will be ah in different synsets so what are all the possible relations that ah that you see in wordnet here is some example so in wordnet you have relations like antonyms hypernyms hyponyms entavment relations synonyms and so on ok many of these we have already seen in the previous lecture lets see some example types of these relations so like what are the relations between ah various ve [ rb ] - nouns so the relation hyponym that we discussed so hyponym is relation between a concept and its super concept ok so like breakfast is a kind of meal so i was a meal is a hyponym of breakfast so this relation will be there in the wordnet where the breakfast one synset or the corresponding sense its connected to meal one by the relation of breakfast is a hyponym of meal and meal is a hyponym of breakfast similarly the converse relation of hyponym meal and lunch member so faculties in so professor is a member of faculty harinstins is composer in instins s bash austin is a instins of author member maronym like capital is a member of crew part maronym from whole two parts so table to leg part holonym the other way round from parts to whole coarse to meal yes and antonym that is a opposite diden follower so what would happen all these relations are defined in wordnet you know what what the relation means and then you can find out for a word collision what are the different pairs or given pair in the wordnet is there any realtion ah that is defined similarly there are relations between verb elements also like hyponym fly and travel so fly is a kind of travel so travel would be the ah hyponym for fly tropoynym from a verb to a specific manner for that verb like walk and stool so stool is a typical manner of or a particular manner of walking they can be eh entanment relation also from verbs to the words that they end till like snoring and sleeping so i'll say snore and tell sleeping so this is also relation that is captured in wordnet and then there can be antonyms for the opposites so increase and decrease they are antonyms for each other further in wordnet we can also capture ah what is the complete hierarchy of a given concept and what do you mean by hierarchy that is starting from the root word and wordnet how do you go down to that particular word ok what are the different concepts that you need to pass through and that you can find for all the different synsets for the for a given word and so thats why we said that ah wordnet is very very hierarchically organized database so you can find out or you can locate each concept or a subset ah each synset in that complete hierarchical tree so for example if i see the word mouse so i'll find it has four synsets so for sense one mouse you can find out the complete hierarchy so mouse is a kind of rodent kind of placental kind of mammal vertebrate and so on up to animal living thing object physical entity and entity ok you can go to the root of the wordnet entity starting from this particular sense of mouse here is another sense of mouse that is a computer mouse again you can keep on going up the hierarchy electronic device device artifact object physical object physical entity entity and you can see where do they depart so mouse comes under whole then an artifact and this mouse comes under living thing thats where they depart in that ah in that wordnet hierarchical tree and this we can do for any any sense any word word in the wordnet you can find out the complete hierarchy now so so as such wordnet will always store which two concepts are related by wap relations and ah suppose car an automobile occur in the same synset if you query in a wordnet there is a very easy way you can query in wordnet ah and you can find out if they are part of the same synsets then you can say ok they are similar but suppose two words are not part of any sense any particular synset in wordnet and i want to give it a number like to what degree are they similar similar to what we ga [ ve ] - did in the case of distribution semantics we found out how much two words are similar to each other by seeing how much their patterns are similar so can i do sim something in the ca [ se ] - case of wordnet that is given two concepts or two words how similar they are even if they are not part of the same synsets so so by using synonymy i can only see that the two words are synonymous or not but suppose you want to do the matrix for word similarity or word distance so what are the different things in wordnet i can use so in wordnet there are many other relations also defined so i can also use the fact that whether there is any other kind of relation between these two words or do the words do these words share a common hypernym hyponym and so on also you might have a have a look at the gloss of these two ah entit entries whether their glosses are very similar so so i i'll say two words are similar if they share many different features of meaning and if features can be captured in terms of ah how many relations are common how many words are common in their gloss and so on now one thing that then that we must keep in mind when we are talking about establishing relation between different en [ tities ] - entities in in wordnet we are talking about relation between different synsets and not the words ok because a word might have mutiple synsets and the relation that is there in in one sense of word one ad one sense of another another word may not be there in the ah other other synsets so so for example here so ah for example is a word bank this is sense one this is in the sense of you can say in the economics and sense two is like river bank ok and then they have i have another word like fund ok and there is s one that is for economy so as you see that this sense one of bank is connected to this particular sense of fund but i cannot say that this is connected to fund this is not true so i cannot say that bank is connected to fund saying that will not be correct what i will say sense one of bank is connected to sense one of fund so we'll talk about relations between synset of words and not word directly all though we'll try to extend it later to the words ok so instead of saying bank is like fund i will say something like bank one is similar to fund three suppose that is the third sense of fund and bank two is similar to slope five the fifth sense of slope and so on now we will also compute similarity over words and synsets both so let us see how do we do that by using the wordnet hierarchy and any other information that we have so now this is something that that we have talked about earlier that if i want to find out similarity between words there are two two ver [ ry ] - very popular methods one is by using distribution algorithms and that we discussed in in detail in the last week there i can find out the distribution patterns of two words and compare those but if you wa [ nt ] - if i want to use a lexar resource like wordnet so here can i use the idea that some words are more near in the wordnet hierarchy than others so if two words are near in the hierarchy i might say they are similar if they are very far apart in the hierarchy they might be different so can i use this idea to establish if two words are similar by using the ah wordnet resource and we'll we'll so we'll now see lot of such methods of doing that so so as such i can use any relations like maronym hyponym troponym glosses examples yes but in particular with this sort of based methods that that we have they mainly use the e j hierarchy tree the hyponym hyponym relation tree sometimes we will also use the glosses of the of the words so we'll start by seeing some examples or some particular ah ah methods that ca [ n ] - that try to use the the hierarchy wordnet hierarchy to capture the similarity between between the words and then we'll also see a particular method that uses the glosses of the of the different synsets to capture their similarity now so one thing that that you might ah have seen or have understood by know that by using all these methods we are not capturing the synonymy as synonymy as such we are not seeing that that we are finding two words that are very very similar what you finding is that two words that are related or are used in similar sort of context and topic so like if i take car and bicycle they are quite similar but car and gasoline they might be related but not similar so by these methods car and gasoline might come come closer but all that means is that they are related they may not be exacty similar so now now coming to the methods to capturing similarity across words so what is the first idea first idea is to use the path between two words in the hypernymn graph and what can be a simple measure i will say that two words are similar if the path that connects the two words in the in that hierarchy is small ok so two words are similar if they are nearby in the hypernymn graph and to give a formal measure or quantity to that i can define the path length between two concepts so path length between two concepts what will be that that is the number of h h in the shortest path in my graph between synsets c one and c two so what is the length of the shortest path that connects c one and c two in my whole graph now once have have found this path length how do i use that to compare the similarity between these two concepts so that is path length is large they are less similar if path length is small they are ah very very similar so my similarity should be inversely proportion to the path length so one measure can be one divided by one plus path length and this is one simple measure that is in which the path similarity between two concept is one divided by one plus path length of c one and c two so thats how we can find out the relation between two synsets so now suppose i want to extend that to find the similarity between two words so one way is to i find out similarity between all the synsets and take an average but a more ah commonly extended measure is find out similarity between all the possible ah payers of synsets between the two words and take the maximum ok so what you mean by that suppose i have word one word two that is sense s one one s one two s one three and this a synsets s two one s two two ok so by using this measure i can find similarity between any two pair s one one s two two what is the similarity similarly i can do for all these pairs now how do i establish similarity between w one and w two so that is similarity is the maximum value of similarity between s one i and s two j you take any sense for the the firs [ t ] first word any sense for the second word whatever the maximum similarity between between a pair this gets me the simi [ larity ] - maximum similarity or the similarity between these two words that is a very simple way in which i can extend all these ideas to word similarity so in whatever we will see ah in the ne [ xt ] - for the next methods we will also alway [ s ] - all always talk about similarity between synsets and extended for similarity between words so i will say similarity between words w and w two is nothing but the maximum similarity between any of the synset of the word one and word two so lets take an example that how what will this ah path way similarity look like so this is my wordnet ah hyponym graph so not all the notes are shown only if if very few notes are shown so we are starting with entity abstraction measure standard although with entity there will other concepts here then you are you are coming to a particular branch with where you have medium of exchange currency coinage coin nickel and so on now i want to find out similarity across two different concepts so what is similarity between nickel and coin so i will say what is the path length path that connects nickel and coin what is the length of this path so here the length is only one so similarity between these two concepts will be one divided by one plus one to point five and what is the similarity between nickel and dayne it will be one divided by one plus path length and path length is two so it will be point three three and if you see nickel in a very different concepts like richard scale so here you will find similarities point one to five because the path length is seven so like that i can capture the similarity between two concepts by using the path length now there is ah another similarity measure along the same lines so this is called l c similarity so what it says the similarity between two concepts is minus log of path length divided by two d so this is just a sim different function over path length so earlier we had a function one divided by one plus x now they have function minus log x divided by two d ok and what is d here d is the maximum depth in my my hierarchy ok wha [ t ] - so starting from the root note what is the maximum depth of a ah hierarchy for any any of the leaf note and and this helps in that this ah this path length will always be less than or equal to two d and this will give me a similarity between these two concepts so ah so what is the problem with this l c similarity or the previous similarity that we have seen so what they are saying for any two concepts find out the path length and and one and the similarity is nothing but a function of path length if path length increases the similarity decreases but when problem with with these approaches is that any two pairs of concepts if the path length is same the similarity will be the same irrespective of wherever they occur in the tree so let us just go back to the previous tree so what these approaches will say so what is similarity between coin and nickel the path length is one similarity is one divided by one plus one that is point five but what would be the similarity between entity and abstraction their path length is also one so their similarity will also become one divided by one plus one so that is point five but ideally what would we want do we want the similarity of entity of entity abstraction to be the same as between coin and nickel so if we think about it as we are going down in the hierarchy we are moving to very very specific concepts so while we are moving this specific concepts the same path length should amount should amount to a higher similarity noun that it was doing earlier so entity and abstraction this similarity should be much lower than the similarity this similarity should be very very high but these methods as of now as of now do not capture this idea so they will have this path length to contribute same way as this path length so now can we do something different so that here the similarity becomes high but here similarity becomes low so so you want a matrix that lets us assign different lengths to different average so so the ah so what is the idea that we will be using and for that we use the idea of concept probability model now what is this so ah so in the wordnet whatever concepts we are seeing we will assign them into probability so what is the probability with which i see this concept in a corpus now idea would be whatever i am seeing in my corpus is an entity because its part of the tree where entity is the root so whatever word is in the tree is an entity so in the other word whatever word i am saying in the corpus is an entity but it may not be an abstraction so there will be some words that are abstraction and some words that are not so what i will do whenever i encounter a word i will find out what are all the concepts to which it contributes and i will add a count to all these concepts and finally i'll give i will convert them to probability values so what would happen the root note will get a probability of one because every everything i see is a is an entity but as you go down these values will keep on decreasing i'll use this idea ok to convert them into log log values and then taking the difference between the two values as the path length ok and that can converted to finding the similarity between two synsets so so let us say p c is the probability that are randomly selected word in the corpus is an instance of c so what would happen probability of root is one and a lower a note in the hierarchy the lower is its probability and how it can be estimated these probabilities we count something called concept activations in the corpus so what would happen whenever encounter a nine i'll also increment a call for coin currency standard etcetera so what is the idea so i have a wordnet hierarchy tree is starting from root that is my entity and going down so what would happen suppose this is my word x whenever i encounter this word x x in my corpus i increment its count by one and all its parents because whenever i am encounting encounting i am also encounting this concept and so on so what would happen whatever i encounter i always add one to the root ok but only to its parents so when i do that root will have so all the counts will be added to root and i can find the probability by dividing everything by the count of root so so suppose i do that on my corpus so this is one example so so here you can see there are one point nine million roughly instances overall because entity has been ah always ah appended with one but their num [ bers ] - the numbers are different so diamond and nickel has only eight and ten coin has thousand hundred and eight ok so you see the we are not showing the whole tree thats why there will some other branches of coin also that are not here now once i have got these counts i know opt in the probability values by dividing everything by this number so this will be my concept probability now how do i use this concept probability to define or define my so what i will do i will convert them in some in some information value what is the information content of each concept and the information contemp content can be directly obtained by the probability values by using minus log probability ok idea is that if the probability of something is very high it does nt have much information but the probability is low it contains a lot of information so i use the information content of a concept as a minus logarithm of the probability of that con [ cepts ] - concept so i can do that for all the concepts and i can also define what is my lowest common subsumer that is if i take two notes or two concepts c one and c two the lowest common subsumer is the lowest note in the hierarchy that subsumes both the notes or what is the is the lowest note in the in the tree where these two concepts meet and now you can see how you can use that ah computing similarity between concepts so these by using from the probability if i take minus log probability the other different numbers i will get so minus log one becomes the different numbers i will get so minus log one becomes zero then point five minus six and so on now what is something that you are seeing here so as you are going down these numbers are increasing so one simple way of capturing ah similarity between two words is by seeing what is the lower common subsumer and what is the information content of that so nickel and dime what is the similarity path length is two but the lowest common subsumer that is coin has a information content of seven point four five five on the other hand if i take these two concepts medium of exchange and scale their common lowest subsumer standard has an information content of six point one one so immediately you can see this this can be said to have a higher similarity than this pair ok so what are the formal method by which we can capture this so one is resting similarity so that says how similar two words are depends on how much they have in common so that is find out their lowest common subsumer and find out their information content of that and that will denote the info the similarity between these two concepts so it measures the commonalty by the information content of the lowest common subsumer ok so like here nickel and dime the similarity would be the information content of the l c s that is coin so similarity between them is seven point four five five now nickel and money similarity would be the information content of their l c s that is medium of exchange so that would be six point two five five and so on ok so now immediately you can see that as you keep on going up in the hierarchy the similarity will be decreasing so to be highest when you have took single leaf notes but if you keep on going up the similarity will ah decrease so this capturing what you wanted to do but still there is one problem here so can you find out what is the problem so one problem here is if i find the similarity between coinage and money this would be same as similarity between coinage and budget yes because their l c s is the same so here what is being captured is that how much information they share but what is not being captured is how much information they do not share or how much they are different so we have a different measure for capturing how much information they do not share and this is called lin similarity so it says that ah the similarity this measure is not about just commonalities we also have to capture the differences so the more information they share the more similar they are but the more information they do not share the less similar they should be ok so accordingly the lin similarity between two concepts is defined as two times logarithm of or two time information content of th l c s divide by information content of the concept plus information content of the concep concepts so while doing that what would happen now if i take the similarity between coinage and money this would be six point two five five times to divide by seven point four one nine plus eight point zero four two and if i take the similarity between the coinage and budget it will have a term of ten point four two three in the denominator instead of eight point zero two four two so immediately this similarity will become lower than this similarity so lin similarity is a much more well accepted measure than the ah the previous measure that we have seen the resting similarity there are some variations here so for example the jc similarity so what they say in jc similarity find out or give a value to each h that is a distance between two concepts and this would be the information content of the concept minus the information content of the hyponym and i can define the distance between two concepts as their distance so how do i go from one concepts concept to its ah l c s and second concept to it its l c s and i just add the distances and i compute similarity by taking the inverse of this distance so what am i doing here i am having different notes in my hierarchy yes and suppose that you have already found what is their information content ok so what do you do in in the case of resting similarity in resting similarity the c one c two c three and this is c zero in resting similarity the similarity of c one and c three is nothing but the information content of c zero so as per resnic similarity of c one c three is i c c g one ok as per lin similarity this would be two times information content of c zero divide by information content of c one plus information content of c three now in jc similarity what you would do you would count you would find out the distance this is distance is nothing but c one minus c zero and this distance is c three minus c zero so for gsc similarity it will be c one minus c zero plus c three minus c zero is the distance between these two concepts and the similarity between the one divided by that ok and thats what is written here information content of c one plus information content of c two minus two times information content of the l c f of the two and the thats what you can say here i c f c one i c f c three minus two times i c f of a i c s so i hope by this example you understood what is the difference between resting similarity lin similarity and jc similarity and among these lin similarity is very very popular ok so i hope it is clear that how do you apply these three different similarity measures so here you have example that a if we use the jc similarity what is the different values you will obtain among different concepts so i'll ena increase that you will you try and find out that say suppose between nickel and richard scale or between nickel and coin can you obtain the same values by by applying the formulas so now i will ah ah also talk about briefly the other approach for computing similarity between two concepts in in wordnet so till now we have only used the the hierarchy tree in wordnet and i am saying two words are similar if they are nearby in the in the hierarchy tree or some other formulation and that that we have seen some examples now suppose i want to use their glosses the way their the different concepts are defined in wordnet for comparing similarity so these are very simple algorithm called lesk algorithm also have a extended lesk version that is used for that and what is the idea two concepts are similar if their glosses contain similar word and the word drawing paper defined as paper that is especially prepared for using drafting and decal the art of transferring designs from specially prepared paper to a wood or glass or metal surface i want to find out how similar they are so what i will do i will see how many words are common there so as such you are seeing that three words that are common paper especially and prepared ok that a kind both the glosses so what algorithm does is that it counts how many n grams are common so n gram in the sense of one unigram bigrams and trigrams and so on see you will see that this bigram is specially prepared is common to both the glosses glosses and the unigram page is common and whenever an n gram is common it adds a score of n square so that you have is given for a commonality of bigram trigram and so on so in this case what would be the similarity one bigram is common so two square and one unigram is common and one square so similarity would be two square plus one square five ok one plus four five that is the similarity of using lesk algorithm now so so we have talked about what are the different ah relations we can capture using wordnet and we have also seen how we can find out similarity of across two words and that looks like very simple method once the wordnet is given and you might ah also wonder this might this might be a better method of capturing similarity than ah distribution similarity ok because i i have a manually created this orders i know which of the words occur where in the tree i can simply use the distance or measure to find out how similar they are but there is one ah inherent problem in using wordnet for any of the task so can you think of what is the problem so so let me give you the hint if you know if you want to capture similarity across synsets wordnet is very good it can capture the similarity between the synsets very nicely but if you want to catch similarity between words that is very difficult now when we encounter natural language we will only encounter the words and when we see the words we do not know what are the synsets that are being used so i cannot directly apply wordnet there because i do not know the sense i can only do an approximation where i can find out ok i am assuming this word corresponds to or i am i am applying the methods i used for synsets for the words ok this would be an approximation and it works sometimes but does not work some other times so to be able to apply wordnet to be able to use wordnet one important problem that we have to deal with is i need to find out if a word is used in in a particular sentence what is the what is the wordnet sense that has been used for that ok and that is is difficult problem that we will try to address but this problem is very very common and we will just take a very simple example for that so we see very sim [ ple ] - very very easy sentence i saw a man who is ninety eight years old and can still walk and tell jokes yes this is a very simple sentence now for the simple sentence what do you think are there many different synsets of this word or this whole sentence or there is one interpretation so when we when we hear this term this sentence we have only one interpretation in mind but in wordnet what are different synsets of individual words so let us see so if i go to wordnet the word saw has twenty five senses man has eleven senses age has four jokes has four tell has eight and so on so now if you combine if you combine all these together they are as a sixty seven million plus senses that are possible sentence this might look like a very extreme case but you might have examples where they are multiple divisions for the same sentence and different words can have can occur in multiple synsets so my problem is if there are so many synsets how do i find out what exact synset wordnet is being used and thats where we'll talk about the problem of word sense among the many possibilities dissimilate the particular sense of the word and that we will start in the next lecture thank you