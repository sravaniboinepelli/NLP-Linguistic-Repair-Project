ok so welcome back for the forth lecture of the first week so in the last lecture we discussed why is then we had and we we took examples of various cases of ambiguities in the language and some non standard disease also ok so we will actually go into a text corpus and try to study some very simple empirical laws that are very very universal about language ok so now before going to that i will i will try to make a simple distinction clear to you so what are the difference between function words versus content words in language so whenever you see a language in it's vocabulary there are various kinds of words and they can be called either function words or content words what are the difference between those ok so function words are mainly used to make the sentence grammatically so things like where is determine us a and ah and pronouns and all are function words content words are various nouns and verbs that convey what are the important concepts in the sentence ok so content words are mainly used for determining the structure of the sentence so now this is in this is your example to give you an idea about what are content words and what are function words so you see here the same sentence as we written in two different ways in the in one of the cases we have replaced the content words by some garbage words so this may dove words that are not in the language in the other in the other example we have the function words by some arbitrary words that are not in the language ok so can you try to find out in which sentence we have replaced the function words with something else so we will see here in the second sentence so we have removed the word like the and replaced with words like glop ok and and so on in the first sentence we have replaced all the nouns like angry and investigator with winfy prunkilmonger that is is research to the garbage word that is not the language now now try to see the two sentences and in which sentence you can see the structure of the sentence clearly and in which of the sentence you can understand the topics of the sentence clear and you will find out that in the second sentence you know all the topics so we are talking about some investigator government and infuriated and all that ok the information you cannot get from the first sentence when in the first sentence you can understand the extraction of the sentence the some x some noun from something and a verb and again a verb all which a noun and a adverb ok and you do not know what are these nouns and adverbs but you know what is the extraction of these sentence do a certain filers in the in the sense of various nouns and verbs so in general function words in a language are closed class words what i mean by that there are very specific grammatically categories that can be called as function words and they they they are generally closed we do not have newer and newer functions what coming into a language ok some examples of function words are like prepositions pronouns auxiliary verbs conjunctions grammatical articles and various particles in the language ok so now let us take a take a corpus and try to see what are the distribution of words that we encounter in the corpus so here we have taken tom saw sawyer so that was a noble written by mark twain and what we are doing in the slide we are just reporting which of the words occur with what frequency so this is a sorted list a starting from the highest frequency to the to some top fifteen twenty words and what is the grammatical category ok so the top word you see here is the so that occurs with the frequency of three thousand three hundred thirty two and this is very very common across different language corpus ok so if you pick up any arbitrary language corpus you will feel that the distribution roughly is so now till we have a look at this list and find out what is special about this list what is the category of words that you see here so so so remember we talked about two categories function words and content words can you try to find out which of the category you see here so we see most of the words here are function words ok so the and a to of and you can see also by the grammatical categories that is also written here so so one thing like that we see in this distribution is that this list is dominated by the little words of english that are that are very important grammatical roles for for the sentence ok now what is one exception that you see to this so we call them function words that we let me say a determine us prepositions complementizers like the word that here so now what is one exception that we see in this list so that exception is probably the word tom so tom is very specified to per topic of this text ok so this is about tom sawyer so you see the word tom also as a very high frequency and this makes into the list of top frequent words ok so so again if you take an arbitrary text and try to find out some top frequent words mostly they will be function words also sometimes known as where is this top words plus you can find some appearances of words that come with the topic of that text so in this that is a word tom so let us try to see something else how many words are there in this text what are the various ah technical terms why which these are indicated so for that we will talk about the type token distinction so type token astrology philosophical distinction between a concept concept is a type and a particular instances of the concept ok so concept is type and instance is a concept are tokens so what do i mean by typing token when it comes to a language ok so a language if the same word occurs multiple times ok so i call them the same type the same type same concept but different tokens ok so if i write the same word say will will the same word twice i say this is a single type this is only one type but there are two tokens so each individual occurrence is a different token but they have the same type so in other sense by type i mean some sort of unique words in my vocabulary and token means the number of words they are not unique so if the word occurs five times i will count it as five different tokens but the same type now so once we have understood what is a distinction between type and token there is a particular concept that is called type token ratio that is you should describe some text ok so what is type token token ratio very simple the ratio of the the number of types that is number of different unique words to the number of running words so all the tokens so you find out the number of types in the text divided by the number of tokens in the text and you get the ttr type token ratio ok so what this ratio indicates that tells on an average how often a new word appears in the text ok so if type divide by token that is my type token ratio if it is high that means in the text lot of new words are keep on coming because the number of types is very close to the number of token but if this is small that means the small words are getting repeated again and again in the corpus ok so suppose we take two different text corpus one is mark twains tom sawyer another we take complete shakespeare work and we try to find out what is the ttr for each of this so our mark twains we find that there are seventy one thousand three seventy word tokens and eight thousand eighteen word types so how do we complete ttr you just find out we just divide types by tokens so eight thousand eighteen divide by seventy one thousand three seventy that will give me the ttr that is close to point one one two now if you take the shakespeare work you find similarly there are eight eighty four thousand plus tokens and twenty nine thousand plus types and type ttr comes to be much more than that of tom sawyer point zero three ok suppose i take four different types of media so four different types of media like conversations if two people are conversing academic prose will be scientific articles and books news and fiction i take four different sorts of media in which ah language is communicated now can you try to have a have a guess which one will have the highest ttr and which will will have the lowest ttr so so let us go to the definition of ttr so if high ttr it will be tendency to use the new words what what about low ttr if you have low ttr that is tendency to use same word repeatedly so now now given that so i have four different ah media one are conversation then i have academic prose then i have news and fiction now which one do you think will have the tendency to repeat the same word again and again so we think over it in conversation we try to repeat the same word again and again so whatever you are conversing i will try to take it from there and repeat it so conversation have the tendency to use the same word repeatedly so that means they will have the lowest ttr ok on the other hand it has been found in news you have tendency to use newer words so news this is the highest ok so can you can you guess which of the remaining one will have the second lowest ttr where the same words have been used repeatedly and and that is actually the academic prose because academic prose is very very formal so they are very very particular kind of words and verbs that that we use ok so that's why ttr is again found to be low in academic prose ok so ttr is scores the lowest value in the case of conversations and highest value in news and the academic prose as the second lowest ttr ok and this also make sense now one thing you must be conscious about that ttr in in itself is not a very valid measure of finding text complexity ok you might say that ok if ttr is high text might be complex but this is not a valid measure in itself so why is that so so think of this scenario if you are using more and more text what would happen to your ttr ratio ok suppose your text contains you are taking a running text so till here you have thirty thousand tokens now you go forward in your text and you go to sixty thousand tokens now what would you guess can you see the ttr at this point it will always be lower than the ttr at this point yes because whatever words were there in the corpus most the words have already been introduced in the in the first half second half most of the words will be again getting repeated ok so the number of tokens will increase linearly with the size of text but number of words in a vocabulary do not ok so that's why if i take say a tom sawyer versus shakespeares works you will see the ttr is high in tom sawyer than shakespeare work because they since having a much larger number of tokens so when it in itself it cannot be a valid measure so when you take into account the size of the text so another way in which this is computed is by taking a running average ok so you take a running average on some consecutive thousand word chunks so what i mean by running average suppose i have a text that as five thousand tokens so now you first compute ttr for the initial thousand tokens one to thousand ok get a ttr for that now you take start from two to thousand one i didnt have ttr three to thousand two compute ttr and then take an average so you compute ttr on moving window of thousand length thousand and then you take an average so that is considered to be a better of a ttr ok so now so you see that we will talk about some empirical laws so for that let us try to have a look at some of the frequency distribution that we see in tom sawyer ok so remember we we said that the ttr on tom sawyer is roughly point one one so what does it indicate it says that on an average here what is repeated nine times yes try to token ratio is one by nine roughly now what is that mean does that mean that each word occurs nine times in the text or is it something different so what we see saying saying here that words do not have a very even distribution it is not happen that each word occurs nine times in the text so what is the usual kind of distribution that we see in the corpus ok so on the left in the left hand side of the slide so you see i have two columns first column gives me the word frequency second column gives me frequency of frequency what do i mean by frequency of frequency how many words in this corpus have that frequency ok so take the first row that means there are three thousand nine hundred ninety three words in the corpus that occur only once they have frequency one and there are twelve hundred ninety two words in the corpus that occur twice and so on similarly there are hundred and two words in the corpus that occur more than hundred times ok so now once you see this statistic then immediately see that the distribution is not very very uniform ok so what are the two different observations that we have here ok so first observation that we make are most words are real so which are the real words the words that occur with frequency one or two so we see here roughly fifty percent of the word types remember in tom sawyer roughly eight thousand what else where their so roughly fifty percent of them occur only once so there were frequency of one and they are also called happax legomena ok so that is a greek term for read only once they occur only once in my corpus what is the other observations that we have hundred words account for fifty one percent of all the tokens of all text so you had only hundred and two words here that were higher frequency greater than hundred but the together account for fifty percent of two of the corpus so this is again a strange so now this particular observation gives the first empirical law that we will study that is called zipfs law ok so very very popular law in in the case of language corpus how the vocabulary is the distribution behave ok so you see zipfs law what do we need to do first take a large corpus ok now take find out what are the individual word types and count the frequency of each word type ok now what you need to do you list the word types in their decreasing order of frequency ok so for example suppose in the previous case we found there are various types like the an and tom and so on in the corpus i will find out the frequency so this occurs with four thousand frequency this occurs with thousand this occurs with say two thousand and this occurs with thousand five hundred and so on there are various first occur occurring the different frequencies now the first thing i will do i will try to sort them in the decreasing order so first i will have the then i will have and then i have tom then i will have an and so on all that the words in the vocabulary so they will have their frequency this is four thousand one thousand sorry two thousand one thousand five hundred and thousand now i will give you then the ranks ok because i have already sorted in decreasing order i put the rank so there is this rank one rank two rank three rank four and so on ok so now what is zipf law say so zipf law gives me a relationship between the frequency of a word the f in this list and the rank of this word ok in this sorted list and what is the relation that zipf law gives a relation is very very easy so relation is the frequency is inversely proportional to the rank of the word ok so inversely ratio between the frequency and the rank so that is i can say that if i multiple f dot r i will get some sort of the constant f dot r is a constant this is for our arbitrary example that we took in this case we can even see that this holds so four thousand into one is so this is four thousand this is again four thousand this is four thousand five hundred and this four thousand they are roughly the same ok so what it means suppose i make a list like that and i have two words so i have a word with as a rank of fifty and remember that as a rank of one fifty and i find it's frequency f one it's frequency f two so i can see that f one is three times f two so f one is three times f two ok so the fiftieth most count word should occur with three times the frequency of the one fiftieth most convert ok this is what zipf laws tells me we can write it in also a different manner so we talked about frequency we can also talk about probability ok so let p r denote the probability of a word with rank r so what is pr pr is nothing but frequency of the word word of rank r divide by the number of tokens that i seen the corpus that's how i will compute the probability of that so n is the total number of word account says tokens ok so what does zipf law tell so it says frequency is inversely proportion to r i can also write f divide by n with some a by r ok so what i have done i taken the constant k i have written that it as a times n ok n is a number of tokens in my vocabulary n a is from other constant and it's fixed given a corpus and using in the constant so now what why we are doing that because it as been seen in that in the corpus a's when we take a valid close to point one ok this is generally more fix than the value of k ok so now going back to the a jumble of tom sawyer let us take some words their frequency and their rank and see whether the relation that f dot r is roughly constant holds to it ok what you are seeing here so we have arrays words like the and he there and so on we have written their frequencies and their ranks and what is f dot r so what the what we are seeing here f dot r if you start form the forth word say he this remains roughly in the same range ok is start it remains in eight thousand to some ten thousand four hundred and two hundred ok it does not the range does not vary a lot so that is very nice empirical evidence that zipf law holds and let us see we will see in various corpus that f dot r remains constant from most of the words that you will see ok so now so zipf law is one of the very very important law so that we have just see but zipf as given some other laws also there are not so popular but still we will is try to have a look quick look at this so one of the laws is relating the number of means a word as with it's frequency so what this law says so the number of meanings m that a word as obeys the simple law that m is proportion to the a square of roots it's frequency so that is as i as we have words with higher and higher frequencies the number of the different sansage or the meanings will which they can be used also increases ok so now if i use it with the first law so what do we get so the first law says that frequency is inversely proportional to the rank this is law one and the second law says the meanings are directly proportional to the as a square root of frequency this is law two if we combine these two laws together we can get meaning r inversely proportional to the rank of that ok so that means if i if i put the words in the decreasing order of your rank so you are starting from rank one to rank two and so on so now so they mean number of meanings will also keep on decreasing so these clause hold an average ok what is the empirical support so as i were saying the hold on an average so if we saw the words that are having rank roughly close to ten thousand so they have roughly two two point one meanings if their words that are having rank roughly five thousand they are found to have roughly three meanings and the words that are having having rank plus two thousand they were having roughly four point six meanings ok and you can do the math and you will find out this roughly ah this roughly follows whatever we have seen in this law ok and these are the law by zipf that tries to correlate the length of the word that means the number of characters the word as with the the frequency of the word and it says the frequencies of the word is inversely proportional to it's length ok and in can you try to see that in the case of english so the very very long words are very very really used so their frequency is very very small but some of the very small words like examples like function words they are used very very often and that also make sense because language is used for having some efficient communication ok so you cannot have long words that are very very common then you will have to write a lot ok so the words that are very very common are generally short ok so we talked about zipfs law now so for doing the processing of language what is the impact of zipfs law so we said there is a good part in the and there is a bad part so what is a good part so in the language we saw that so remember there were two important observations that we made from a zipf law what was those so one observation was that fifty percent of the vocabulary that means the types occur only once so they are very rare rare words and this is most words are rare words this fifty percent of the words occur only once and what is the their observation you say that roughly hundred roughly words account for fifty percent of the tokens ok so that is from common words are very very common they occur they occur a lot so words are very common ok they of fifty percent of your text so now given these observations what do you think is the good part what is a bad part when it comes to an end ok so so this is a bad part ok why is it the bad part so what it says is is that a fifty percent of my words in the vocabulary they can only once so it is not easy for me to gather some statistics to do some meaning analysis so there are only once let's all the knowledge i have about these words ok so that's why this is called the bad part i cannot do lot of modeling for these words and the other part that hundred roughly hundred words account for fifty percent of token it's we call sometimes the good part why because for most of the analysis you can always remove these words and so will reduce you total number of tokens by half so we will have to process roughly half of the text because how the text is simply these stop words and the function words ok so that's what i we mean meant here the good part is that the stop words account for a large fraction of text thus they eliminate if i eliminate the stopwords it reduces the size of my text roughly by half and the bad part is most of the words are extremely rare and it is very very difficult to gather data to some meaningful analysis with each words ok so now so we will talk about one more law here so that is the relation between the size of my vocabulary and the number of tokens in my text ok so so that is how does the size of my vocabulary go with the size of my corpus ok so we talked about type token ratio so that is one sort of analysis that we can do but that does not give me a particular law so what will be the rough distribution their rough relationship between them ok so we have heaps law they tries with the relationship between the size of my vocabulary that we call as v and the number of tokens n what this law says that it gives a symbol relationship it says the size of my vocabulary is some constant times k n is a number of tokens to the power beta and the ranges are also observing the corpus so k has been found to be roughly in the range of ten to hundred although this can vary this is our roughly this as been found to be in this range and beta is very close to the square root so it is found in the range of point four to point six so what it says so vocabulary generally grows as for the a square root of the number of tokens so that also make sense remember we are talking about what if i keep on teaching my vocabulary i will tend to use the sorry keep on teaching my tokens i will tend to use the same word again and again so number of unique tokens or unique types in each roughly to the square root of my ah corpus size ok so what is some sort of empirical evidence for that so so here so there are four different corpus that has been taking care of taken taken so like moisture wsj and there are some other different new corpus and what what this plot shows on the x axis you have the words in collection in millions so how many words in my collection so there are ten million twenty million thirty million forty million on the y axis it is showing the number of words in a vocabulary in thousand so twenty fifty thousand hundred thousand hundred fifty thousand and so on and the plot so the relationship between the vocabulary and the number of tokens and you see at the plot shows roughly is square root behaviour ok and this as been found to to match with whatever he proposed so this is again an empirical law that is very very universal to the language ok so that's for this lecture and in the next lecture we will start talking about some actual task related to the preprocessing of the corpus thank you