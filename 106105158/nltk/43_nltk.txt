so welcome back for the second lecture of this week so we had a started a formulation of topic models in the in the last lecture so but we mainly focused on ah the basic intuitions so so today we will do the the so mathematical formulation of ah latent dirichlet allocation so that is the main ah sort of topic models that i used also known as l d a so so we were here and we were seeing that what is the central problem problem of l d a so problem we were saying was that we we have observing only the documents but we know nothing about all the parameters of my generative model so i do not know what are my topic ah what are my topics what are the topic proportions of the document and i do not know what is the per document or per per word topic assignment i do not know that so my real problem is from my observation i want to reverse the gen gen ah generative model and come up with all these probabilities so ah so yeah so this figure will ah will help you in understanding this goal so remember we saw an earlier figure where we had all these topics given to us we knew what are the proportion topic proportion for this document and then we were drawing topic for each word in this document but now so in in reality i will only have the or the documents so i know all these documents so i know what are the words but i do not know anything about what are my topics what are these proportions and what are these topics assigner for individual word so i have to compute the distribution conditioned on all the documents that i am seeing in my data so ah another simple example to listen the addition behind the generative model so here you have some thirty seven thousand text passages from some educational material and suppose you run l d a you found roughly three hundred topics so here in this slide what you are seeing you are seeing four different topics so you are having the topic like two forty seven which has what like drugs drug medicine effects body etcetera another topic having words like red blue green yellow white another word about mind thought remember memory thinking and another topic about doctor patient hospital care and so on so therefore the topics that that are there in the corpus by running l d a over these thirty seven thousand text passages now to to get intuition about the generative model so what was in the generative model you have some topics now how to generate topics we take some of these topics in a in some proportions and you start generating words for that now so so suppose you try that so suppose you take topic two forty seven and five ok so so suppose i give an equal probability of the first two topics so what can a document you can generate ok this thing we will see these topics so first topic is about drugs medicine person marijuana second and another topic is red blue green colors so suppose i blind these two topics together so what kind of document can i generate ok so there can be some different sort of documents you can think of so one could be that someone who was taking a lot of alcohol marijuana and so on and it affected it's color perception so suppose you want to document general document like that we will blend these two topics together and and write that similarly suppose you want to blend these two topics together ok so this topic is about mind thought remember memory forgotten ok and this is about doctor medical nurse patients and so on so here if you blend these two topics together you may generate the document that says ok someone who had suffered some sort of memory loss and it's led to visit of the doctor so ok so like that you are having different topics you can blend some of these topics and then generate doc document so this is the generative view ok so we can give a equal probability to the first two topics that gives some sort of documents and equal probability to the last two topics that give me another sort of document so now this single this single picture explains both the the parts the generative part and the inference part together so what you are seeing the left figure so this is the generative model so you are generating some documents and how are you doing that so we are having some topics suppose you are having two topics probably one topic to each topic is nothing but a distribution over words so you are having some words like bank loan bank money loan ok so these mean this words coming multiple times here so this words are having high probability on the other hand second topic is river stream bank river so on so these are two topics now using these two topics i am trying to generate my documents so how can i generate the documents i will mix these topics so suppose i only take topic one and i can have a generate document that contains words like money bank loan and so on i can just take topic two and generate words like river bank stream and so on but i may also take both these topics instead say equal proportion and generate document like money bank they were loan stream bank money so what you seen here in this document to the words are labeled with topic one as well as two because the words could have come from another topic on the other hand in document one all the words are labeled with topic one doc three also all the words are labeled with topic two because we have the only topics pre possible in these documents so this is my generative model idea so here everything here you have the topics you have per document proper proportions and you also have per document per word topic assignment so you know the topics for each individual word also now what is the statistical infe statistical inference part so inference part you only know your three documents so you know my doc one contains these words doc two contains these words doc three contains these words especially but you do not know what are your topics and you do not know what are the ah proportion of various topics that are represent in each document so all these numbers you do not know and you also do not know for each word what is the topic assignment and all these you have to infer so i i hope with this figure this clear what do i mean by my generative model and what is my problem that is to infer all these probabilities of my generative models only from my observation so now important points about l d a so first of all it uses a bag of words assumption so i hope by now you understand what is a bag of words assumption that is i am not looking at the order in which the words occur so this is like a bag forming so i am taking all the words and putting them in in a set it's not like in list where some order is important so l d a does not model the word order as it's so it takes only what are the words present in any order second and i guess he would have also noticed that in the previous ah example l d a s are also good at capturing polysemy so remember what is polysemy polysemy is a given word might have multiple senses so in the case of topic models what can i translate that that means the same word might correspond to multiple topics and this is perfectly allowed because each topic can have it's own probability distribution so that means the same word can come in two different topics also i remember the previous slide we are having the word like bank that was coming in both the topics and that is perfectly allowed so that way topic models can capture better this word is coming from topic one and topic two in it's different senses so the way the model is defined there is no notion of the words being mutually exclusive to the topics ok so for example money and river can give high probability to the word so both money and river topics can have high probability for the word bank you think that is perfectly defined now to understand the l d a model so what is the l d a model so you have to first see what is a graphical notation ok so if you are not gone through some of this graphical notations so this slide tries to explain how do we interpret the graphical notations so here what you are seeing i am having some variables so having a variable y and some variables x one to x n so ah all these nodes when that i see my graphical ah model these are random variables so i have random variable x one to x n now what are these edges these edges will denote the possible dependence so here i know that x one depends on y x two depends on y and x n also sub two x and all these depend on the n y but there is not depend on each other so that's why there is no edge from x two to x four and x one to x two so these depend only on y then there is a difference that some are shaded some are not so what is that so observes observed variables are shaded so that means these are the variables that i am observing and this is a hidden variable that is not shaded and to simplify this notations what we can use we can also group some variables together so some max instruction that is being repeated you can put it in the plate notation so this is you are seen in the in the right hand side so you are having these and different variables and you can group them together by x n and you write here capital and that means x n is repeated capital n times so this and these are equivalent and this further simplify these notation ok so once we have seen this ah so how do we interpret graphical notations then we can look at look at the graphical notation for l d a so what is the so ok so just one more thing that once we have this notation we can also compute the the probability of this ah the whole graphical structure so i have variables y x one to x n so probability would be i have the probability of y and each of these depend only on y so the probability of this whole structure can be probability of y times probability of x one given y x two given y up to x n given y so so this graphical ah the the structure of my graph also defines what are the conditional dependence between various variables and that i can use to write my joint probability distribution ok so this is my joint probability distribution over all these variables so now let us see what is the graphical model for l d a so now so you know ah from the previous slide how do interpret graphical model so all these nodes are random variables and observed variables are shaded ok so the only shaded part is ah w d n these are my object words everything else is hidden now what are everything else so there are some plates here so let us see this one capital k and what are these topics so what i am saying here there are capital k different topics and each topic is a probability distribution ok so this is the probability distribution over top over different words for a given topic for a topic it's small k ok so beta k is what is the probability of word one what is the probability of word two etcetera for the topic k and this i am doing for all the capital k topics then let us look at this part so i have capital d different documents in my collection so for a given document small d i have figure d variable that denotes per document topic proportions so for this document is small d what are the proportions in which you are blending different topics together so this will be different for different topic different documents so you are having capital d different documents and there is again a dirichlet parameter here and a topic hyper parameter here so we will discuss what are these ah but right now you can understand by simply these beta k is the distribution over words for a given topic and theta d is a distribution over topics for a given document now let us go inside so here now we are going to one particular document and this can have capital n words and what are these so you are having per word topic assignment z n yes ah each word will have only one topic and this is the actual word that you are observing and these are all hidden so i do not know what my betas i do not know my theta i do not my z n so this is only the explain what are my ah what is what are different nodes here what are the different plates here so your plates corresponding to topics documents as well as the words in a single document and you have all the variables that we were ah using ah all the the notion that we were using earlier there are variables for each of these now let us see how what is the actual generative model how we generated the words using this structure so what is the generative model so firstly you draw each topic beta i as for the dirichlet higher higher hyper parameter eta ok and you do that for all the k topics so first of all we generate the model you are drawing drawing your topics ok so fine i have my capital k topics now now you go to your documents so for each document that is small d draw topic proportions so you find out what are the topics that are involved in this document as dirichlet distribution over alpha so we will talk about this dirichlet distributions what what do i mean this dirichlet distributions so you draw a top topic proportion ok and then for each word in my observation so far i am go now going to these document for each word you draw z n as a multiple from a multinomial distribution over theta d and draw a word from this ah so draw a topic and then draw a word from this multinomial distribution over beta z n now yeah let's try to understand this a bit more so let us go one by one draw is topic beta i is your dirichlet distribution over eta so ah so what i am doing here i have k different topics so i am trying to draw the proportion the the probability distributions from my ah vocabulary so what i am saying so i have k different topics topic t one t two t three up to t k for these topic i am drawing beta one beta two beta three and beta capital k and what are these they are nothing but the probability distribution over my ah words in my vocabulary so it can be something like yeah this is point zero one point zero two point zero five so on this can be zero zero point one so these distributions are drawn by using a dirichlet over eta so dirichlet it's a distribution over distributions so it helps you decide what kind of distribution will be preferred over another so that we will again discuss so this we will discuss later but the idea is which kind of distribution you will prefer and this helps you draw some distributions for all the k topics ok now so this you have drawn now the next line says for each document draw topic proportion of theta d using dirichlet over alpha so now these are your topics but in your collection you have again capital e documents so document ah one two ah small d capital d so they are capital d documents in your collection now what is it what are your drawing for each document you are drawing theta d what is theta d theta d is a probability distribution over the topics so it will be something like ok what is the probability of topic one what is the probability of topic two what is the probability up to topic capital d so for each document you are drawing these distributions and what are theta d theta d are coming from the dirichlet over alpha so again this alpha is telling me what kind of topic distributions will i prefer over others so these are my so first drawing the topics then drawing the topic proportion for the documents now what else now it says now suppose i am going to this document then for each word how do you ab [ out ] - words draw z d n as in from a multinomial over theta d so theta d is a probability distribution and it's a multinomial distribution from there sample one topic ok so this distribution k topics suppose the z n is equal to five that is i am taking topic five so for each what i will draw one so it can be whatever that that comes according to this multinomial distribution suppose it is five now how do i generate the word now i know the topic now i have to generate a word so i go to so it says draw w d n from multinomial of beta z d n what is z d n now z d n knows my five that is the topic you that you draw and now you take multinomial over beta five so that means for beta five fifth topic i will find out what is the so i will have the distribution and from there i will sample one word so i will sample about from this probability distribution and that is what i am generating so this you will do for each of the capital n words in my document and is the whole generative model first you are generating your topics then for each document topic proportions then you are going to the individual topic going to the individual word and generating that word ok fine so that's what we were saying in this slide now so one interesting point here is why do we name it at latent dirichlet allocation ok so we are doing some allocation what why latent and dirichlet coming so ah so latent in this ah l d a say has a same sensing energy latent semantic indexing so that is all these topics are some some sort of latent variables ok so there are some sort of hidden topics some latent topics i do not know what are these topics as such i cannot give them a good maybe good labels also but there are some distributions some hidden distributions over my words and these are called latent then what is dirichlet here so you you saw the dirichlet distribution at two places so that is the distribution that is used to draw the per document topic distributions is a dirichlet distribution ok so that is how am i sampling topics for a given doc document that is coming from a dirichlet distribution and this result is used to allocate the words of the documents to different topics and that's why the word allocation is also coming ok so you are having latent dirichlet and allocation so now we will look at this dirichlet part in bit more in bit more details so what are the dirichlet distributions so if you ah think about it so so if you incredibly try to understand that this is a distribution over probability distributions ok what do i mean by that so dirichlet distribution is an exponential family distribution over the simplex that is the povasitive positive vectors that sum to one ok so when i talk about simplex so you can talk about say one simplex one simplex would be two elements say x one x two such that x one plus x two is equal to one positive vectors that added to one this is one simplex so you can see that they are infinite solutions here ok and you can read it as a line it's a line and this might correspond to say zero one this might be ah one zero so that is topic one has zero topic to each or yeah x one is zero x two is one here x one is one x two is zero and any point you can accordingly give some definition what are the values of x one x two ok so you can see that this will be a line simple line x one plus x two is equal to one this is my one simplex then you can have two simplex then you are having three things x one plus x two plus x three there are two one ok in this would be some sort of triangle ok so this point might correspond to say x one is one x two is zero x three is zero so this might be x one is zero x two is one x three is zero this might be zero zero one and then each point we will have some proportion such that all three add up to one this is my two simplex like that you can define of any and simplex so if you are having k topics you can think of it as k minus one simplex ok so now what is my dirichlet distribution so so now we are saying it's an exponential family distribution over the simplex that is a positive vectors that up add up to one so again let us try to understand that intuitively first so what i am i am saying here suppose i am having a two simplex then lot of different value is my three ah x one x two x three they can take they can take a lot of different values so i can take values like one zero zero i can take values like point three three point three three point three four and things like that they can take a different values like point five point two five point two five etcetera so what my dirichlet distribution does it gives me a probability of these probability distribution so what is the probability of getting distribution like this what is the probability of getting a distribution like this ok so that is what kind of distributions i will prefer so i can use that to say that i will prefer distributions where one topic one of the topics as a high probability and others have very low probability or i will like to have a distribution where all the topics have a equal probability so these kind of constraints you can put by using your alpha ok so that's where the formulation is so what is the probability of theta that is a distribution over these ah topics given my alpha that is some - [ d ] rawn function so even if we forget about this term so this is multiplication over theta i to the power alpha i minus one ok so let's look at this term only so ah what is being said here so probability theta given alpha is multiplication over theta i to the power alpha i minus one ok so let's try to understand that suppose my alpha i is say a point one versus alpha i is equal to say two what would happen in the two cases and let us say i took look at two different thetas so my theta one is zero point nine eight zero point zero one zero point zero one ok one topic has a high probability others have roughly zero and theta two is say point three three point three three point three four and now you can see what kind of alpha will prefer one theta over another one so let us take the case with zero point one alpha i zero point one so what would you is the probability of this getting this distribution it will be ah zero point nine eight to the power minus point nine zero point zero one to the power minus point nine zero point zero one to the power minus point nine ok and this probability would be same point three to the power minus point nine point three three to the power minus point nine and so on on the other hand if i take alpha i is equal to two then this would be point nine eight times so alpha i minus one will become become one the power is one point zero one times point zero one and this will become point three three times point three three times point three three now what is your observation here so one thing we see it that if you ah take alpha is equal to two if you take alpha is equal to two then the the topics where so the distribution where one topic is having very small probability we will get a over all very small probability right your multiple point nine eight by point zero one times point zero one this will become very small only when this will be ok this is like one by three times one by three times one by three so as you increase alpha it will prefer to have topics or or the distributions where each topic the word probability is roughly equal ok so it will prefer this one but if you having a smaller alpha then what you are seeing so now point zero one to the power point nine this will be now can written as ah hundred to the power point nine ok this will become very large and this will be ah ah roughly this will not be large ok so so what will happen as your alpha becomes small they will prefer the probability distribution where one topic is having high probability and others are having low probability so by tuning your alpha you can prefer one topic probability one sort of distribution over the distribution ok so so this is my dirichlet distribution so now again to give you some visualization so here are two different sort of simplex ah so alphas as such you can interpret it as some prior observation count on the number of times a topic j is sampled individual alpha j so that is how many times this topic will be sampled but so so and you can also think as some forces and higher alpha will move the topics away from the corner of the simplex so let me come back to this point again so you are saying two different simplex one where ah the topics are moved away from the the corners ok these are being moved away and here you are going towards the corners and this is for because of higher values of alpha and we will come back to this again so now when alpha is less than one there is a bias to big topic distribution is favoring just a few topics and this is what we saw just now on paper that when alpha is equal to is less than one it tends to prefer the distributions where only a few topics as a high probability and others have lower probability now while in general you can take different alphas for your different topics what is convenient effect you take all help us to be rough to be the same so you have a singular hyper parameter alpha so i will all alphas are same now what matters now is what is the value of this alpha ok so the relatively they are the same all alpha one to alpha ah and alpha capital d all for all the documents they are the same ah sorry alpha one to alpha call capital k they are same but what is the relative number is it like what is the number is it like point one two five there will be a matter so now if alpha is small what will happen you will tend to prefer topics with ah way which sorry you will tend to prefer distribution where one topics will have a higher probability so remember how do we define simplex so in this case what this it is boundary means so this colors denote what is the probability distributions ok so black means a high dist probability distribution and and so on and as you as the ah as it diminishes so the the color becomes ah so faded then you are seeing that the the probability is decreasing so in the left hand side figure the probability is mostly centered for in the in the in the center of the simplex by in right hand side it is moving towards the corners ok so that you can interpret it as if in this simplex whenever all three topics have the same weight it is given a higher probability here even if one topic is having in in more proportions than others it is getting a high probability so this is not moving away from the center of this simplex so it is going towards the corner so if you go to the corner that means one topic has the high probability other two have the small probabilities ok so this is favoring also to have topics where sorry also to have distributions where one topic has high probability than others while this we favor the distributions where all three topics have roughly equal probability ok so now you can easily tell which one is corresponding to higher alpha lower alpha so the one that corresponds to lower alpha will be like that it will favor distributions where one topic is having a high probability than others now this is some some ah from simulation what happens if you take different values of alpha so if you take alpha is equal to one what kind of distributions are preferred so there are fifteen documents here that are being shown and there are ten different topics so we see the distributions are ok so you are having some distribution for topic t two t three and so on for these fifteen documents they are different different distributions but suppose you try to now increase the value of alpha as you go to ten so as you increase the value of alpha it will start favoring those distributions where all the topics are roughly same probability you see now this is getting flattens so you are having you are now seeing all the topics and if you try to increase ah alpha to hundred you will see all the topics have same probability and it's not something that you will there so it's not good that each document has all the topics in same probability then if the topic model is does not put in any sense so ah is equal to one was looking ok so so this kind of distribution we would like but now suppose i want to i want to decrease the value of alpha if suppose i go from one to point one now what you are seeing it will start favoring the distributions where one topic has a high probability or one or maybe two ok but suppose i increase i decrease it further to point zero one so most of the problems of probability zero only one topic comes as a probability one or here two topics are coming if you further reduce this value to point zero one you get only one topic in each document so that will give you some idea of how if you modify or change this parameter alpha how does this effect the overall topic distribution in my corpus ok certain kind of distribution we will get reference over others now for l d a there are a lot of interventions that are available so ah and these are like very very popular you can also use implementations that are available in gensim so now so we are again so we have discussed all the in genetic part in but now i am full details that how do we what is the generative model of l d a but remember what is your problem how do we infer all these probabilities so that is i am given a correction of documents and i want to infer per document topic assignment z d n per document topic topic distributions theta d and per corpus topic distributions beta k i want to infer all this now once i am able to infer all this i can use this to find out say to use to for information retrieval document similarity and many other task but the question is once i am given these observations of the of the words how do i infer all these ah probability values ah so so there are different ways of doing that so we will discuss about one such method in the next lecture thank you