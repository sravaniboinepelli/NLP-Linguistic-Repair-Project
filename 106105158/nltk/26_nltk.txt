so welcome back for the final lecture of this week so in the last lecture we had started with the concept of inside outside probabilities and how do you use them for answering certain questions like what is the probability of a sentence as per my grammar and what is the most likely parse ok so i have use the inside algorithm specifically to find out what is the probabilities of this sentence as per my grammar and then in the end i was saying that we will also use this this concept to find out the rule probabilities of my grammar and how do we do that again exactly what we will discuss in this lecture so now ( Refer Time : 01:00 ) in general how do you obtain the the rule probabilities and remember this is very similar to what we were also talking about in the case of h m m s when i have to learn the parameters of my h m m i can do that in two manner one where where i am given already the labeled data set that means i am given what are the sentences and what is their past edge if i am given all that so then from there i can compute how many times non terminal derives a particular sequence divide by number of times a particular non terminal has been used that will give me the probability of this particular rule ok if there is only one rule possible from n j this is always you want but if there multiple rules are possible i can find out what fraction of times this particular instance has been used in my labeled data sets and by ( Refer Time : 02:00 ) that i can compute all the rule probabilities now so this is easy but what about the case where the training data is not given to us that is i have no way to find out in which sentence what is the rule that has been used so what is in fact given to us so we are assume that we are given the underlying context free grammar so i am given all the possible rules but what i am not given what is the probabilities for each individual rules so that means the the particular p c f gs part the rule probabilities is not given to us and i am given a lot of sentences and i am the given the grammar that will generate these sentences but not the rule probabilities and my my task is how do i find out these rules probabilities so so in other words how do i find out the parameters of my p c f gs and we will use the same sort of idea that we did earlier that is ( Refer Time : 03:00 ) you are given the observation of sentences find out the parameters of a model that maximize the likelihood of observing the sentences ok so this is what we are going to do maximize the likelihood of the sentences in the data under the p c f g constraints and for that we will use some sort of expectation expression algorithm so so lets take a simple example and what is the intuition for using this so we start with this simple sentence like she eats pizza without anchovies and a particular parse tree is given to us so what kind of rules to you see here so you have rules of the form a single non terminal derives to two different non terminals like s derives n and v v derives v and n and so on and there are rules of the form a non terminal derives ( Refer Time : 04:00 ) a terminal like here and derives anchovies pp derives without n derives pizza v derives eats and so on so these are different sort of non terminals that are given the rules that are given to me i do not know the probabilities for each individual rule now can you think of any other parse for this sentence she eats pizza without anchovies ok so one parse that we saw was she eats pizza and pizza without anchovies modifying pizza ok what is the other possibilities ok so other could be the without anchovies does not modify pizza it is a separate phrase altogether starting from v ok so something like she eats pizza with fog ok in in that sort of meaning ( Refer Time : 05:00 ) so yeah so she eats pizza without hesitation that is another sort of possibility now a single v here is giving me v and n p so this is the different parse tree for the same sentence now i can what are the new rules that i have added v gives me v n and p and n gives me hesitation these are two new rules i have added so now what do you see i have two different sentences she eats pizza without anchovies and she eats pizza without hesitation and both have two possible parse edge and i know what are the all possible rules now my task is given these sentences how do i find out what should be the ideal rule probabilities so now for that let me first define so we need to find out all these probabilities so probability of this rule s gives me n and v probability of rule n gives me pizza and so on now ( Refer Time : 06:00 ) what is the p c f g requirement the p c f g requirement is that it starting from a single non terminal all the possible sets of right hand side that i can generate so that is so all the possible rules starting from a to any alpha the probability should also added to one ok so now if i look at my grammar so i have five rules pair and occurs in the left hand side n gives me np n gives me pizza anchovies and so on so all these should also added to one similarly three rules where v occurs in the left hand side so all these should also added to one and then there are some other rules like s gives me n v n v nothing else this would be one and so on so this constraints i can obtain from my from my grammar i know what are the rules and what is the constraint starting from the left hand side all possible rule should have a probability adding up to one now i have two sentences ( Refer Time : 07:00 ) can i compute the likelihood of the sentences w one and w two so what do i mean by likelihood what is the probability of generating the sentence as of my grammar right now i am not giving the rule probabilities but i can write down in terms of the variable rule probabilities so what will the likelihood of w one ok so for the that i have to take the two possible parse trees so here t one is the first parse tree so here i am giving the probabilities of both the sentences as per the first parse tree ok so probability of sentence w one as per the first parse tree t one and probability of sentence w two as per the first parse tree t one how do i compute that it is very similar to what we saw in the case of p c f g how do we compute the probability of a parse tree if the rule probabilities are given that was very easy here the possibilities are not given but you can parameterize so you will say what is the probability of s giving me n v and so on ( Refer Time : 08:00 ) up to you go to the leaves and i do not know these whole probabilities similarly i can write down this likelihood of the sentence w two as per my first parse tree similarly i can do further second parse tree also for both the sentences so this tells me so if i know all the possible parse trees because my ah c f g s given to me i will know all the possible parse tree i can ah put my all the rule probabilities as variables and define what is the likelihood of various parse tree now what is the likelihood of the sentence it will be summation of the likelihood as per different possible parse trees ok so probability sentence such summation over all the possible trees that can generate this this p phi w t in this case for both sentences w one w two i had two different parse trees so i will just add the two two probabilities to get the likelihood of the sentence and how do i get the ( Refer Time : 09:00 ) likelihood of the whole corpus that has multiple sentences for that i will come to the likelihood of each sentence and multiply those so if i have a sentences w 1 to w n i compute the likelihood of each and keep on multiplying now now we know that how do i express the likelihood of my corpus in terms of the rule probabilities right the only variable here are all the rule probabilities now i can further define my problem so my problem would be so this is some sort of e m approach i will start with some initial parameters phi phi means the rule probabilities i want to re estimate so that i obtain some new parameters phi prime such that the likelihood of my corpus increases now so l phi prime will be greater than equal to l phi and i keep on doing that until l converge ( Refer Time : 10:00 ) so now here we have to apply algorithm so that we can so that we can keep on updating our rule ah rule probabilities this is the parameter of my system and how do we do that if you remember that like what we did in the case of learning parameters for g s analogous to that what we will do here we will start with some arbitrary rule probabilities phi ok and use that to compute something intermediate so in this case what we will compute what is the expected number of times a particular rule has been used if the rule probabilities are as per the current parameters i will compute the expected value again use the expected value to compute the probabilities so i will compute my phi prime again use the phi prime to compute the expected number of times each rule has been used and again compute five dwell prime and keep on doing that until you converge and thats why we will be using the inside outside probabilities so let us see ( Refer Time : 11:00 ) so so idea is that i start with some rules probabilities phi and i am given a corpus that that what are sentences that i observing w i w j and i will obtain the new parameters phi prime using the simple idea so this is something that we were saying if we are given the labeled data thats why i will compute the rule probabilities so i am saying i can always define probability of the rule a given b c as the number of times the rule a given b c a gives goes to b c or a derives to b c has been applied in my corpus divide by all the possible all the different times where a derives alpha for all possible alpha and this gives me the probability for a deriving b c similarly i can compute probability a deriving w by saying how many times this rule has been used ( Refer Time : 12:00 ) divide by number of times a gives me alpha has been used in my corpus in my actual corpus but we do not have any labeled corpus we only know what parse are possible ok and for each parse we can compute the probabilities using the previous parameter phi so how do i write down this count a derives b c number of times this rule has been used for that i use the idea that i have multiple sentences any sentence i can find the expected number of times this rule a deriving b c has been used so that is count a deriving b c is nothing but summation over all the sentences number of times a deriving b c has been used for the particular sentence ok same one for the count of a deriving w each sentence find out the expected number of times a particular rule has been used now how do i ( Refer Time : 13:00 ) actually come up with this formulation expected number of times a particular rule has been used in a sentence and for that we use the inside probabilities and outside probabilities ok so now coming back to this inside and outside probabilities and how do we use that to compute the expected number of times a rule has been used in a sentence now this must be clear by the previous slide that if i can compute the expected number of times a rule has been used in a sentence i can keep on updating my parameters this is the only bottleneck in the previous computation and we will see how to do that using the inside and outside probabilities so let me give the definitions again so inside probability is starting from non terminal a i derive the words w i to w j in the sentence ok so that is probability that a derives w i to w j as per my grammar and the outside ( Refer Time : 14:00 ) probabilities starting from the symbol s i can derive the string w one to w i minus one a and w j plus one to w n ok so it starting from s i can derive w one to w i minus one a w j plus one to w n now once we are given the inside and outside probabilities we can actually compute the expected number of times the rule has been used and the expression comes out to be this one expected number of times a rule a has been a derives b c has been used in my sentence w each the rule probability a given b c divide by the probability of the sentence and this very ah peculiar term that you see that you are seeing here so you are seeing here alpha i k a beta i j b and beta j plus one k c now how how do i actually come up with a term like that and how do i come up with this expression that is expected count is given by this ( Refer Time : 15:00 ) so for that let us go back to what we were discussing in the last lecture that i can multiply inside and outside probabilities to know something about the probability for the sentence so let us go back to that so what we were saying if i multiply alpha j p q and beta j p q what does it give me it gives me the probability that it starting from n one i can derive w one m and it starting from n j i can derive w p q as per my grammar ok so now i can use the chain rule here to write it like that so its probability n one derives w one m so this ( Refer Time : 16:00 ) derives any number of given by grammar times probability n j derives w p q given n one derives w one m and my grammar so now what is this probability that n one derives w one m given by grammar second write as the p phi w probability of the sentence ok and what is this this say what is the probability that this rule n j has been used to derive w p q given that the sentences there and my grammar is there ok so and how many times this has been used in this particular context only one time so can i ( Refer Time : 17:00 ) write down expected number of times n j derives w p q is used that would be alpha j p q beta j p q given divided by p phi w ok and suppose because i do not want to fix this p q i just want to say expected number of times the rule n j has been used so each time it has been used only once for deriving w to w q so here i will have to sum over all the possible p and q ( Refer Time : 18:00 ) so i will say p can go from one to m suppose there are w one to w m so these are number of words and q can go from p up to m ok so this is the expected number of times my rule and j has been used now now what is something that i have to express i want to find out for example expected number of times a rule like a goes to b c has been used or a goes to w has been used what is the expected number of times these has been used now for that suppose let us take the easy case expected number of times the rule a goes to w or a derives w has been used so in this case what i am saying n j derives a particular terminal here that ( Refer Time : 19:00 ) is some p p so i can write the beta j p p for that case and beta j p p is simply the rule probability that is what is the probability that in non terminal derives this word w p so we will see the expression for that so this one is easy but what about this case when the rule a derives b c so in the particular notation that i have written suppose you want to say expected number of times n j derives n r n s is used ok so now what would happen so this beta j p q is when the terminal n j derives the whole sequence p q ( Refer Time : 20:00 ) w p to w q and can use any possible rules yes n r n s or n j n z whatever it can use any non terminals now what i am my limiting i am saying this rule should only use this so this non terminal should only derive n r n s ok so then i am saying so that means my n j will derive n r n s and this n r n s will again derive say w p to some w d and this will derive w d plus one to w q ok so now how do i modify this equation so alpha j p q is the outside probability that will remain the same nothing has changed for outside probability but inside probability because i am saying this should be the situation so i ( Refer Time : 21:00 ) further express it like with a particular path so i will write in place of beta j p q i will write probability of the rule n j derives n r n s times this beta probability that is beta r p d times beta s d plus one to q but now the d can vary i have already been given that n j gives me n r n s but they can take different possible d s so this will be summation over d and d can vary from p to q minus one ok so these between p and q now if you put that can you see that you can you can actually obtain the same expression that was given in the slide so if you go back to the slide ( Refer Time : 22:00 ) thats what we have been doing here so you see the expression we have three parameters beta i j b so three parameters i j and k that corresponds to p d and q ok and this was the outside probability and this is the inside probability for ah the i j and j plus one k and then you have the rule probability here ok so this expected number of count has been derived in this particular form and same thing you can try with the next formula the expected number of times the rule a deriving the w has been used in my graph and you will obtain this particular expression ok so so what we are seeing here suppose i start with some initial rule probabilities ok ( Refer Time : 23:00 ) so i can use the inside outside probability so all the recursive formulation to compute all the inside outside probabilities for my various rules and stages once i do that i can compute what is the expected number of times each and each individual rule has been used in my corpus as per the current parameters once i have the expected number of times rule has been used i can further estimate my parameters by number of times the rule used divide by number of times any particular rule starting with that non terminal has been used and that will give me the new parameters again i will compute inside outside probabilities expected count the parameters and this i will continue until this converges and yeah so so computing inside outside probabilities is as we discussed earlier by this inductive manner so ( Refer Time : 24:00 ) so what we discussed in this module was that what is parsing in terms of a constituency structure and how do you use the formulation of context free grammar to do parsing how do we incorporate the rule probabilities there how do we learn the rule probabilities using this interesting concept of inside outside probabilities ok so i hope the by the example that we did in the class you will be able to ah understand how it is actual exactly works in practice so in the next week so we will starting with this different notion of parsing so right now we have done a constituency parsing so we will see there is a different notion of parsing called dependency parsing so what is the formulation that that dependency parsing follows how it is different from this constituency parsing and what are different methods we can used for that that will be a topic for the next week thank you ( Refer Time : 25:00 )