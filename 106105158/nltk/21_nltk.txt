so welcome back for the final lecture of this week so ah we are talking about the problem of part of speech tagging and the methods that we are discussing our generic methods for any sequence labeling task so input is a sequence like here sequence of the words and output is again a sequence were for each word i going to predict what is a corresponding part of speech tags so we talked about hidden markov models and also maxent model so in general in maxent model we talked about how do you we use that simple classifier for a sequence labeling task so we will call that the maximum entropy markov model ok and the formation was very easy that is if predict the tag for each word and then multiply the probabilities for the whole sequence there was one ah problem though that because we we need the sequence of the we need the tag of the previous word in certain features to assign the tag at this this particular word we will need to use beam search algorithm and we also discuss what is the beam search algorithm so what i will do in this lecture i will take a take an example there we we will see how to use beam search algorithm and then i briefly discuss what a condition random fields and how are the different from maximum entropy models so condition random fields again is a very vast topic we will not cover fully we will only give you the hint that once you know maxcent or m e m a models what are condition random fields how are they different from those so starting with a practice question so here so so we are having the same sentence the light book and you are given that ah for all the three words the light and book the top put x are determine a noun verb adjective and verb noun ok now you want to use your m e m a model so for the given tag or given word w i the you use particular context what is a context here the previous word next word and the tag given to the previous word this is the context so that means all your features we will be defined over this context so here is a example we have showing some sample like you have given a different features and now we have to use the beam search algorithm to find out what will be the appropriate tag sequence for the sentence so ah so what are the features giving so features are simple like the previous tag the tag given to the previous word is determiner and current tag is adjective previous tag is noun current tag is verb previous tag is adjective current tag is noun previous word is the current tag is adjective so on ok now then you also have feature like the next word is light current tag is determiner previous word is null and current tag is noun what it means is that this the ith word will be starting a sentence thats why the previous word will be null so now you are given this features and for simplicity you are should given that each feature has a uniform weight of one now your task is to use beam search algorithm with the beam size of two what do you mean by a beam size of two at any given point you will keep only the top two highest ah probability tag sequence and everything else you will forget so at any point you will know what are the top two tag sequences till this point and overall you have to find the highest probability tag sequence for the sentence that is the light book so let we see how do we solve this so we are having three ah words in the sentence the light and book ok the word the has two text two possible text it can be either a determiner or a noun the word light can be a verb or an adjective and the word book can be a verb or a noun ok so now how do we start you to find out probability tag i given w i and so del y let me write the context x i or h i give different notations for that so here context is w i minus one w i w i plus one and t is tag ok and what is the formulation of this tag i given ah the current word ah current here it will be exponent summation lambda i f i ok and feature we know is a function of input and the tag divide by z and z is nothing but a normalization constant so that all the text probability let of two one so let us try to do that when the tag is determiner word is the ok so what so let we just write down exponent of summation lambda i f i and lambda i is lambda h is one here in this problem so it will be simply exponent summation over f i so what features are one and what features are zero so for this word so everywhere where we need the previous tag or the previous word should be zero because this is the start of the sentence so i do not have any previous word tag or any previous word so all these features value will become zero now what is the feature that will become one what should be this one this needs the next word i is light and the current tag is determiner is that ah will that you one so if you see here current tag is determiner on the next word is light so it could be one that is for feature f i so so it is one so i will say it is exponent or let we write e to the power one this lambda is one divided by z let we find out that later let if find out the the value for noun noun so again similar to this one all of features from f one to f six will become zero because you do not know what is a previous word or previous tag this feature could have become one but the current tag is not determiner but noun so this will also be zero this feature previous word is null yes that is true is a start of the sentence and the current tag is noun this will also become one ok so this the only features that become the one for noun so this is e to the power one and what is the normalization this plus this ok so two times e to the power divided by two times e to the power one so both will become point five ok so fine so have this point all the two tags have we [ bo / both ] both the tags has probability point five and any of because i am using a beam size of two i will have to keep both these tags so now i keeping both this tags with probability point five point five now let us go to the next word light now again so i have to use probability of verb given this history now the this stage previous word current word next word and the previous tag so now when i going about talking about verb if i have to compute this features i need to know what are the what is the previous tag ok here it can be a either determiner or noun so that's why i need talk in terms of the sequences so here i have one sequence determiner noun determiner verb and second sequence with noun verb so we have to take both the sequences separately in compute the probability similarly i will have to do the same for adjective ok so let us try do that for one sequence so noun verb sequence ok so what will be the probability of ah tag i given the word so this we can write here so let me write only this summation lambda i f i part so this will be summation lambda i f i so let us go to the features here so noun verb so let us go to the features first features previous tag is determiner no previous tag is noun here this is zero previous tag is noun yes and the current tag is verb this is ok this could be one so f two is one f three zero because previous tag is not a adjective f four previous tag is the no yes previous word is the current tag is adjective no it's verb then this will be also not correct because the current tag is not a adjective but verb previous word is light noun next word is light noun previous word is null noun so only f two is one so this will be for this sequence it will be a to the power one divided by the normalization that now what will this normalization depend on this will depend on from all everywhere where this context is taken what are the ah probabilities so i have to compute the probability for this one also to find out this z ok so i know this probability i know this function this function and i will normalize them to at two one so what will be for the function for noun and adjective that is try that from the features again previous tag is adject determiner gone previous tag is noun but current tag is verb noun previous tag is adjective noun previous word is the yes and tag is which adjective yes so this will become one f four will become become one previous word is the yes next word is book yes current tag is adjective this will also become one all fill will be zero so now this will become e to the power two divide by z and z i can write as e plus e square same here e plus e square ok so now i know the probability of getting verb at this position given the previous tag is noun and adjective at this position given the previous tag is noun but what is the probability of this whole sequence it will be multiplied by the probability of getting noun that is half similarly here half ok so this is the probability of selecting this sequence similarly i will compute the probability of selecting this and this normalized them multiplied by point five so now i will get the probability for four sequences so that is noun verb determiner verb determiner adjective and noun adjective i have the probability of sequence of this system and then i will select only top two from there ok so suppose the top two could be say noun verb and determiner adjective so what will happen now for the next step i will consider only say determiner adjective and noun verb and other know also their probabilities then i will take each individual as a history and see ok determiner adjective then noun determiner adjective then verb normalizer probability see will noun verb noun noun verb verb normalizer probability accordingly multiply this so again you will have four sequences here you will have probability for all four sequences and take the one that is having the highest probability and that will be your final ah tag sequence in this example ok so i hope the idea is clear i am not solving this fully but i am i will encourage you that you you do it on your own and see that you can you can find out what is the appropriate sequence using the m m a model so this was how do you use beam search algorithm for m m a model now i will just talk briefly with what is the problem with this ah what is the single problem with this ah maximum entropy model then so let we have to think about ah condition random fields so in maximum entropy model we do a per state normalization that is all the mass that arrives at a state must be distributed among the possible successor states and this is giving rise to a label bias problem ok so let's see what is the intuition so what do i mean by this so let we ah take the same example so first let me take the same example to explain what do i mean by normalization at each state so take this one ok so you are computing noun verb and noun adjective ok and you had the features like e square and e one but you where normalizing them by so even normalizing then such that these two add up to one same thing will you will do with determiner you will make sure that these two add up to one so your normalizing at each state ok so why will the that we a problem so let we just taken hypothetical example suppose in your ah maximum entropy model you were having a tag t one and tag t two at any given point so next point so this one t one you can go to go two different tags t one prime and t one double prime and from t two again you can go to t two prime t two double prime then may be same they may not be same now how do you compute this probability it will be e to the power summation lambda i f i divide by z z is something but the addition of these two same here now this is what is the importance for how how many features you were having and so on suppose for a particular ah choice of these tags it happens that in one of the branch they are having this value has zero point zero zero two ok and this value has zero point ah zero zero zero one ok or let us say ah even much smaller value and this branch is having value of zero point zero three zero point zero four so so what will happen now this not normalized values ok so it tells that this summation lambda f i is getting a higher score in these two cases and lower score in these two cases but because you are doing for it should normalization you will divided by zero point zero zero two plus zero point zero zero zero one and that will be closed to say very closed to one zero point nine eight or something ok on the other hand this will be closed to point four five or point five five so what is happening here even though this probability was low when i normalized this became very ah this became very high that is one particular problem on the other hand suppose that from t one there was no possibility of going to t one double prime so there was only one tag possible so you will in independent of the context it will always get a probability of one because you have to normalize at each state that is if from t one i can only go to one tag t one prime and everything else has a probability of zero because if normalization this will become one so we multiply this probability t one independent of the context and this is called as the label bias problem and this comes because you are normalizing at each step that is one problem with the ah maximum entropy markov models so so let us see how we avoid this problem in condition random fields so i hope this problem is clear that you are doing normalization at easy step at easy state and that is giving some bias towards ah those states that are having few transitions then the having more number of transitions ok so ok so let me just telling one thing so suppose i have two different tags from t one you have two possible transitions from t two you have five possible transitions what will happen these will get a but there will be a bias towards choosing this state because this will be normalized and one of this will gave a get a higher value and this may not happen here because there are five possible transitions so this get's a bias and that is not ah what is idle so how do we avoid this problem and conditional random fields so conditional random fields are under acted graphical models and while there are many variations of conditional random fields so there is a generic structure we will look at the linear generic structure so here is shows we has here we are seen the linear generic structure of conditional random fields so again like in the previous case you are having a sequence x one two x n and these are the tags y one to y n assign to these tag now how they are so in what way they are very similar to maximum entropy model they are similar in the sense that we they use the same for the feature functions so what you are seeing here for the ith point the feature function so suppose i is equal to three would be a function over the previous tag y two current tag y three the whole the input you can take any any number of words before and after x three and this is ith index ok so feature functions are again function of the input current tag and previous tag this is in the linear generic structure so conditional random fields are like ah ah factor graphs so what happens the probability of each ah node will depending on only its its neighbor ok so and you can use the same sort of features so like ah so this is what we had discuss in maximum entropy model also so i have a feature that is one if previous tag is i n current tag is n n p and the current word is september and zero otherwise so we will see there are very similar sort functions that we are using in maxent so so they are same in as maxent in in that sense but how they are different so a difference comes in how the normalization is done in maxent model or maximum entropy markov model we are doing normalization for each state ok so that is at easy state if i have multiple transitions i will make sure that the probability for them at adds up two one this does not happen in conditional random fields so we will compute ah the features ah expectations of feature values for each possible transition whole sequence and then i will do normalization so that we can see from the probability function here so y is the whole y is a whole sequence y one to y n given a current the current input sequence x one to x n and lambda is the the feature ah we its that you will learn and this is one by z x say see a single nomination parameter exponent summation over i is equal to one two one summation j lambda j f j ok so ah let us try to understand quickly what this ah function means so here having one up on z x exponent summation i is equal to one to n summation over j lambda j f j and say f j is a jth feature and this is probability y given x lambda lambda are the feature x let us try to understand this what do you mean by y y is a sequence y one to y n and x is the input sequence and there are many such sequences possible so this is a probability for a given sequence and this z is a normalization that is done over all so that is a summation over all the sequences so we can write it as so so this will be summation over all the sequences i will i have this value for only sequence y current sequence y i will add it over all the sequence that will give me z so sum mission over all y possible y e x p and whatever was in set so now how do we get this equation so remember this equation summation j lambda j f j that is for a particular tag ah y is a given at the ah sorry a particular tag given at the ith position ok so i have exponent summation j lambda j f j that is probability of a tag y i given x i x i can be all history at the given point divided by z was there but forget about the now here we are computing probability for the whole sequence y i i is equal to one to n so so this will be multiplication so multiply i is equal to one to n so multiply i is equal to one to n now if i multiplying multiple exponent this is like so so for example e to the power x times e to the power y becomes e to the power x plus y so that is multiplication of all the exponent is nothing but like summation of what is inside exponent summation i is equal to one to n summation j lambda j f j ok and that is the function a and z x is normalization over all such ah transitions all such sorry all such ah sequences so you are not doing normalization here so what happens in maxent you are doing normalization here so for for a given i you make sure that everything adds up to one and that is not being done here you are you are not making sure that at each i all the tags will the probability for the text will add up to one now you are doing a normalization only in the end i know the probability or something that is proportion to probability for each tag sequence and then i will normalize everything by this z x and that's why this this avoids the label bias problem so this is the function that is using conditional random fields so what we can see that so ah conditional random fields have the advantage of maximum entropy markov model they use a same sort of features the kind of model is very very similar to maximum entropy markov model but they avoid the label bias problem so c r fs are globally normalized whereas m m ms are locally normalized so that we had discussed and they are very widely used and applied for many many sequence labeling task and so so they were very closed to the state of the art models for many of these sequence labeling task so so what whatever sequence labeling task that comes your mind so starting from part of speech tagging to name and recognition so there you can apply a conditional random field model and lot of libraries are also [ avai / available ] available so one particular appeal that is very popular and then there are c r a plus plus and and many other libraries that you can use what is important is that you understand what is the sort of features that that you need to use ok and then the model will will help you to train you are own c r f so this ends our discussion on ah part of speech tagging we did discuss a lot about what will be the models for sequence tagging so now so in the from the next week we will start discussion on ah on syntax that how do we find out what are the word what are the word arrangement in a sentence and how do we group them in various sort of ah phrases so so i will see you next week thank you