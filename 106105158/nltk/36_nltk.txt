so hello everyone welcome to the fifth lecture of this week so we are talking about word embeddings so in the last lecture we discussed the what is the different ideas behind using word embeddings what what can be some interpretation given to the different dimensions here and what are different tasks where you can use these word embeddings but we do not go through the learning part how do we actually obtained these word embeddings for different words so today in this lecture we will try to discuss in detail how do we obtain these embeddings so so in the last lecture we discussed that there are two different models for word embeddings so for their main to main models one is continuous back away words model and another is skip gram model what is the idea there in continuous back of words model using the map neighbor neighboring words i am trying to predict the center word in a skip gram model using the center word i am trying to break the neighboring words so let us see what we do will c b o w model so here let us say we have a prose like this the recently introduced continuous skip gram model is an efficient method for learning high quality distributed vector representation and so on so that i data that i have so much and i am i am trying to learn representation for various words embeddings so at a time i will go through one particular window here so i will focus on a word so let us say i am focusing on one ah word like learning ok so what i will do i will take a window around that word now window size can vary and this can be a hyper parameter that you will choose so you can chose a i will take forward before there and forward after that so using these eight words around this word learning i will try to predict this word learning so how do we do that so yes so imagine is sliding window over the text that includes the central word and with four words that precede and the four words that follow it like here learning is my focus word and there are four words before it and for words after that and you can also call that as the context words and usually the context words you are trying to break the focus word so so this is some sort of network representation and how this learning will take place so you are seeing here in the input you have a so input you have various column vectors so what do i mean y this so here so what you are doing here you are putting in one hot encoding as the input so i have this column vector of size v so the column of size v now so v is the size of my vocabulary i have that many words my vocabulary so i may not use this only v ok v is the size of my vocabulary now in your context your heading having certain word that will be there in the vocabulary so let us say this is the index of that word so this input would be everything else a zero and this will be one ok so one hot encoding of that word so column vector so like that in you are context suppose you have here eight words so we will feed all these eight one hot forms so here it might be one here zero here it might be one somewhere and everything else zero ok that is your input that you are feeding now using this input then you are having a hidden layer this is n dimension ok i remember this n will be important we will see what is this n so now the size v now i want to map these two this n dimensions so i will have a weight matrix let us set up do one of dimension so this is ah v cross one sorry so this you can take it as one cross v so you will have it h v cross n ok so that the final representation is n dimensional now so you will have the same weight matrix at each input so you will get and then you can take the average of all these now from these n dimension you again go to your v dimension this is you are output layer so what will be the second weight here this will be n cross v ok see i starting with a v dimension going to n dimension and hidden and then going to the v dimension in the output now so what you are doing here so before going into what are the connections a network you are putting some input there are your context words so suppose you are having eight different one hot vectors and whatever happens here finally you are trying to create your ah center word this should be your center word ok so what will happen here you will have various weights w one w n some numbers you will w v some v different numbers you will get and you want to ensure that the actual center word suppose this is my center word this is the highest probability and if it is not having the highest probability you will try to modify the weights in your network so that it it it gets a higher probability then what is getting right now so now let us see what do we mean by all these connections ok so what would happen from here so let us go back to the slides so each word is encoded in one hot form that is here we have a single hidden layer and an output layer so you are having two different weight matrixes one is v cross n another n cross v ok so so from input you go to hidden from hidden you go to output output you are trying to predict the ah the center word so what is my training object is here so you see i am getting some word some numbers for different all my v words at the output layer and this you can think of as the probability value and you might have a question that how do i convert these numbers to probability we will see that so assume that you having v different probability values now you want to maximize the conditional probability of observing the actual output word given the input context word with regard to weights so you say that i want to maximize the probability that i will obtain the actual what that i saw in my input and not any other word so whatever probability i am getting that becomes my objective function so i want to maximize this condition probability of output word given all my input words and this probability will be express in terms of all these weights that i am having in my network so in our example what would happen i am giving this input all these eight words an efficient method for high quality distributed vector and i want to maximize the probability of getting learning as the output this my centre word now so what happens from input to the hidden layer so input we are saying you are feeding one hot vectors so if there are eight different input words i am having eight different one hot vectors now what do i mean by multi bank this one hot vector with my first weight matrix the dimension we cross n so you can think of it like that so you put using an input so this is one word and having a corresponding ah element is one that that is my input word and you are multiplying it with v cross n weight matrix so this operation is nothing like this nothing but you are taking the corresponding row of this matrix this matrix has v different rows so these v rows you can think of as if corresponding to v different words in my vocabulary so whenever you are feeding a one hot form of one word you are picking up that row ok and so you are doing it for a different words so you are picking a different rows in my ah initial weight word weight for matrix and then so you are having given c input words so activation function for the hidden layer will be simply summing the corresponding hot rows so i will pick up the hot rows here that correspond to the input words and divide by c so that i have an average representation so now you understand what is going from input to hidden layer you are taking ah c different one hot c different rows from your weight matrix and averaging those thats what goes in your ah hidden layer now what happendes from hidden to output layer so remember we have a ah second weight matrix w two that goes from hidden to output layer and its dimension is n cross v so now a hidden layer dimension each one cross m so if i multiply this one cross a matrix with this n cross v matrix i will get a one cross v matrix ok and that is my output layer so these one cross v matrix even think of as having weights for different words in my vocabulary and i want to maximize weight for my center word ok so from the hidden layer to output layer the second word matrix w two can be used to compute a score for each word in my vocabulary and now so you can obtain the weights how do we convert them to probability values and for that you use soft max what is the idea of soft max so here you will you get the weights w one to w v they can be any real numbers ok now how do you convert that to probability distribution so this is a simple idea that is in soft max so in soft max what you do is you are given w one to w v and you want to convert that to a probability distribution so you will see that i simply multiply all these by so i will put an exponent over these so e to the power w one e to the power w v now they are all positive numbers and then to convert them to probability i will just divided by summation over e w i for all i ok so now this is this will some off to one because this is normalize and these are my probability distribution so i have these weights i use a soft max to convert them to probability values and then i will try to maximize the probability of my actual center word this will become my training objective and based on that i will learn my weights ok so i will i will talk a bit about this learning problem in the when i go to the next model of escape ground but suppose we have some way of learning these weights and finally i have the optimal set of weights w one and w two now where are the word vectors what are your word vectors here i am feeding one hot vector this is not being learned this is the same so where are my word vectors being learned now if you think about it so what i am learning are the weight matter research ok this is off v cross n this is n cross v so i can take a transfer this also will become v cross n so you can think of it as if for each word you are learning a n dimension representation ok so after you learn this matrix these weight one and wait two will correspond to your ah word embeddings ok so w one will be of size so w one will be of size v cross n so you can take any vector i and get a n dimensional representation and the sign you can also think of as your d d dimensional representation ok so for each word you are getting a d dimensional vector similarly for w two you will get similarly if you take a transpose you will get a v cross n representation and in general what you what you do you can take a ah so you are getting two vectors one for from w one one from w two so we can finally combine these two vectors you can either concatenate these vectors or the same word or some these over or taken average and that works fine so this is the addition about what is the kind of network that you use for learning these embeddings ok so these weights are nothing but my embeddings that i am learning ok so this is about continuous back of words model now what will happen skip gram in a skip gram model the network will slightly change so now i will feed only the center word here only one input vector but i will predict multiple contexts words ok so now from input to hidden there are only one so the only one input vector but output there are multiple vector so let us look at the skip gram model skip gram model is the opposite of c b o w model so so you have the center word as the sing single input vector and the target context words are at the output layer this is my output layer and you can now very easily correlate with what we did in c b o w so the the weights correspond to roughly the same idea so now let us formally define so we see how in the using the network we can think about the learning how the learning will take place and how the weight updates will take place but suppose you want to write it mathematically and how do we do that so so yeah this is very analogues to what we find the case of c b o w so here from input to hidden layer i am simply copying the row from the weight matrix w one so i am having only one input so i will copy only one row and that will go to my hidden layer now at the output layer we will have see different distributions and i will try to predict the see different context words using my hidden layer and objective is to maximize the some prediction errors across all the context version my output layer so you want to predict although so i want to maximize the probability for observing my actual context words so let us say we have a window where i am having ah small c words in the left small c words in the right so what can be my training objective it can be i am trying to maximize suppose i take a log probability w t plus a given w t w t is the center word and i am going from minus c less than equal to j that is equal to c window of size c around j and j is not equal to zero and this i can do for all possibility all possible center words and this becomes my training objective ok so we will see that so i am predicting surrounding words in the window of length c of each word my objective function is i want to maximize the log probability of any context word given the current center word so i am trying to maximize this probability probability of w t plus j given w t and sum over all the possible context words and then i can sum it over all the possible words in my input and this becomes my overall objective so j theta and theta of all the parameters all the weight matrix w one and w two that i am trying to learn so now this is my objective function and i want to maximize that ah and by doing that i want to learn my parameter theta and how do we do that so firstly let us see how will be compute these probability probability w t plus j given w t now this will be actually you have already seen that in the case of ah using the network but using which ah some mathematics can be show that so idea is that let us say i am having a probability for p w o given w i w i is my context world and w is the output word that is the i am sorry so i i should say as the center word and this is my output word these are all the context words ok let us say for a given context words how do we write it and i am saying we can write it in this form that is exponent v prime w o transpose v w i divide by sum over w is equal to one to capital w v prime w transpose v w i ok and these are formulation of this probability and let us try to understand that so whatever different is we have written here so one thing you are seeing there is a v and there is a v prime so for each word you have two vectors one is v another is v prime so v is for the so v is use only for the input center word and v prime is use for output so when i am trying to use the input word to pre the output word i will simply take a dot product of these now both both is vector like column vectors so v w i will be of this form and v prime w will also be as a column vector so how do i get a number by multiplying these two i will take a transpose of this ok and multiplied with this and thats what i am doing here v prime w transpose times v w i that gives me single number now this number i want to convert to a probability value so then i am using a softmax over that exponent over this number divide by i will do it for all the words in output ok so hey thats why i have a summation over all the words w is the total number of words in my vocabulary this is nothing about a probability distribution and you can see that summation p w o given w i for all words w o will add to one because because of this normalization factor so this is added to word this is giving me a condition probability of w o given w i now as a simple exercise you can also try to see how this number comes from the network that we talked about ok so so we had a networking interpretation now we have a simple matrix short of interpretation so how they correspond to each other and they use the idea that word one sorry weights one and weights two what are these weights weight one correspond to the input and way to correspond to output use this idea and see that by using the network also are you getting the same form of this probability by putting x softness over the output layer and you will you will see that you are actually getting the same form so now i have this formulation w o given w i this particular formulation and i put it here for all the words in my context and this becomes my j theta this is my objective function now what is my objective i want to learn my v an v prime so i will try to learn my v v and v prime such that this is optimized and for that there is a simple ah way that is you can use getting descent algorithm so you can try to take partial derivative of j theta with respect to each of these parameters and update your weights accordingly so so this is the ah probability condition probability distribution function that we have found and v n v prime are input and output vector representation of the same of the same word w so every word has now two vectors so suppose i have d dimensional vectors so d is my ah so so d is the dimensional of my hidden layers in network terms or for each word what is the representation that we using we using dimension representation so i have the many words so what is the size of my parameter theta so i have ah capital v words for each word i have been input vector and output vector so there are two v vector and each vector of dimension d so i have two d times capital v that many parameter slower this should my whole set theta and what is the ah simple gradient descent formula so this is like you take the derivative with respect to the ah that particular parameter so so theta j new is theta j old minus alpha is my learning date and you take a partial derivative of j theta with respect to the parameter when putting the old values now i i i am supposing that you know the idea ah gradient descent that is used to optimize your minimize a particular function ah so if i want to give this idea very briefly so this is like suppose you have a simple function like this and i want to find out for what parameter theta of what parameter x will suppose this is a function over x and this my y for what value of x this is minimized so what i will do i will start with n e x ok so suppose i am starting with this x zero i will find out the value of function so i i go to the function so why is suppose f x zero now i want to know what is the value of x where there is minimized so what i will do i will take the derivative of the function at this point f prime at x zero and so now if i have to minimize if this direction of my gradient i have to go in the opposite direction of my gradient so my new value should be x naught minus f prime x naught so i will go into opposite direction and i will use some learning rate with what rate you will go in this direction and this is simply the idea of you keep on doing that so you go somewhere again you find our suppose you are going at this point ok so now your gradient will point you to this direction so will try to go in this direction again gradient will point you some direction and finally you will converge so this is a very simple idea of gradient descent but i will suggest that you have a look at it that what is actually the gradient descent so this is simple function but your function can be over multiple parameters so like here i have my theta two d v dimension so i have two d times capital v many parameters my theta is a function my j theta is a function of all these parameters so what i will do i take a partial derivative with respect to each of these parameters and accordingly update those parameters and this is how it will look like so we are not going in in doing the derivation because that wont be in the scope of this course but if you are interested you can try to find out if i derive this form what update values do i get for different what vectors how do i update these vectors and now once i have done that i have two different vectors v and v prime and i can simply some these over and that will give me the final embedding for each word and if you want to understand more about how do we learn these parameters so you might have a look at this paper and this gives the nice tutorial for parameter learning and also if you want to try out an interactive demo so you can try out here now so so what to work is one famous model and the such two variation c v w and skip gram and both are used so in some tasks skip gram has shown to be better in some c b o w has shown to be better and there are many other tricks that i used for learning these parameters that we have not covered i have only tried to give you the intuition so i hope you will at least have an idea that how these word vectors are learned from my data by just predicting neighboring words from the context or context from the neighboring word and what is the embedding so how these weight vectors for my embeddings so now there are some other models so one other popular model is glove model ok and so so what is a basic cognition of glove model when so in this skip gram so i am trying to learn my ah all my embedding from scratch ok so in glove model what they are saying so from a coppers you can kind of count the co occurrence see know with so thats what we did in the distribution semantics we would counting co occurrence now what they are saying so this gives a very good idea about the words so which words are similar to what are the words and suppose you find out what is the co occurrence probability for any towards now try to make word vectors such that when you do a dot product between towards you get close to the actual co occurrence that you are seeing in the corpus and by doing that you will get a low dimensional representation and also it will take care of the words that are coming very sparsely where you do not have enough data in the corpus to predict the co occurrence and this is the simple objective function so that is if you look at the objective function so forget word f f is a few simple function that that make sure that if certain words are certain co occurrence we are very very popular you do not bayes your learning towards those see you just clip those at certain point so everything above that will be converted to this one now what so this is the main idea here of this glove vectors so you are having in word definition for i and j that you want to learn an idea is try to learn them that are that such that they are resembling their whatever you get from the co occurrence probability so p i j is from the co occurrence you can use either p m i condition probability or many other things so find out w i w j such that they are close to this co occurrence probability and then we have these objective function and you optimize this objective function and try to learn your different weights so so you are trying to combine the best of both words using the count based methods and the direct prediction methods and using both of these you are trying to come up with your word vectors and these vectors have also become very very popular because the training is much more faster then then my skip gram model because now you are not going to the each individual word in your and all the windows you have already computer the counts now you are only looking at the pairs word pairs and one maximizing that and so even with the small corpus and small vectors they have shown very good performance so so one good thing is that suppose you want to use this vector finding of your task so either word vectors or glove vectors are freely available on different websites so what do we already told the website in the previous lecture for glove you can go to this stanford website and there you can download code as well as the vectors and these vectors you can use for many of your tasks where you are trying to find out whether two words are similar or you are trying to find out some analogy task a is to b then c is to what and many other application they have been used in so i think this is what i had to say for this tropical distribution semantics this is a very ah very well growing research field and lot of new thinks happening so i hope whatever we have discussed will help you to to also understand the papers in this field if you want to going in further depth and also you can try out these ideas for many of your applications so in the next week what we will do so we will start with a separate notion of semantics that is leska semantics so how to use lexicon to find semantics between words thank you