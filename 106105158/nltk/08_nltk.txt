ok so welcome back for the third lecture of this week so in the last lecture we talked about what are the various variations of a edit distance that you might be using in general ok so today we will now talk about very extended algorithm for doing a spelling correction so in general when i talk about spelling correction i might be talking about isolated word reduction or correction that is a word without the context even there it might be a word that is in my vocabulary or not in my vocabulary ok so we will see examples for that or i might talk about doing correction in the context there again a word might be in the vocabulary or out of the vocabulary and and we will see how this this simple approach of noisy channel model can be applied for doing a spelling correction ok now what is my noisy channel model so if you if you think of the the model name what is happening we having a observation x that is a word that is misspelled and you want to correct it that is your observation but what what is the idea of this model so whenever have a miss spelling you wanted to write a correct word ok and you went through a channel that can be your keyboard or something else and that's how you mod your word got miss spelled so that is what you are assuming you have a correct word w ok this is your correct word and you have a incorrect word x that is your observation yes now you wanted to type this correct word w but you went through a channel and this we are saying is a noisy channel and you are making some mistake while going through this channel and the word that you are observing is x but not w now your observation in x now given this observation x how do i find out what is my mostly slightly candidate for w what is my w ok so in general how can i try to up to this problem i am seeing a word x that i know it is miss spelt now how do i know it is miss spelt one two way of doing that is i maintained a dictionary that that is my set of all the words my vocabulary and if the word x does not match with any of this word i say this might be in misspelled ok now once i know x is a misspelled word that means i have to correct it now what are i have to first started with ah what are all the possible candidates for correction so suppose possible there could be i find out all the words within the added distance of something ok now among those which one is the most slightly candidate ok so so in terms of noisy channel model what is the w from which x has been written written so miss spelt and and we wrote x so in terms of noisy channel model i want to find out w hat that gives me the maximum probability p w given x among all the possible words in my vocabulary or it can be in my set of candidates what is the what that gives me the maximum value for p w given x now how do i compute this probability p w given x you see as for the noisy channel model i first started with the word w and then i went through the channel and i ended up with the word x so i can only find out using my channel what is the probability of x given w ok i will have to somehow used to find out probability of w given x yes so what is a particular theorem in probability that you can use for is so remember bayes theorem so i i want to find out probably w given x but i already can find out from my channel probability of x given w so how do i write p w x in this term so i say this will be p x given w p w divide by p x yes so i will i can replace i by argmax of this now what is your variable here the variable is w you can have multiple different words but your x remains the same for each individual word so what in fact you are trying to do for each word your completing this value and then you are seeing that is the maximum value which word gives you the maximum value now among the three different probabilities that you are using for this formula which one you might remove for this computation is it the probability p x remains the consent across all the words so it will not matter to my decision of which one is having the highest probability because this was the simply depend on this particular multiplication so i can as well remove p x and keep my argument ok and that's how we actually use ok so i write it like that and i remove the probability p x and that's how that's so how i will find out which one is the correct word w for x which give me the maximum probability for probability x given w times probability w so now i will we will see how do we compute these probabilities individually what are the various which we can compute this probabilities ok so so we are taking about non word spelling errors so again what do i mean by non word spelling errors a word that is not in the dictionary or my vocabulary and i know ok this might by misspelled word suppose i have taken the word here actress ok so actress is not in the dictionary it can be actress across or whatever but we do not know so this is the observation that we made now using noisy channel model i want to find out what should have been the correct word ok so now the first thing i will needed to do i will need to find out what are the candidate words that might be correct ok so how do i find the candidate words i will try to find out words that are having similar spellings that are having small edit distances ok or i might try to use the words that are having similar pronunciation so both of these are possible so then i make some sort of candidate set so suppose i have ten different words that might have given to actress so these are my [ var / various ] various candidates for w and i need to use the noisy channel model to find out which one is the most slightly candidate so and suppose i am using might d l at a distance ok this is small variation of my levenshtein distance remember levenshtein distance what were the operations we at insertion deletion and substitution in this particular ah edit distance we also have the transposition ok so insertion deletion substitution and transposition of two adjacent correct words so we discussed about how to how to modify our dynamic problem algorithm to take into count of this transposition in the in the last lecture so now so given to two strings i i can always find out what is the ah word so given a string i can find out what are the other words that come within additions of one or two ok so suppose i start with actress that is my observation in correct word i try to find out that are the algo candidates are come with a additions of one ok so here is i have here is the word acress now i find the words like actress cress caress access across acres and acres occurs twice in this list why and we will see that let us try to understand first candidate word actress so that does we written here so we have written so this is my correct word this is my incorrect word so what change did i make for going from correct word to incorrect word so i went i had a letter t in my correct word but it repla i replace it with null so this is an deletion i delete it t from my correct word so that's why this is deletion ok similarly how would i go from cress to actress i have to have insert a so this is the again insertion yes similarly how i go from caress to acress i am doing a transposition so c a change to a c transposition acress access to acress c got change into r across to acress o got change to e now so why there were two acres you see acres to [ acr / acress ] acress i can go by doing one insertion but insertion can happen at two points it can happen either here or it can happen here and these two will have different probabilities ok remember with the probabilities of substitution will depend on what is the pervious word that what is the pervious character in this word ok so they will different probabilities that's why they are two different insertions ok now so that means whenever i have a misspelled word i have to first generate all the possible candidates ok so so there we try to use some sort of a statistics how many different candidates should i be using so so the general statistics is that eighty percent of the errors are within distance of one and nearly all the errors are within the additions of two so that means i can mostly try to use the candidates within a additions of one or two ok may be one will do for most of the cases so that's what we did in the pervious example starting from acress we found out all the candidates with in a additions of one and while generating the candidates i might also allow for deletion of space or hyphen so if i a word like this idea without a space i might allow this idea with this space and similarly for hyphens and not with a hyphen so now now when we are trying to compute the probabilities we had two different terms probability of x given w and probability of w so you will first see how do we compute probability of x given w now suppose we are taking the simple case where there are ah only one this only one edit operation from the input string or the correct word to the incorrect word so what do i mean by probability of x given w i would mean what is the probability of that edit operation being done so remember we had four operations insertion deletion substitution and transposition so we need to define probabilities for each of these so here that's what we are defining we are defining deletion over x y what do i mean by that whenever in the input string a input word i had the sequence x y this is my correct and it got typed as x this is my incorrect so x y got typed as x so this is i have deleted y so this will depend on the pervious character x in the correction similarly what about insertion i had x in the correct and i inserted y in the incorrect so this is the case of insertion substitution x got typed as y so i had x in my correction i got typed as y in my in correct decision what is transposition again x y here incorrect got typed as y x in my incorrect so these are the four different possibilities if i am having only one edit operation between the two strings so now the question is how do i find out probability of a particular ah deletion or insertion operation so let us take one of these and see how do how we will actually find out find out these probabilities so let us take the simple case of deletion so how do i found probability that in a in a string a charact a set of characters x y a character bigram i can also say it is a hecta bigram got converted to only x so how i will feel find out this probability so we will have to see in general in corpus how likely is this error so now what kind of data do i need to count to find this probability so i need some sort of data were you just have written some texts and they made some mistakes not knowingly ok so they made some mistakes that if general people do and somebody has then seen this corpus and tried to correct it ok so suppose i have a corpus that is ah corpus is a real corpus and that contains various errors and then i also have a corrected corpus now how will i use these two different corpora to find out the probability of deletion x y i will say how many times do i seen my corrected corpus a word containing the bigram x y for which the corresponding word in my actual real corpus had only x that means actually the word should have been have having x y but it was remains x so how will compute the probability i will see in my real corpus how many times i have this corrected bigram x y and out of those how many times y was deleted in my real corpus and that will give me the probability that how after x what is the probability that y is deleted so whenever how many times x y occurred together out those how many cases y was deleted as similarly i will find out find it out for all cases of insertion substitution and transposition so let us see the actual matrices ok so you are seeing again insertion deletion are conditioned on the pervious character ok so that's how i find out the probability of x given w x is my incorrect word and w is my possible candidate for correction so so remember how we found out deletion we said how many times this correct a bigram w i minus one and w i occur in my corrected corpus and among those how many times w i was deleted after w i minus one ok similarly here insertion how many times the word w i minus one occurs in my corpus and among those how many times i inserted x i in my real corpus were people made mistake how many times they actually inserted x i after w i ok w i minus one similar substitution how many times w i occurs in my corpus among those how many times it was substituted with x i same thing you will do for transposition how many times these two characters occurred together among those what fraction of times they were transposed so that means we need two corpus one we we have made mistakes another the same corpus that has been corrected ok and then you can compute all these probabilities now if we go back to the pervious example of finding various candidates for actress so we had seven different candidates here now we already saw what is the corrected error word and and we are using the the model we saw saw in the previous slide to find out probability of x given word ok this one so now first we are writing what is my x given w so whenever this first my deletion from actress t got delete and we got actress so how did you find the probability of deletion we said what is the probability that c t among all the times that c t occurs what is the probability that t gets deleted so out of ct i see only c in my corpus and this probability i find my corpus to be point zero zero zero one one seven ok similarly insertion that means a got inserted in the beginning of the string transposition a c in place of c a and so on in so here we have you see there are two different ways you can do insertion after e or after s that's why you have two different probability that is also ok so so again from the corpus you will find all these probabilities values probability of x given my word for different words what is probability x given now what is the other probability of the compute probability of w itself so what is the intuition as such you will try to give try to favor a word that is more likely to occur in the corpus then a word that is not so likely in the corpus ok so suppose i use a corpus and i also find out the probability of the word ok so now i have found out probability of x given word and the probability of the word both of this so remember my noisy channel model i have to multiply these probabilities and find out which of the candidates gives me the highest probability value so suppose i do the same so in this slide you will see so i am multiplying these two probabilities and we are putting ten to the power nine just so that we are able to see these numbers ok they were be a point zero zero zero to the point that we will not able to compare so the first probability is two point seven times ten to the over nine minus nine sorry so what are the two candidates that that have the highest probability you see here i have actress and i have across so these are two words that i having the highest probability and in general you might write across has the correct candidate ok so now remember we were saying we can do it without with the context or with the context so without using the context this is the best we can do and suppose they come up to be really close two point seven two point seven one without using the context you cannot find out which is the which is a better candidate than the other but supposed you are allowed to use the contexts so can you do better ok so let us see for the same spelling correction if i have the context what will i do so suppose my sentence says versatile acress whose ok and i have to correct it and remember what were two candidates the two candidates are actress and across so the two sentences are versatile across actress whose versatile across whose now which one is more likely ok now how do i find out how do i use the fact that probably versatile actress or actress whose is more common than versatile across and across whose so on so for that what we can do we take some corpus so this is some corpus of contemporary american english and we use some smoothing so do not worry about what i mean by smoothing we will cover that in detail in language modeling so suppose i do some smoothing so idea is that if were we try to find out the probabilities of a word coming after another word that's what we are doing and from there i find out these probabilities that the that the word actress occurs after versatile is point zero zero zero zero two one and the probability that the word across occurs after versatile is the same and that makes sense also versatile across many different fields and versatile actress they might have the same sort of probability of occurrence in the corpus now what about the other bigram what about the other two word combination that is actress whose and across whose so we see in the corpus we find actress whose has a much higher probability than across whose ok and if you have this information we can use that to refine the probabilities for that if suppose i simply multiply the probabilities with my earlier corpus so what will i get so i will say probability of versatile actress whose is the initial the two probabilities two hundred ten times ten to the minus ten and probability of versatile across whose will be one times ten to the minus ten so this gives me that versatile [ ac / actress ] actress whose is much more lightly than versatile actress whose as per my corpus if i am using the context and this can further help me to find out what is the correct ah correct word that should have been there in place of actress ok so now the next concept is the real word spelling errors so so what they mean by that there are there might be errors where the [ mis / misspelled ] misspelled word is actually in my dictionary ok so like you see the sentence here the study was conducted mainly be john black and we know the correct word should have been by but the user ended up typing be [ no / now ] now just think for a moment can i solve this problem without using the context so without using the context how can i even say that it should be by and not be ok i might use the individual probability of a word occurrence but that is not a good method to say that so in that case i might find errors everywhere in my corpus so i should be try try to using my context in somewhere ok same in the next sentence the design in construction of of the system and i know this should be and but the user ended up typing an now these are real words in the vocabulary and now i want to correct the spellings one thing is sure i should be using the context now how do i actually use the context for trying to correct these errors ok and this is again some statistics that twenty five to forty percent of the spelling errors that this here are real words ok so now suppose i i have a sentence x that contains words w one to w one now we do not know which of the which of these words is in error ok because all of these are real words in my dictionary so how do i [ e / actually ] actually start doing the spell correction so what i will do for each of the individual word i will try to find out what are the [ bes / best ] best possible candidates that might be the correct words ok and there i should not forget to include this world itself because this might be a correct word also so here what we are doing for the word w one we are saying ok candidates are w one or some word w one prime w one double prime w one triple prime and so one they are possible candidates for correction w two again that's what w two and other candidates same way w three now what is my problem now i can have any sequence from the candidates of w one i can choose any one w two i can choose any one and so on and among all these possible sequences i have to find out which one is giving me the highest probability in somethings ok so i want to find the sequence w that maximizes the probability of w given x ok that is whether this would be a good sequence or this would be a [ goo / good ] good sequence or this would be a good sequence as for all which of this sequence will be the best in this case so again you have to find out w that maximizes probability of w given x so again can we try to use the noisy channel model so we will see ok so now we are taking a real example here two of w thew and probably it should have been two of the and we are not seeing the other words right so as my the model what will i do i will take the word two i will put this word along with all the other possible candidates so i have put in put t w o t a o t u all are having a smaller distance similarly with off i am putting o n o double f with the thew i put thew threw thaw the and so on and we know the correct sequences two off the fine but in general suppose i had to find out the probability for each sequence this might become exponential so you can do the simple math suppose each word on an average has four possible candidates including itself and there are five different words in my sentence so how many different sequences i will have four to the power five ok and if the number of words increase in my sentence this will keep on increasing so now to avoid this problem we will try to use some sort of we make some assumptions that it is also simplification to the model what is that we say ok [ ll / let ] let us assume there will be a at most one [ err / error ] error in the sentence ok so how does it help it helps in that whenever i am choosing the the candidates i will only choose a a different word for one of the one of the words so so what do i mean by that suppose i had my sentence w one w two w three and this has some other possible things like w two prime w two double prime this has w one double prime w one double prime and this has w three prime w three double prime so in the previous model we would have taken all the three cube that is twenty seven different candidates in the previous one so with this assumption what we will do for w one if i am choosing a candidate of w one prime i will assume w two two and w three were correct ok so this is the assumption there might be at most one error per sentence so how many candidates will be there i will have two possibilities here two possibilities here and two possibilities here ok so there would be at most six plus one if you are taking the one there the sentence might have been correct say w one w two w three as it is ok so so the candidates are w one prime w two w three only one error per sentence w one double prime w two w three and so on so this hugely reduces the number of candidates for which i have to check and this is quite true also and that is why you do not make more than one error per sentence in general in general ok so now once we have this assumption we want to find this [ sec / sequence ] sequence w that maximizes the probability of w given x so i can again try to use my noisy channel model how find out the sequence w hat that maximizes the probability of p w given x x is my observed sequence and w is one of my candidate sequences and we know how to find out the candidate sequences so that is for each of these sequence is w i have to find out the probability w given x now as for the noisy channel model how will i write it down so i will again use the bayes theorem here because i am starting from w and going to x so i can only find out the probability of x given w so this is what i will do ok now how do i write probability x given w x is a sequence w is a sequence so suppose they have the same number of words i can make a simplifying assumption that p x given w is simply multiplication of probability of individual word given that in the sequence ok so suppose my word is this is my w and this is my x so probability x given w i will write x probability of w one prime given w one probability w two given w two probability w three given w three ok so now there are two kind of probabilities here one is when the two words are not the same probability of doing an error yes one they at the same now how do i find out the individual probabilities this one we have already see in the previous case i will find out what are the [ audit / edit ] edit operation and what is the corresponding probability if it is deletion insertion and so on so this is same as the previous case same as in previous but how do i find out the probability of w two given w two should it be one now it will depend on what is the kind of errors that you are seeing in your corpus if you are seeing that they are on an average one error per ten words you will say this probability is point nine thus the this is what is correct these are probability of that the same the word is correct if there are one error in hundred words you will say that this probability is point nine nine and that's how you will fix this probability ok so p x given w same as non word spelling correction but we also need this probability w given w and that is the probability that you have correctly typed a word and that you can find by the kind of the the amount of errors you are seeing in your source if there is one error per ten words you will say this probability is point nine if there is one error per hundred words you will say this probability is point nine nine that's how you will set all these probabilities and you will choose the candidate x that gives you the maximum probability ok candidate w sorry now the other part is finding out probability w ok by w i mean the the sequence w one prime w two w three how do i define the probability of the sequence w ok for that we will use a technique called language modeling so that will give me the probability of n is the sequence of words and we will use any of this unigram model bigram model or any other model for for finding out this probability of so so this here will be the topic for the next lecture so this in this lecture we discussed what are non word errors re word errors how do we use noisy channel model to correct those but there we also need something like probability of the sentence as such probability of the sequence of words w how do we obtain that and that's why we will go to the idea of language modeling in the next lecture ok