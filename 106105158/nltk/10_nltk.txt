okay so welcome that for the final lecture for second week so in the last lecture we were discussing about the basic of language modeling and there we we just hinted about the one problem about the zeros in language models so today in this lecture we will discuss how not only how to handle this zeros in language modeling's but also how do you go about evaluating your language model that you have learned from some data [ vocalized noise ] ok so these are the two main topics for this lecture ok so so we start with the evaluation of language models so so what is the basic criteria for evaluating a language model so one symbol indication could be a language model is better if it is assigning a high probability to the actual sentences the real sentences and a low probability sentences that are not so grammatical that can be one criteria for evaluating language models but can but this has to be done manually so you will have to check what is a probability that your model is assigning to various sentences that are real most of the sentences that are not real that are not grammatical but can we do that in an automatic manner so for that we might we may also use some sort of training and test data so so what is the idea behind using this training and test data so i have some corpus and i learn my language model on some part of that okay so this is my training set now once i have learned that language model from my training set i will test whether it is providing a good high probability for my test data and i will have some evaluation evaluation metric for finding out how good this model is doing on my test data and by this method i can i can even compare across different language models so if i have learned three different language models for example i can find out which one does better on my test data and that will be the best model ok so in general what are the different ways in which language models can be evaluated so as such there are two different criteria for evaluation one is called extrinsic another one is called intrinsic ok we will discuss what are these individually [ vocalized noise ] so what do i mean by extrinsic evaluation of my language models [ vocalized noise ] so suppose i have learned two different language models a and b from some training data now what do we do in extrinsic evaluation we try to use them on certain task so remember some of the applications that we have discussed offline language models so they can used in the spelling correction or in speech recog recognition and all that so now what i will do in this extrin extrinsic evaluation is that once i have learnt two different models i will try apply them to these tasks individually and then see what is the performance that i am obtaining for each of this task by different models now i will say whichever model is giving a better performance on these task is my preferable model so this is called extrinsic evaluation or task based evaluation ok so here we have discussed if we have mentioned three different tasks a spelling correction a speech recognition and machine translation i will have two models a and b i will get accuracy values for these three tasks and i will compare the accuracy and tell which language model is preferable than other so this is my extrinsic evaluation but if there is some intrinsic way of evaluation without actually applying it on some task ok so for that so we have the motion of perplexity so this is how we evaluate language model for for for intrinsic evaluation so intuition comes from the shannon game if you heard about this name so so what is that what is that game how well can you predict the next word given sudden context ok so let us see some examples so i have the first sentences here i always order pizza with cheese and and there is a word that you have to predict may be you can say pepper or whatever and similarly the second sentence the president of india is and you can predict the next word similarly in the third sentence i wrote a and you can predict letter or program or whatever ok so now in my data i know what are the words that will fill up these blanks now suppose i have two different language models i will find out which of these fills up these blanks better than the other one ok so whichever one is good at predicting the next word is my preferable language models so one thing you can easily say from here the unigram models are probably not built for this task ok so suppose you want to predict the next word you just you need a model what would happen so if you remember unigram models do not use the context at all so you will just end up providing every at every place the word that is having the highest probability ok so probably unigram models are not good for this task but you can apply a bigram models the the model which uses the previous word trigram models that uses the previous two words and so on [ vocalized noise ] ok so so finally among different language models a better model is one that you will assign a higher probability to the actual word ok now what is the formal definition of perplexity so in general what we are trying to find out the best language model that predicts an unseen test data ok so how do i define perplexity ok so so this is a simple definition of perplexity i have some test data i am finding out the inverse probability of test data normalized by the number of words that i am seeing in my test data ok so thats means finding the probability of test data and then normalize it with respect to the number of words and its inverse probability ok that means if i have a low perplexity i have a better model ok so formally i can define my per perplexity like that suppose in my test data i have words w one to w n i am finding the probability of this whole sequence and normalizing it by minus one by n here ok so now now this is is a general definition now the next question that you might have is that where does the language model come into picture in this definition is simple definition of finding probability of this whole address w one w n and sub with some normalization now where does the language model come into picture now remember the chain rule that we discussed of probabilities if i have this sequence w one to w n how can write the probability of the sequence in terms using the chain rule of probability so that is what you see here so i can write the same expression like that one divided by probability of w i given the previous i minus one words ok this is in general this definition of probability w one to w n now now can you see how do we apply language models in this definition so in language model we take one particular assumption about using this chain rule that is how many previous context will i will be be using ok so you can in place of this you can replace any of the model that you have learnt so suppose you want to replace with your bi bigram model that means you want to find out what is the probability that the bigram model will give for this utterance so what you will do so we simply replace here the bigram model probability so that will give you the perplexity for the bigram model that you have learnt or trained using some training data ok so you will feed in all this probability w i given w i minus one in that model and that will give you the perplexity of the bigram model [ vocalized noise ] ok so now just to give you an intuition what do i mean what the number of number that i will get indicates perplexity value what does that indicate let us take a simple example [ vocalized noise ] so suppose i have a sentence that contains n random digits ok and i have a model that assigns a prob a probability of one by ten to each digit ok now i want to find a perplexity of the sentence using that model that will give me a probability of one by ten to each digit so what will be the perplexity let us try to use the definition ok so what how the did we define perplexity that is probability of this whole sequence yes w one w n to the power minus one by n yes so can you try to fill in the values suppose my model gives a probability of one by ten to each word so this would be one by ten to each word to the power n for n different words n to the power minus one by n so that is nothing but one by ten to the power minus one and that will give me ten so this tells me [ vocalized noise ] that the perplexity of this model is ten the model that assigns a probability of one by ten to each digit ok now this might give you a hint of what this number indicates ok so let us take another example from some test data oath so what kind of perplexity values we get and what can how we can interpret this values [ vocalized noise ] so an experiment was experiment was conducted over wall street journal corpus so in training they had thirty eight millionwords on which the language model was trained and for testing they had one point five million words on which perplexity was computed and they trained three different models unigram model bigram model and trigram model now and they found out what is the perplexity of the model in using the test set so these are the numbers that were found for unigram model perplexity was nine sixty two bigram one seventy and trigram one zero nine so now let us try to answer this question what is this value of nine sixty two perplexity in unigram indicate ok so what it means is that whenever my model is trying to assign a word it is it is as if it is has to choose among nine sixty two different possib possibilities at each individual choice point independently and randomly ok so this is this means the model is very very perplexed so if the perplexity is high my model is very very confused if it is low my model is not so much confused so in this case what you are seeing if i use a unigram model perplexity is nine sixty two the model is very very perplexed but if i go for a bigram model it is one seventy the model is not so much perplexed now trigram model it becomes even better its only one hundred nine ok so thats what you are seeing unigram per perplexity of nine sixty two means the model is as confused on the test data as if it had it had to choose uniformly and independently among nine six sixty two different possibilities for each word ok and that you can also relate with your previous example because every time it had it had to choose among ten different possibilities for each digit ok because it is about giving a probability of one by ten to each digit [ vocalized noise ] ok so now you understood what the perplexity means now so once we have built a language model we can also use it for other tasks so one very interesting task is called shannon visualization method that is can i use this language model to visualize or generate sentences ok so if you have read the original paper of shannon on the mathematical information theory so there he uses this method to generate various sequences of words ok so what is the idea suppose i have learnt a language model can i use that to generate various sequences of words or sentences so how do we actually apply this shannon visualization method [ vocalized noise ] so suppose i have to generate a sentence so what i will do i will have to generate a sentence so i will first choose a random bigram that is starting the sentence as be the probability so what do i mean by that [ vocalized noise ] ok so i am learning my bigrams so i have in with the start of the sentence whether the word w one occurs with what probability with the start of this sentence what w two occurs with some probability so this is a complete probability distribution that will add up to one now if i have to generate a sentence i have to choose one among this possibilities and this will depend on the probability of that bigram ok so this you can think of as multinomial distribution and you are sampling one word from this multinomial distribution so suppose you pick picked up a particular word here ok so you start generating a sentence you start n w two ok now i have to choose the next word so that is the next step choose a random bigram wx as per its probability so what i will do now i will go to the distribution w to n different words w one w two w two and so on and from this distribution again from this multinomial distribution i will sample one sample one word suppose i find sum w fifty ok again i will try to a sample word with its distribution w fifty and the next word now the question is when do i stop when do i say that my sentence is complete i will say my sentence is complete once i sample a word with the end of the sentence ok at some point i sample wi and the next word is end of the sentence i say my sentence is closed ok so this is the whole idea of shannon visualization method so i do that until i choose end of the sentence and we will see one example from one corpus so from restaurant corpus that we discussed in the last lecture suppose i have learned my bigram model and i want to generate a sentence so how will i do that i choose start of a sentence find out the first word i find i then i take the first word as i choose the next word how do i choose choose the words by sampling from the multinomial distributions ok so in this case if we sample we might end up with getting the sentence i i want want to to eat eat chinese food and after food we get the end of the sentence so i want to eat chinese food would be a sentence that is generated by this method now suppose we try to use this method over the shakespeares corpus so there so we have some number of tokens and the vocabulary size is twenty nine thousand sixty six ok just to give you an indication that bigrams are actually very very sparse so if you take the voc vocabulary size of twenty nine thousand something how many bigrams are possible so what do you mean by a bigram two words together so i can take say v square are the possible bigrams because every possible combination can occur but so this gives me a number of eight forty four million possible bigrams but in the corpus how many bigrams were actually observed so we find there were only three hundred thousand bigrams that we have observed in the corpus so this is very very sparse ok so now suppose i build various language models from this corpus and try to generate various paragraphs and sentences so what do we actually observe ok you will see what happens if i take unigram model bigram model trigram model and and high order models so so here you are seeing if you take unigram model you are getting some words that are probably very popular in shakespeares but the sentence themselves do ah are not making sense ok so you see the word like which here it occurs independent of any other words and this becomes a complete sentence that will not probably something that you will see in the shakespeares corpus so now suppose you go to the bigram model ok now so you start making some sense so you have words sequences like what means ok i confess and all sorts ok they have good bigrams but if you's try to observe the sentence probably they are not making much sense now if you go to trigram model then again you have getting some more sense five first if shelled i and the shell forward it should be should be bent they have some nice sequences again and this is starts making a much more sense in terms of sentences and if you go to quad terms then you are getting something that is resembling ok king henry henry watt i shall go see the traitor gloucester ok so this look like a valid sentences from the shakespeares corpus so that is the idea as you go to higher and highder higher order model the kind of sentences you generate will be something that actually resemble the corpus from which we are hm training this language model okay so this is my visualization method so now we will try to go to the problem of a smoothing that we discussed in the last lecture so remember what the problem was in my language model suppose i am training for bigrams so so yes so just take the statistic that we saw from shakespeares corpus there were eight hundred forty four million possible bigrams out of which only three hundred thousand bigram actually occur so now if you assign the probability to each bigram very few will will get a probability greater than zero others will get a probability of zero now suppose you are taking a test data and finding out how much it resembles shakespeares corpus and whenever you see a bigram you are taking probability from the trained language model now suppose a bigram occur that is not there in the shakespeares corpus imagine the probability is zero now what happens to perplexity value remember this is simply the multiplication of all the different probabilities so this one becomes zero ok so that will not to very very helpful so you actually would like to give it some probability so that this does not become zero ok thats why we will study topic of smoothing how we can assign different probability values ok to get get around this problem of zeros ok so let us take a simple example suppose i am learning my trigrams and my training data i have seen the following sentences ok i have seen these many occurrences denied the allegations denied the reports denied the claims denied the request ok and i am learning a model of finding out the word after denied the ok now suppose in my test data i see these two occurrences denied the offer and denied the loan so what would happen to the perplexity of my model ok so in the test data i have seen these two occurrences that were not there in the training data so so probability of offer given denied the will be zero same with probability of loan given denied the so the test set will be assigned a probability of zero and the perplexity cannot be defined ok and that was thats what we were saying initially so how can i go around this problem so that i can compute my perplexity even if there are certain bigrams or trigrams they did not occur in my training data so what is the idea of smoothing so idea is suppose so this is what we were seeing i am computing the trigram model for the word w after the occurrence denied the and suppose in my data i see four different words right i see allegations reports claims and request in total seven words and i can assign the probability to each of these as three by seven two by seven one by seven and one by seven ok and that is what you are seeing in this plot three by seven two by seven one by seven one by by seven so these adds up to one now what about the probabilities for the other other trigrams like denied the attack denied the man denied the outcome these are also very very feasible trigrams so here what will happen we will assign a probability of zero to all of these yes so all these have a probability of zero now what is the idea of smoothing the idea of a smoothing is can i take some probability mass from each of these four words and assign that to the three words or any other words that i have not seen in my training data ok so that is can i steal some probability mass from these four words to assign some probability mass to the other words in my co in my data so that is suppose here i have taken a probability mass of point five each from the four words ok so that means i get a probability mass of two by seven and that mass i distribute among all the other words in my corpus and this can be distributed in multiple different ways ok we will see some solve some possible ways in which we can distribute this poss this stolen mass to some oth to the other words that we were dint see in my training data ok and how much mass has to be distributed that also we will see so there are different methods that that do that ok so the basic idea is clear so i exactly we do that ok so a simple method is called laplace smoothing or add one estimation so what is the idea so pretend as if you have seen every n gram one more time that we actually did in my training data so that is suppose i saw it only once so i will pretend as if i have seen it twice so what will happen to the n grams that i have not seen at all in my training data so i will pretend as if i have seen them once ok so this is simple idea so so we will just add ones to their their actually counts that we found from the training data we will just add one to that so remember this maximum likelihood estimate for this bigram probability of w i given w i minus one we find it using number of times if we observe w i minus one followed by w i divide by the number of times i observe wi minus one in my training data so this is my definition of mle so how do i estimate language model or a bigram model using mle now how do i use the laplaces smoothing there so what did we say we will pretend as if we have seen each n gram one more time than we actually did so what we will do here so we will add one to the actual count ok so will do that for each possible bigram but to ensure that the probability adds up to one i have to normalize i have to make some modifications to my denominator so that so that they are normalized so what will i add to my denominator ok so to get this answer you can see how many different bigrams will be there for which i will be adding one so how many different wis will be there that will be number of words in my vocabulary ok that means i will add one capital v times so to normalize i will also have to add a v here in the denominator ok and that is essentially the idea of my add one smoothing that i add a one to my actual n gram count and add a v in my denominator two to to normalize it ok so this is my add one estimate number of counts plus one divide by the count of the unigram wi minus one plus the vocabulary size so that is the very very simple smoothing technique that you can use in general ok so that does not require a lot of ah fancy estimates you can just get your language model and easily apply this smoothing method ok now we can all when we apply this add one smoothing method we can also talk about what is the effective bigram count so let me understand what do i mean by this effective bigram count so what i mean is so you see here you have modified your actual counts or the probability so what is the idea so now what is in effective what would have been the count in your act actual training data such that you have got the same would have got the same probability ok so what i am saying so this is your new probability probability wn given wn minus one as per the add one smoothing yes so question here is what would have been my effective bigram count c star w n minus one w n such that if i do the mle estimate pmle w n given w n minus one i will get the same value as this ok so how do i get the effective bigram counts so this effective bigram count if i would divide by counter w n minus one i should have got this probability so thats means i can compare c this probability with this probability and that will give me what is my effective bigram count ok so thats how i define my effective bigram count so we will see some example so if i apply this for my restaurant corpus so what kind of ef effective bigrams do we observe ok so we remember this is what this is the counts that we see in my bigram this my restaurant corpus ok so we saw that in the last lecture i wont occurs very high number of time sense so on so this is the actual bigram that we see now suppose i apply add one smoothing so so you cannot apply to this data right exactly because is not the complete data this is just a small screenshot ok so now suppose you use add one smoothing and then if you find that what is my effective bigram counts so what kind of effective bigram count do you see you see in this table certain counts are zero so one thing you would ah assume that after applying this smoothing technique these will not be zero this will be greater than zero and whatever for having a high value should have a smaller value because some mass would be sh esti would be stolen from there to give it to the words they did not occur and thats what exactly you will see when we apply this add one smoothing and then do the effective bigram counts so here so these are my effective bigram counts so you see here whenever the word i want the bigram i want occur eight hundred twenty seven times initially the effective bigram count now is only five twenty seven on the other hand i too did not occur at all in my training data but now it has a effective bigram count of point six four another observation you can make from here given the previous word for the next word if that occur zero times the effective bigram count remains the same so its point six four whenever the previous word is i for the words like to chinese food and lunch but this varies across different previous words so if you take the word like want for wants this value becomes point three time for to it becomes point six three ok so this depends on the previous word and that you can also see why ok because i am i am doing this plus v to the count of the unigram yes and that will be different for different words thats why this value will be also different for different words [ vocalized noise ] ok so in general so so here we talked about the add one estimation but in general there are some simple variants of this also ok so one simple variant is called add k var add k estimation so we are adding one to each bigram so why why want why cant we do some general thing like k k can be point five or more than one depending on how big your data is ok so this is called add k estimation so here what is the idea we add k to each count and accordingly we will ah add kv to my denominator yes so this is actually the same as add one ah estimation if i take k is equal to one now i can also make some variant here so suppose i say kv is equal to m ok so now i can write it as this plus m and this plus m by v yes this is effective the same now m is k times v so this is another variant and this also gives me an idea on how i can improve this basic smoothing method so here what i am doing i am adding m by v to each word effectively i am doing m times one by v to all the v words in my vocabulary ok and if we add up for all the words in a vocabulary m by v you get an m so now can we do something better there the idea is instead of adding mtimes a uniform one by v to each word can you add m times the unigram probability for the word and you say this will add up to one for all the words you will effectively end end up guiding the same values in numerator and denomi denominator when you normalize the probability but this might be a better estimate ok why is that so let us first see all these valuations ok so this is when i replace kv by m thats what i get and in place of one so here in place of one by v i can also replace the probability of the word ok that will be a different smoothing but what we are saying this might be a better way of doing a smoothing this called unigram prior smoothing so so let us just discuss this point why this might be a better way of doing a smoothing ok so remember what we are trying to do here we are trying to find a probability or different words that do not occur in my training data wi given w i minus one ok take a word w i that is very very common ok so like i have a word government that is very very common ok and i might have some other word like may be something like smoothing ok that may be other word w one w two ok what are you doing in your add one smoothing or add k smoothing you are just studying k or k or one and dividing by the count of the previous word plus kv and this will be same for both the words yes so now so what is the idea of unigram prior smoothing idea is that if i know that this word is more common in my corpus then this word probably i can say that the probability of this word occurring of a w i minus one will also be higher than the probability of this word occurring after wi minus one and that i am trying to exploit this data i have from my corpus and i am trying to exploit that for doing my smoothing ok so this is called unigram prior smoothing ok so in general so there are many other ways of doing smoothing also so we discussed add one smoothing and the institution behind unigram prior smoothing but there are other advance models of smoothing also that we will see in the next lecture so in week three ok