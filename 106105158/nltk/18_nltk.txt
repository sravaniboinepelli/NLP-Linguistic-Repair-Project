so welcome back for the second lecture of this week so in the last lecture we were ah discussing viterbi decoding for h m ms and in the end we discussed the problem of learning the parameters of h m ms and we say that when the label data set available we can simply estimate using maximum likelihood estimate ok by by the label data set but if the label data set is not available then how do we actually learn the parameters of the system and we said we will be using some sort of expectation algorithm and in this particular case this is called baum welch baum welch algorithm so baum welch algorithm as i was saying this is a well known e m algorithm to estimate the maximum likelihood for the parameters of the h m m ok so ah so what are the parameters of h m m that you want to estimate ok let we just formally define them and then we will see how do we estimate them using buam welch so in in the h m m model so we have a hidden state at every time point and an observation variable ok so here i am using capital x t to denote the random variable that is the hidden state and capital y to denote the observation variable ok so both x t and y t can take many many different values for our part of speech tag is each x t can take one of the part of speech tags and each y t can take one of the possible votes ok so this my my ah hidden variable hidden state x t and the observation y t the parameters are a b and pi that we discussed last time what is a so a is my state transition matrix so given given that the previous state was i what is the probability that the current state will be j ok so here the t as such does not matter what matters is that the state j occurred after state i at any any given time point then i have the parameter pi that is what is the probability that this particular state is starts the sequence so probability of x one that the the first state beam the i x state that is my pi a and then thirdly i have the emission matrix so where the entries are what is the probability of observing this particular word or this observation given the current state ok so this i am denoting like b j y t at the state j what is the probability of i am meeting the observation variable or observation y so either three parameters three set of parameters that parameters i have learnt so now what is given to me ok so i was saying that we are given the coppers not labeled but the seek the word sequences are given so this is this can be seen as y observations are given different sentences are given i knew what are different words occurred in the sentences so i can say that i am given a set of observations in data ok so here i can say that i am given a set of observation sequences that the first observation was the word y one then y two up to some y t where y t denote the end of the sentence so i am given a set of observation sequences and my aim is to find out the optimal set of parameters theta that maximize the probability of this observation so these observation sequences so that is where i am using buam welch algorithm find out what are the optimal set of parameters theta that will maximize the pro probability of likelihood of observing this observation that's why i am using expectation maximization algorithm so what so what is the main intuition of of e m so idea would be i want to estimate the parameters a b n pi so what i will do i will start with some random initial ah probabilities for all all these parameters i will initialize them with some random values so i have all these probabilities ok now using this initial values i will try to find out what are the probabilities of various state paths so i give on a given sequence what is the probability that a particular state would have occurred given these parameters so once i have obtained the probabilities of different state paths or different states a different point now i i got this this probability i will use that again to compute to re to re estimate my parameters so i will get started with theta zero i get some theta prime ok now i will again use the theta prime to complete my state paths probabilities again use that to compute recompute my parameters until it somehow converges ok so this is my trade devaluate first use some parameters theta to get some likely from the data for the hidden variable here that is the states once you have that that likelihood or the probabilities use that to compute my my theta ok so so what we are doing here so i am start starting by choosing some initial values of the parameters a b m pi ok and then i have to repeat the following steps until convenience what is that firstly determine what are the probable paths ok that x t minus one that t minus one object point i see the state i at tth i see the state j and so on what is the probability of various state paths now once i have these will teach count the number of times so what is important expect a number of times because we are only computing probabilities we are not finding the actual paths so count the expected number of transitions a i j as well as the expected number of times various emissions b j y t m eight ok so using these state paths can you do that yes i know what is the probability of x t minus one is equal to i h t is equal to chain i can use it to compute an expected value for a i j and this is what we will be doing so will compute a i j similarly i al already know the observation so i can also computes unexpected vanish for b j y t what is the probability that i see objection y t for the particular state j so once i computed this a i j and d j y t i will again estimate my parameters theta using these these computed values ok now i will go back in the loop i have this theta i will compute the probabilities of various state paths ok again compute a i j b j y t and again compute theta and i repeat that until convergence so now what is so so all these are interesting here like firstly once you are given some set of parameters theta it can be either the initial parameters or some intermediate parameters how do you actually compute different the probabilities of different paths how do you actually do it and for that we use a forward backward algorithm that is the ah main concept of this algorithm ok so let us see what is this forward backward algorithm ok so in forward by back backward algorithm we have a forward procedure and a backup procedure ok so so far for explaining let me just take show it on paper ones so what is happening i am observing this sequence y one y two y small t y t minus one y t this is my observation ok and correspondingly so you can say this is y one is equal to y one first observation y two is equal to y two and so on ok and correspondingly there are states so is my observation and then there are states so there will be some state x t this is y t observation is my state x t is equal to y it can be any of the possible states x t now what do i do in this forward backward algorithm i compute two different probabilities one i called as the forward probability this is the probability of observing y one to y t plus and x t is equal to i give my parameter theta so this is my alpha t for the i of the state the probability that i am observing y one to y t and at t h point the state is i given a parameters theta ok y one is equal to y one y two is equal to y two y t is equal to small y t and x t is equal to y given parameters theta this is my forward ah probability then i compute a backward probability what is that this is the probability of observing this given this state ok so this is my alpha i t and there is beta i t that is probability that y t plus one is small y t plus one up to capitol y t is y t given x t is equal to i and theta so these are two different probabilities that i state so what do you see here so i am taking just a state at a particular time t a particular state time now why do you actually compute this alpha i t and beta i t how does that help now i have these probabilities i can simply multiply these alpha t times beta i t to get the probability that i observe the sequence and the i of the state ok and to actually compute the probability of c saying that at t h time point the state was i i can margini marginalize it by all the possible states that can happen at time t ok so i can multiply sigma i t beta i t and divide by all the possible sigma j t alpha j t n beta j t to compute the probability of seeing state i at time point t and this is thats how this forward and backward probabilities help ok so you will see that in detail ok so right now just the formulation so i am this higher the sequence i divided in two parts for a given t this is state i i have the forward probability alpha i t and a backward probability beta i t alpha i t is this further sequence and the state beta is the ending sequence given this is state the previous state x is equal to i ok so now so this is my alpha i t ok so now how do i compute these values of alpha i t i want to compute this for all possible i's and all possible values of t so this is a forward probability so i have to start with the first point so how do i actually compute alpha i one ok what is that so alpha i one from this equation which probability of observing y one is equal to y one ok and x one is i given theta so what is the probability of x one is equal to i given theta that is pi i ok this is my pi i and this probability would be emission from x one from the either state so this will be b i y one ok so this is how i will compute the initial one alpha i one for all possible states i now at some point of time ok at some point t plus one how will i compute alpha j t plus one given the previous alpha i t i have completed so alpha j t plus one ok so t plus one means the probability of seeing the sequence up to white t plus one and the state x t plus one is j so how can i compute that using the previous alphas so i have the i have the previous alpha that gives me this so alpha i t at the previous time step i had some state i i have the transition from i to j so times a i j and now i have the emission for the t plus oneth observation that is b j y t plus one and this i can obtain from any of the previous states so i have to sum over all the possible states ok so from all the previous states at the t h time step i can be an estate i i have the transition probabilities and the emission probability for t plus one and that is how i can compute sigma alpha j t plus one from all the alpha i t h so this is how you can compute your alpha then because a manner starting from alpha i one what will you do for your betas ok so this is a backward processor so what is my beta i t this gives me the probability of ending the sequence with y t plus one two y t given the state i a time this is my backward procedure now this i again compute aggressively in alpha i was starting from the first word i was campaigning alpha i t in betas because the backward procedure i will first compute beta i capital t t is the end of the sequence and what will the beta i t so beta i t would be probability of so given that previous the t h point state is i having a sequence on t plus one but there is no word from t plus one so this will always be so this is always be one this will be a null sequence so this is always one now i have to compute it recursively for all other beta i at time point t ok so here i am going backwards so you assume that you have completed all the beta j at time t plus one and you want to use that to compute beta i at time point t how will you do that so this is very similar to what you did in the case of alpha so we can see this so this is beta j at time t plus one transition probabilities from i to j and the emission probabilities probability for the t plus oneth observation and you sum over all the possible states at times so very very similar to what you did in the case of forward procedure so that's what you will do in the backward procedure for computing beta h in a recursive manner ok so now you have the a way that given the set of parameters minus theta you can compute all this alpha and beta ok so all the alpha i t is an beta i t you can complete now remember what is the next step in the algorithm you want to find out what are the best possible paths or what is the probability of various paths that is what we intended by doing this alpha and beta let us see how exactly we can compute these probabilities of various paths so i want to compute the probabilities of say probability x is equal to y given my observation and parameters theta also i want to compute x t is equal to y and x t plus one is equal to j given y n theta i want to compute both of this so if i can compete both of this i can i can compute all the parameters i can complete my transition probabilities using that i can compute my initial probabilities using that by taking t is equal to one and you can also compute my ambition probabilities so now my problem is once i commuted alphas and betas can i compute these probabilities ok so that is what we will see next so i am giving these some some names so firstly i have this gamma i t that gives me the probability that at t th point the state is i given my observations y plus theta parameters and similarly i have zeta i j t that is probability that at t th point i have state i t plus one point i will change a given by n theta so gamma i t is written terms of multiplication of alpha i t and beta i t ok divided by summation over all possible sigma z alpha j t and beta j t over all possible change so now how how do we actually come up with this ah equation of gamma i t and and zeta i j t let us look at it ok so here i want to compute probability x t given x t is equal to y given y n theta so now i will computed using probability x t is equal to y y given theta divided by probability y given theta ok the simple conditions ok so if you follow that theta this is nothing but x t is equal to y given by x t is equal to y y divided by probability y this simple the condition probability row now this one i am writing is summation j is equal to one over n equal to one to n probability x t is equal to j y given theta ok so now you can see the symmetry in this equation now what i what i have to show that alpha i t times beta i t is probability x t is equal to y y given theta and that is very easy because what is alpha i t this is probability y one to y t plus x t is equal to i what is beta i t probability of y t plus one to y capital t and now x t is equal to y is already given for beta and this is given my parameters theta ok and this is what we are seeing here this is my alpha i t times beta i t and this is marginalizing over all possible states that time bind so this will be sigma j equal to one to n alpha j t beta j t so this gives me the equation for gamma i t similarly you can see for zeta i j t that is for probability of x is equal to i x t plus one is equal to j y given y n theta so in the same way you can write it as probability x t is equal to i x t plus one is equal to j y given theta divided by probability y given theta that will marginalize over all possible i n j ok and you will say probability x t is equal to i x t plus one is equal to j y given theta for all possible values of phi n j now what we have to show that is that this is actually this formula ok now so what is this so you are given state at time point t and time point t plus one so what you are actually doing is that using the forward procedure from y one to y t and you will also get the state at state i at time point t and you will use the backward procedure because you are given the state at time t plus one state j beggar procedure for from y t plus two up to y capital t ok so this is what is cap captured in alpha i t and this is captured condition on j in beta in beta j t plus one then you will compute the probability of transition from i to j that is a i j ok and y t plus one given j that is your d j y t plus one ok so this equation is nothing but probability of x t is equal to y x t plus one is equal to j y given theta this whole sequence is there and these two states are also and similarly you can see for the for here this is over all the possible values of i and j ok so i hope this is clear that once i have completed all the possible alpha and beta and the parameters are the previous parameters of the ah we can compute this gamma i t and zeta i j t th that are probability of state at a given time and a sequence i and j at time t and t plus one ok this we can compute using alpha and beta now coming to the last part that is once i computed some probability is over by possible state paths how do i estimate my parameters again ok i have started with some initial parameters computed this now i want to re estimate my parameters theta so how do i do that so now i have to estimate my pi i a j and b i v k so how do i estimate this pi i is probability of seeing the state i at time one so now from these parameters which one can give me this value probability that x one is equal to i the second get from gamma a one ok that is that can give me my pi i is probability that at x x one is i so this is to estimate my pi next thing i have to estimate is transition probabilities probability of going from state i to state j now i have completed this ok so how do i compute my e i j e i j would be at all possible times how many times you are having the sequence i n j so this will be summation over t zeta i j ok jee ji zeta i j t is each i followed by j divide by over all the times when you are seeing this t ok so when you are seeing the state i and this you can compute using your gamma i that is summation over t gamma i t and that will give me my a i j all the possible cases where all the all the probabilities of i and j having together at any point divide by all the probabilities of anyway this is my a i j and similarly how can you compute your b j say v k that is at state j you are emitting v k how can you estimate that so for that you have to find out whenever you are in state j is the observation actually v k how many times we are in state j by the observation is we can divide by number of times you are in state j ok so this is so here the nominator is easy that is number of times you are so this b j v k a number of times you are in state j this is gamma j t numerator number of times or the probability that you are in state j times the observation is actually v k ok now so at time t the observation is small y t ok so i can use some sort of a simple notation or indicator function one v k is equal to y t so indicator function ok what do indicator function so one v k is equal to y t will be one if v k is actually y t and zero otherwise listen to my indicator function so now whenever if v k is y t whenever i observe y t i will add it otherwise it will be zero and this is this will give me the emission probabilities so i could estimate my pi a i j as well as b j v k using this zeta i j and gamma i i which i computed using my forward backward probabilities and this is one complete pass of this algorithm ok now i have these parameters i will plug these again to compute alphas and betas again compute my gamma and zetas and again estimate the parameters until it converges ok and this is the buam welch algorithm for for landing the parameters of h m m when the label data set is not available so this gives you a very nice handle on ah h m ms so you can apply h m ms when you have the label data set very easily by by learning the parameters directly from h m ms maximum likelihood estimate and data beta decoding but even if you get a new ah problem or a new language furtherer you have to compute part-of-speech tax but you do not have the labels you can use it this buam welch algorithm to estimate the parameters of a h m m and then you should have enter the code to to actually given a new sentence find at part of speech tag sequence so and you can use that for any other sequence label task so this completes our discussions on h m ms that are that were one of the very popular models for ah this part is fish tagging in many other sequence labeling task but they they have certain limitations and that is what we will discuss next then that what are these limitations and how do some other models some so we will go to now some discriminative classifiers like maxim and role models they get rid of these limitations so this is for h m ms let us lets let's look at maxima into the models in the next lecture thank you