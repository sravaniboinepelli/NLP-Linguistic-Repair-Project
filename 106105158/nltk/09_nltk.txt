ok so so welcome ro the forth lecture of second week so in the last lecture so we dis we were discussing about a spelling correction and also seeing how do we do that it using the contexts and remember the last ah problem that we are facing is how do you find the probability of sequence of words ok so I have sequence of words in in my sentence that might be candidate what the probability of that sequence and thats why we will we said we will be using language models for it so today the talk that the topic of this lecture is to see what are my n gram language models and we will start with some motivation of what are the different applications were we might need them so n gram language modeling is a very very ah nice technique in n n l p and its applied in many different applications it is very very one of the very basic concepts in n l p ok let us see what are my n gram language models ok so i will start with the motivation of context sensitive spelling correction that was a topic of just the previous lecture suppose i am having the sentence the office is about fifteen minutes from my house ok so here you can clearly see that this is just a spelling error minutes has been typed as minuets but suppose the word in minutes is also in my vocabulary ok suppose i see my vocabulary and i find that minuet is a some sort of slow graceful dance ok that was popular in seventeenth and eighteenth cen centuries so this word in my vocabulary so now that means i cannot easily detect that this is the incorrect word by using some isolated word as correction so i should be using the context for that so what is the probability that so i will try to use some sort of language model to say with a probability of this a trends about fifteen minutes from its higher than the probability of a trends about fifteen minutes from ok if you observe these two a trends you can you can yourself see that for it will should be more probable but how do you formally defined these probabilities and what is the model that we use for it and thats what we will be seeing in this language model ok now so what are some of the other applications for this is helpful remember one of the earlier slides in a ourlast week you were saying in this case of speech recognition you face this problem that when you are uttering something you have to trans you have to transcribe that ok so whenever I am saying i saw a van it might also sound like eyes awe of an ok so among the two a trends is which one is more likely so again can i give some sort of probability value to each of these sequence in the say that probabilities i saw a van is more probable than the other similarly in machine translation there is a problem of collocations that certain words even if they are correct translations do not occur much in the language with a in the context of others like if i say if i have the word winds and before that as an adjective i can put either high or large because so for example i have an hindi actress tej and i want to translate that into english how i we will winds now for tej it might happened that both translations high and larger possible in in english now among those which one should i choose and suppose from my corpus again find this language knowledge that high winds occur with more probability then large winds then i would say that high is a more probability translation than large and there are other applications is spelling correction we have seen and you might have to use that for generation whenever you have to generate sentences again which particular generation has the higher probability ok another possible application that you might be using it in say google search or ah other places whenever you are trying to type something that they will be predict what will be the next word so whenever i am saying please turn off your cell what will be the next one you can bit probably phone please turn off your cell phone and your program does not probably compared or something ok so given is a in a sequence of words how they predict what is the next for that is that is going to come in this sentence so that is my auto completion task that is huge a lot in a coyote completion or even when you are typing something in your s m s or you might have certain softwares that that do that so so they also use the concept of language modeling ok so these are also called the predictive text input systems so given whatever you are typing they will give you choices in how do you complete it what does the various possibilities were which will complete ok so now so given these applications in background let us see what is my language model what is the goal what do we gone to achieve so we talked about this in the previous lecture we said we i want to complete probability of w that is what is the probability of the sequence w one to w one this is the sequence of words what is the probability of the sequence so this is one of the gold compute the probability of a sentence or a sequence of words ok pro p w what is the other related task that we saw in prediction system that given three words w one w two w three what you might w four so probability of w four given the three previous words probability of an upcoming words now in general any model that computes either of these probability of the sequence or probability of the word given the previous a trend is is called a language model ok we were see language model using these definitions ok so now how do i actually compute probability of the sequence w means sequence of words here so suppose from the examples that we were taking initially i have the sequence about fifteen minutes from i want to compute the probability of the sequence now suppose i do not tell you anything else what is the simplest model that you will applied to compute this probability so you will probably apply the chain rule of probability yes ok so remember chain rule of probability so it derives from conditional probabilities ok so you conditional probabilities you might remember so that is what is the probability of b given a that is probability of the joint the joint probability p a b divided by probability a now you can also use that to writing in some other way so you can say probability a b which probability b given a times probability a ok and if you you can do that for any number of words in my sentence or in general any number of evens in probability so you can say probability a b c which probability a probability b given a probability c given a b and you can keep on doing that finite numbers and that is my chain rule of probability ok i can write p a b a p a times p b given a in general even if i am more variables i can do the same same chain rule of probabilities idea and this is the general formulation probability x one given x one is probability x one probability x two given x one and so on and probability x one given x one to x n minus one so now given this chain rule now go back to a initial ah problem probability of about fifteen minutes from so how do i write it using the chain rule of probabilities so how will you you right this probability you will say this is same as probability of about times probability of fifteen given about times probability minutes given about fifteen probability from given about fifteen minutes so you can right it like that yes ok now suppose i even increase this to about fifteen minutes from office what will you do ok and you will write it in terms of this probability also probability of this given about fifteen minutes from so now how the question that comes is find i can write it in terms of chain probability chain rules chain rules how do i actually compute this probabilities now what is probability of office given about fifteen fifteen minutes from that would be in my corpus how many times do i observe about fifteen minutes from among those how many times i observe office after that now what is one particular proper you will face in doing it in this work you might not have seen sufficient number of about fifteen minutes from in the corpus suppose it occurred only once it occur with the office this probability will be one but does not mean that only office can occur this about this occurrence and this problem becomes savior as you keep on going to the higher and higher number of words in the conditional ok so so we will never see enough data for estimating these so i cannot estimate easily what is the probability of office given about fifteen minutes from so that means this will not work as as it is so we we will need to do certain simplifications ok now so what is the simplifying assumptions that we make we say ok we can probably use only the previous word and forget about everything else so I have to compute the probability of office given about fifteen minutes from and its a that is approximated by probability of office given from i forget the other terms ok so or i can use the previous two words so this can be written as the probability of office given minutes in from ok so this is simplification that we are making why we are doing that because now it is easy to find complete the probabilities what are the words that come of that from and how what session of times office comes after from but this was not possible when i was taking condition on four previous words because information was probably not seeing them enough in my data so this helps us giving a better estimate of these probabilities ok more formally we can use kth order markov model ok so i have this information have to compute the probability of the sequence of w one to w n by using chain rule i write it as as this w i given the previous i minus one words multiply for all the words now in kth order markov model assumption what i will do i conditioned it only on the previous k words so i will write probability w i given w one w i minus one as w i given only the previous k words ok so here you are using only one up to k three previous words not all the i minus one words so this is kth order morkov model assumption ok so we can do that for all the come all the components in this product ok now and that thats how we will define ever n gram models ok so if i take this particular probability probability office given about fifteen minutes from if i am using only the previous word ok that will be using it two gram language model so in general if i am using only n minus one words of prior context i am defining in n gram language model ok so here if i am using if i am not using any word from the context zero words from the context this is a unigram language model n is equal to one here if i am using one word from the context then its a bigram language model n is equal to two if i am taking two two words from the context it is a trigram language model n is equal to three ok now can you try to relate in n gram language model with some kth order markovs assumption in kth order markov assumptions you were using kth previous word so if i am using k previous words i have a k plus one gram language model so i can say an n gram language model is an n minus oneth order markov model ok in n gram language model i use n minus one words from the context so this is the relation between an n gram language model and an n minus one order markov model now internal we can extended to trigrams we assume two words from the context four grams three words from the context of five grams four words from the context ok but in general we cannot see that any n gram will be sufficient model for the language why because language we also see some long term dependencies so consider this sentence the computer which i had just put into the machine room on the fifth floor crashed now what is the word on these the word crashed depends ok if you were see here the word crashed depends on the about computer but this cannot be captured by using two gram three gram four gram five gram six gram so on to see so you might have to go to eleven twelve grams and that is probably not very advisable so you use must always knew that an any n gram model is not a sufficient model of a language but it captures many word ordering relative regularities ok so in most of the applications we can use simple n gram model with an it will two three and we will get up with it ok so now the question is how do we estimate these n gram probabilities from a corpus so for that the simple method of estimating these probabilities is by using some maximum likelihood estimate ok so what is that suppose i have to compute the probability of w i given w i minus one so i will find out in my corpus how many times the word w w i minus one occurred among those what fraction of times w i occurs after that ok remember we were doing it from the one of the earlier slides in versatile actress whose so how many times the word actress comes after versatile what fraction of times same here what is the fraction of times that the word w i occurs after the w i minus one so which defines a probability distribution after w i minus one each word my vocabulary can come and this will be a probability distribution ok so in general it i can give a notations of c instead of count c w i mi minus one means number of times w i minus one occurred in my corpus and so on now lets take an examples and see how do i estimate these probabilities so i have three sentences in this corpus i am here who am i and i would like to know and you also have some tokens on a start of the sentence and the end of the sentence and you want to compute the bi bigram probability w i given w i minus one so how will you do that so suppose i want compute all these probabilities probability of i coming at the start of the sentence probability of here coming with this probability end of the sentence coming after here probability of would coming after again so on so how do i compute this probabilities so i just take the first one probability of i coming at the start of the sentence so i will find out how many of start of the sentences i have seen among those perfection of the times i occurred so i have seen three start of the sentence out of those two is highly occurred so this probability would be two by three here how many times i have seen here one ok and among those how many times the end of the sentence occurred also one this will be one by one and so on ok so so this is what i find two by three one one by three one by two and zero so this is easy given a corpus you can find out the bi bigram probabilities by its just giving the counts now this is some example from some restaurant corpus so that had nine thousand plus sentences and these some bigrams that were the numbers are given here so can you see some regularities here so i and i do not occur together much ok but i and want occurred eight hundred twenty seven times so you are in a restaurant after i mostly say want i want some sort of food ok so i want is a bigram that occurs a lot in this data what are the others bigrams that occurs a lot want to i want to do something eat chinese ok chinese food eat lunch to spend so all these bigrams that occurs a lot so this tells in lot about that corpus so i am talking about the restaurant corpus where its more about ordering some food and eating some so so a kind of foods ok now suppose i i have to compute the probability bigram probability what is do i need i need the count number of times this were occurs without the word and you know divided by the unigram probability of the previous word so suppose i give you the unigram counts also of all these words can you come with the bigram probabilities yes it is becomes quite easy so now i just divided by the number of times i occurs so here so number of times i amount of the together divided that number of times i occurs so this gives you probability goes to point three three so probability of want occurring after i is point three three from this corpus and so on i can compute all the words in this corpus ok so now given a corpus you should be confident now that how do i compute the bigram probabilities now suppose you have computed bigrams probabilities from the corpus now can use that to solve over initial problem that is how do i find the probability of this sentence this sequence of words so i have the sentence here a start of the sentence i want english food and in end of the sentence i i want to find the probability of the sentence so how do use bigram model do that i will say ok this is probability of i given as the start of the sentence times probability want given i times probability english even want and so on because i am actually using the chin rule of the probabilities but i am using in first short of a markov assumption ok so that gives me all this probabilities and i multiply that and then it gives me the probability of this sentences each ok so if i use the previous probability that will be flip point zero zero zero zero three one and now you can give me any sentence and i can use my bigram probabilities to find out the probability of the sentence and i can solve all my problems of spelling corrections and other scenarios so now so from this corpus what does the knowledge n gram represent ok so these are some values so what do you ah try to infer from there so probability english given want is point zero zero one one and probability chinese given want is point zero zero six five what does it that tell so if you see that will tell that the ah any chinese food its six time more popular than the english food in whatever from whatever detail it was taken from ok to given want is point six six so that is two occurs a lot with a corpus after i want i want to do something eat after to is again point two eight someone want to eating to do something into eat occurs a point two eight probability food given to a zero that means the word food never occurs after to and that is something that talks about the language in language generally use a verb after two i want to do something and here the food is a noun so this will not this will not occur in my data similarly want given a spend as a zero again some fact about language that two words generally do not occur occurred simultaneously ok and i given the start of the sentence point two five again gives gives some in instigation that most of the sentences start with i in that in that corpus so mo mostly about i want to do something so you started with i so these n grams might represent some knowledge about the language and grammar as such or about the particular data or domain that you are trying to build it these form and this is some idea that we can use for even modeling different domains in in my data separately i can build different language models for each domain ok so will talk about this further ok so now so there are certain practical issues that you might have to aware of like when i am doing this probability computation for a sentence i am actually doing multiplication of many probability values and all of these might be very very small so if i simply do multiplication of probabilities it might lead to some underflow and this might just end up in getting a zero values for all the probabilities so its better that you do everything in log a space in that way you are simply adding them and in any case adding each individual operation or more efficient operation then multiplication so you are just storing the log of probabilities and you are adding these problem these logs ok so you can if you aware to find out probabilities p one times p two times p three times p four if you convert in log space that is nothing but log p one plus log p two plus log p three plus and so on so you can restore a log values and you can just add this there is another problem of handling zeros ok suppose in your a trends there is a particular bigram that you see that never occurred you a trend data so what would happen you will give it a value zero but that will convert the whole the ah everything zero ok so you need to do something for that and we will see that in the in the in in the concept of a smoothing ok so there are some popular toolkits are available so s r i s r i s r i l m is one popular toolkit but there are many other toolkits that you can use for language modeling ok you can also if you want to use some large corpus data you can try to use these google n grams so there again available on this link and there you will find out for each different n gram what is the number of times they occurred in the corpus ok so if you go to this link you will find out there are huge number of tokens sentences and all ok they they gave you you unigrams bigrams trigrams fourgrams and fivegrams so what are some of the examples so suppose i am sing it after a starting from serve as the so the that data will tell you serve as the inspector occurred sixty six times the in the in the corpus serve as the inspiration occurs thirteen ninety times serve as the installation occurred one thirty six times and so on so this is a fourgram that you find from the data but you see these are only the counts now how will you use that the compute the probability that probability will inspector given serve as the so for that you will also need to use the trigram count of serve as the so again this five is available you can you can get this fourgram of the data and trigram data and try to compute the all the probability values ok now so they also give a nice a p i by which you can visualize many interesting patterns in the usage of words ok so because heritage divide across many different centuries you can also plot it temporally so what we are seeing here suppose i give three different queries albert einstein sherlock holmes and frankenstein on google n gram viewer it tells me over the years what is the probability of these bigrams so can you see something interesting here so sherlock holmes became popular around eighteen eighty five and so on before that is when nearly zero frankenstein probably there in the novels even before that so even from the six eighteen hundred he find the occurrence of frankenstein that keeps on increasing even two thousand its more its much more than albert einstein and sherlock holmes albert ein einstein started getting popular in in the data around nineteen seventies thats why the most of the dict discoveries happen happened and and it came into the books and at some at some point of time it became more popular than even sherlock holmes ok so you can do nice sort of analysis of what kind of words came into ah the language all my data in what times and how the popularity changed over the years by using these kind of data ok so so today we will we gave only some in intuition and what is language model how do we compute that and what we can use it from for there were some problems that we saw that how do we handle the zeros ok so with zero is the whole probability of the a trends will go to zero even if one of the component has a probability of zero so how do we avoid that problem how do we solve that problem thats were the concept of the smoothing will come in picture and thats what we will covering the next lecture ok