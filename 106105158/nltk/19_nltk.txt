so welcome back for the third module of this week so in the last week we have in the last module we had finished the discussions on hidden markov models how do we learn the parameters and how do we use that for estimating the part of speech tags sequences for a given sentence hidden markov models for generative models ok and if you remember we discussed this distinction between generative model and discriminative model so we will see a discriminative model maximum entropy model for this sequence labeling task but before that let me start with some sort of motivation and why do we move from h m m s to maximum entropy model what are the limitations of h m m so then problem with the h m m s also this is known as the markov model tagging is handling unknown words suppose at run time you get some unknown word that was not there in the corpus because the prob the emission probabilities will not be known there is no where you can come to you can actually use the viterbi decoding so what do you do in that case so you might want to use some sort of morphological cues ok suppose the word we word is ending with e d you might think that over this might be a past tense of a word that we did not encounter in in the corpus or suppose you are finding capitalization in the middle of the set of the sentence you might think that this might be a proper name name but how do we actually include that in hidden markov models second so we use the first order markov assumption where each tag dependent on the previous only on the previous tag so this does not take care of certain certain situations for example here i have two sentences is clearly marked was such he clearly marked so what you seeing here ah marked in one case is a past participle in the first case second case it is a verb in past tense but just the previous context is not sufficient to tell this ok so so so because in h m m s we are only using this the previous context this might not be sufficient to handle this particular case so how will you ah take care of this issue so you might want to move to a higher order h m m say it second order ok so you can use a high order model but this is just one some some limitations suppose you want to use what whether this is the first word in the sentence ok or you ah you want to you want use any arbitrary arbitrary think like ah whether there is a verb ending with e d previous to this word you want to use this information there is no way you can use that in in the case of achievements so that's where we we move from achievements to my explanatory model that allow very very heterogeneous set of features so you can use a lot of different of your wishes from the data for your model so so what is the important distinction so now in the maximum entropy model so they are discriminative as compared to ah generative models which are as means you are allowed to identify very very heterogeneous set of features not only the tag given the previous tag or the word given the tag you can use any sort of features which might which you think might contribute to the choice of part of speech tags ok so so for example whether this is the first word of the article this is something that you can use as a feature in in maximum entropy model or whether the next word is t o you can use the next word as a feature whether one of the five previous five words is a preposition this is again something that you can use as a feature in maximum entropy model but here h m m cannot make each of this so what maximum model does it combines all this various heterogeneous features in probabilistic framework ok to be able to come up with the actual part of speech tag sequence for a given sentence so so this is the maximum entropy model so this is how you write the probability of so why is the class that is one of the the probability for any of the tags given x is my observation it can include anything in my in my context it can load load the current word previous word next word anything in the sentence ok you can so you can define your context while you are dealing with maximum entropy model you are saying that i can i can use all this information in my features given this i want to compute the probability of my class and here discriminative model so i can directly compute the probability of class given the observation ok so i can compete y given x and how do i i compute that it is simply x is a logging in model so summation over lambda i and f i x y now f i x y are my various features that i have identified over my observation x and my class y and lambda is are the weight is the weight given to the feature f i so each feature f i is given a weight lambda so i i i summed over all the possible lambda is fi's and taking the exponent and then i do some sort of normalization so that probability y given x adds up to one for all the possible classes y ok this is a normalization constant so this is the very very simple form of maximum entropy model so so let me tell again so z x condition on the parameters lambda is normalization constant so this is nothing but this exponent you sum over all the possible class x y ok for all the possible class x y you compute this and add this so once you do that you are you are ensuring that probability y given x will added to one for all possible values and lambdas are my features the weights given to my different features now x de denotes my observed data and y and y denotes my class now what is interesting is that what is the form of form that the features in maximum entry model can take as opposed to something like h m m what are the features so what is interesting here you are seeing f is is a function of x and y both in the observation in the class so i am defining my feature on on both of this and we are all binary binary features so let us let us see what are my features so in maximum entropy model my feature send code elements from the context x and the tag y now context x is now this is something that you you must be careful about context is not my current word w but is taken around the word w ok i can define my context it can be the sentence that contains w it can be it's previous three words next three words it it is something that can be defined on the on the current word ok and y is the tag that i have i am predicting now what are my features these are binary valued functions over x and y and this is like f x y is one if a certain conditions condition hold on x and y and zero otherwise and here is one example what can be some conditions like i am having the current word w if the word w is capitalized and my tag y is n n p then my feature f x y will take a value of one otherwise it will take a value of zero that is one sort of feature ok similarly you can define any arbitrary set of features that we have z so it can be f x y is equal to one if one of the previous five words is as a tag of preposition and the current tag is save up this is one sort of a zero otherwise so like that you can define an so you have a lot of freedom here in defining and choosing your features and once you have completed all these features the model learns the the weights for all these feature features the the weights lambdas and then this is very very simple function to compute the probability of ah plus given any observation ok so let's see some other examples for the features ok so this is again for any generic sequence labeling task so here it is for finding named entities so it can be here like location drug and person three different named entities and some examples are given like in arcadia arcadia is a location in quebec location taking zantac zantac is drug and saw sauce you so you see you as a person and now you are defining certain features so let us see certain features like the first feature is my tag is location so my class y is location previous word that is on x previous word is in and my current word is capitalized so what are all the cases we have this feature will be one it will be one here in arcadia previous what is a in and it is capitalized and the tag is location it will also be one for second case is capitalized previous word is in and the tag is location next feature so y is location and w has an extended leg in latin character so you can see that you are using a very very arbitrary set of features ok but possibly you have some some hypothesis that this features will be helpful for this task operating the predicting the tag for this word so this is this feature so for which of the case it will be one so it will be one for the second case it has an extended latin character ok the and the tag is location third feature y is a drug and the word ends with c and there will be one for taking zantac yes the word w ends to the c so you so you can see some some examples of what kind of features you can take but important thing is to know the intuition that you can choose any feature around the word w and that involves also the current class so how do we so now once i have all these features suppose the the learning has taken place i know all the parameters lambdas how do i go about tagging my sentence with that x so i have the words in my corpus w one to w a and i i want to find out the tags document tags so what i will do i want to find out tags sequence t one to t n so any sequence will have this condition probability right so n so t i conditioned on x i for i is equal to one to i now this is x i not w i x i is my context around the word w i and this is independent so i can use this model to find out what is the optimal tag sequence i can do it independently for each word and then i will find out tag sequence ok so so as we have said my context x i includes may also include some previously assigned tags for a fixed history and i will be using a beam search algorithm for finding the optimal or the most probable sequence this is something we will take up in the in the next ah lecture in detail that how do we actually come go about doing beam search in maximum entropy model ok so yeah just a brief idea here in beef search beam search what you do actually so you have ah positions in the sentence a starting from one to the end so at each position we will keep only the top k possible sequences and next time starting from this k ah k complete sequences you find out what are the next k based sequences ok so so let me just quickly say so i have from suppose my sentence as n words so what is the idea suppose my size beam size is k so at point one i will keep top k sequences so at point one i will have only one part of speech tags so that means i will keep top k part of speech tags at it's position one now when i go to position two now i will have a sequence now two might have some some other possibilities of part of speech tags so what i will do i will find out the probability of each of these k with all part of speech tags here and you store top k sequences again ok so tag one tag two some case top k sequences again at three i have some possibilities top k times these probabilities this possibilities again you store only top k out of this and you keep on doing that ah until you are here so here you will to choose the best one and that will give you the complete sequence so important here is that at each point you are is storing top k full sequences which starting from the initial point so here you will you will you will find some t one t two you will find some t one t two t three and so on top k at it's time so we will look at this in detail in the next lecture so for now let us look at the basics of maxent maximum entropy model so what is a maximum entropy model so maximum entropy model have this particular principle that is you are given the observation and you have certain hypothesis so ah you take what is given to you but do not make any additional ah assumptions so that is take the model that satisfies all the questions that you having but does not take any other ah assumptions ok so so what does that mean so you are given a collection of facts yes and you want to take a model that is satisfying all these facts or you are saying that is consistent with all these facts but otherwise it is not making any further assumption that is after being ah after satisfying all these constraints it is as uniform as possible or in other words for all the possible models that satisfy a given set of constraints choose the one that is the most uniform of all now what do i mean by this let's take a simple example so here i have an so this i have a simple example i have an english word in and i want to make some transition system that tries to mimic some so it is translating from english to french ok and i want i have some data already with me from some actual translator who who translated some sentences from english to french now so i want to model the this expert translators decision that how do i i translate this word in to any french word now how do i start so so suppose you have a vocabulary of all possible french phrases ok and you want to estimate probability p f so that is what is the probability that that french phrase will be a translation of the english word in so you want to compute that for all the possible french phrases they might be say millions of french phrases so so what you will do we will start by collecting a large sample of instances of expert decisions ok now from this facts you will try from this sample you will try to observe some ah some facts for example so so the goal would be extract some facts from the sample of instances and then use these facts inside your model ok so firstly you have to extract the facts so in our scenario this will be equivalent to finding out the features that is my facts and once i found these features or the facts i use them rightly in my model so let us see what are the different sort of facts i can find out for example this can be my first clue so i find that in all the samples i have seen the translator always translates in into one of these five phrases dans ah a au cours de pendant so they are five different french phrases where the translator is translating so now now this is now this is helpful to explain what is the my maximum entropy model now i want to find out the probability of the probability distribution over french phrases all so suppose i have ah say one million french phrases so f one to f ah million ok so i have one million french phrases i want to compute this probability distribution i want to find out the probability distribution given this constraint that is the translator always chooses among from one of these five phases so what is my constraint this would be so let me call these ah f one f two f three f four or say dans ah and also let me call these probability say f one plus probability f two plus probability f three plus probability f four plus probability f five is equal to so this is my constraint now so so now given this constraint i have to choose one model so suppose i have only this much information from the data now what is the model that i should choose among all the possibilities now firstly do you think there are infinite number of possibilities in which i can choose my model that satisfy this constraint so of course probability for f six to probability f million will be zero ok because if any if any of this is nonzero this constraint will not be satisfied yes so because all all the probabilities need to add up to one so that means all this has is this is anyways easy but what are all the models that will satisfy this constraint so you can see there are infinite solutions for this for this equation so you can choose p f one any number from zero to two one and you can always find some f two f three f four f five that will satisfy and same goes for f two and so on so there are infinite models that satisfy this constraint now what maximum entropy model does among all the models that satisfy this constraint it takes the one that is most uniform now among all these models what is the most uniform model so that is you are not making any dimensions given this constraint take the model that's most uniform so it is easy to see in this case that the most uniform model will be one that gives p f i is equal to one by five for i is equal to one to five ok for all these five phrases gives the probability one by five and everything else it gives a probability zero so this is the most uniform model given this constraint yes so allocate the total probability evenly among all these five phrases that is the most uniform model subject to our knowledge now is this the most uniform model overall suppose i didn't have this constraint what was the most uniform model as such the most uniform model will be that gives equal probability to each of the million french phrases so it's gives the probability of one by million to each of these that is a most uniform model so the model that i have obtained which is giving probability one by five to each of these five it not the most uniform model it is most uniform given this constraint ok given this constraint this is the most uniform model and this is the idea so so i will take some facts some observations given the observations i will find out what is the most uniform model let's take let's go further so this was easy in this case so where we had five phases only knows i got a second clue that the expert chose either dans or en thirty percent of the time ok so we were supposed f one and f two so now i have so this is my constraint one so now i have constraint two p f one plus p f two is equal to zero point three now given these two constraints what would be the most uniform model so if you just take a quick look you can see that the most uniform model will be p f one is equal to point one five p f two is equal to point one five and f three f of five f five which will get point seven divided by three and that will set both both the constraint this will be the most uniform model ok so this is not as uniform as this model so as you are getting more and more observations you are going far from the most uniform models but you are coming up with some other models that i as uniform as possible but satisfying these constraints now what happens if you get a third clue like in half of the case cases expert shows either dans or an so that is p f two plus p f three is equal to point five that is three you know immediately you will see that it becomes difficult to just look at this equation and try to solve for what is the distribution that is most uniform and that's why we we need to understand the the concept of maximum entropy so i want to find out the model that is most uniform doing the constraints that is equivalent to finding the model that is having the maximum entropy among model ok so now so yes how do we so as we get more and more ah clues it becomes difficult to see which is the most uniform model so for that you have to understand how do we measure the uniformity of a model and that's where we use the idea of entropy so some of you might have already come across this term of entropy so what is entropy entropy measures what is the ah uncertainty of dis dis distribution so so going to the basics so suppose i have an event x that occurs to the probability of p x ok so entropy measures what is the amount of surprise in this event so if it is very very surprising then ah the entropy will be high if it is like very obvious then entropy will be low so for an event when it is ah more surprising when the probability is low probability is high and the probability hi high it is like obvious ok so then the entropy will be low but when the probability when the so the interview will be low but when the probability is low so there is a lot of surprise in that even if that happens then the probability entropy is high so that's why entropy is ah log of one by probability ok so the probability is low entropy will be high so for a distribution how do we compute the entropy for a distribution so i just take the expected value of surprise that is summation over p x log one by p x ok so so i take the expected surprise over all the possible values of p so this is a very very a standard formula for entropy minus summation p x log p x now so here we are talking about distributions in the case of distributions when will be the entropy maximum so if you look at this formula so let's take one simple example of coin tossing ok so i am tossing a coin so it can be head or tail let us say that head has a probability of p and tail has a probability of one minus p ok so what is the entropy of this model it will be minus p log p minus one minus p log one minus p and if you actually so these are all based on too if you actually go on plotting this as a function of p from zero to one you will find that the entropy it starts increasing it becomes the highest at p is equal to half and it starts decreasing afterwards now can we intuitively understand this so what we are saying when both head and tail have equal probability then the entropy of them of my model is the highest what is entropy it measures how much is the surprise ok so let's take a point like this so at this point what will happen so head has a probability very low probability but tail as a very high probability so you more or less no ok it will probably tail when you toss a coin it is probable most probably be a tail so there is not ah so there is not much more surprises that are coming but when both head and tail have equal probability then you are very very confused you do not know whether the it will be head or it will be tail so the amount of so expected surprise that you will get is the maximum ok so this is the idea so this is a distribution over only ah this two variables p and ah this p n q are you can say this have p n one minus p but even if you take any any n number of variables the reservation will be having the maximum en entropy wave whenever it is the most uniform ok so this gives such the intuition that why we chose the most uniform model for getting the maximum entropy now coming back to a maximum entropy model so we want to among all the possible distributions we want to choose a model that maximizes by entropy ok now so i want to maximize entropy subject to certain constraints right so we will have certain constraints from the data and these constraints are nothing but my features so i will select some features and i will make sure that the the ambiguous probability from the data that i am finding for the features is similar to the probability that my model is giving that and subject to this constraint i will ah i will maximize my problem my entropy as you also seen as we keep on adding constraints one by one the entropy of my model decreases ok it comes close to my data initially might start with the most uniform model that is giving the equal probability to everything but as you keep on adding constraint one by one you come close you go far far away from the most uniform model so you decrease your entropy and you come closer to data so this is the idea so give us some constraints we want to come as close as possible to the data but otherwise we want to obtain the most ah most uniform model in terms of maximum entropy so coming close the data in this case means finding the ah empirical probabilities of these features and ah i am trying to make it trying the model probabilities to be close to this so this is my maximum entropy principle i am given some n different feature functions f one to f n i want my probably distribution p to lie in the subset in the subset of all the possible distributions that satisfy this constraint ok so all the all the ah probability distributions that satisfy this constraint among those i want to choose the one that is having the maximum entropy ok so what do i mean by satisfying this constraint so what is the empirical count of or expectation of a feature so empirical expectation means how many times this features actually ah this x y actually occurred and f i x y was one in my data ok so p hat is my ah empirical probability now what will be my model probability so it will be so model probability p f i it will be summation x y p x y f x y now this p x y because we are computing finally p y given x i can write as summation x y p x p y given x f x y ok and because we are not computing p s n l model i can probably approximate it with the ah empirical count of x so this gives me the model probability of the feature so i want to make it closer to the p tilde f i that is the empirical probability of the feature ok so i have the empirical count model count and i want to select the distribution that is most uniform subject to this constraint ok so now so i am computing this probability distribution so what will be the maximum entropy model that is the one that gives me the high entropy h y given x and that if you do some quickness that will be minus sigma x y p x p y given x log p y given x ok and again we will approximate this by p tilde ok so this is my maximum entropy model and i want to maximize it subject to constraint that this and these are equal so this gives me the model maximize this h subject to these constraints ok so for that we use the trick of being the lagrangian ok so so i want to now a k lagrangian p or lambda that is my h p plus summation i lambda i p f i minus p tilde f i so that is i want to maximize this sorry so i want to maximize this yes subject to this constraint ok so now if i put these values inside this so i will get plus sigma lambda i summation x y p x y stepwise p tilde x p y given x f x y minus summation x y p tilde x y f x y ok this is my ah lagrangian and i want to maximize that so i will i want to find out this distribution that maximizes this so let me just substitute this fifty everywhere ( Refer Time : 36:00 ok so in term what you are doing you are maximizing it with respect to this d and that you can do by differentiating it with respect to d ok and then putting it to zero and that will actually give you the the equation that we have seen that is p lambda y given x that is our t in this case it will come out to be exponential over summation lambda f i x y divided by the normalization constant ok this is simple exercise that you can do you can just differentiate it with respect to this parameter t and you will actually come up with something close to what we have seen in the formulation of maximum entropy model and this normalization constant is nothing but something that that makes this probability p lambda y given x summation over y to one and that gives me the normalization constant um ok so this is my maximum entropy model so you are you have to the the important part here is to find out the set of features once you have a set of features you can use this particular equation to compute the probability of a class y given the observation x so next in the next lecture i will try to take up ah is a problem that will make the idea of maximum entropy model for that clear then we will see how given a new sentence we use this maximum entropy model to come up with the tag sequence and there we will study this beam algorithm beam search algorithm ok so see you then