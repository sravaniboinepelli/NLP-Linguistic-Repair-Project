so welcome back for the fourth model of this week so in the last model we had started talking about the maximum entropy classifiers what is the basic principles for using this maximum entropy classifiers ok and we saw that we can use any hidden new set of features that were not allowed in the earlier model like h m m so today i will start by taking a simple example that will give you some idea on what how maximum entropy classifier actually works for a simple classification problem and then we will see how it can be used for a sequence tagging problem that is i am given a sentence so this is by problem for part of speech tagging and given the sentence for each of the words i have to find out what is the actual part of speech tag for that so right now what we had seen in maximum entropy classifier it gives me a probability on the class y given a context ok so for each individual word for the context around that word i can predict what is the appropriate part of speech category but once i am given the sequence how how this model will be actually utilized so there it has a special name called ma m e m m maximum entropy markov model and we will see what is the algorithm for ah using the classifier that we have discussed for a sequence tagging problem so now i starting with a simple problem on how to use maximum entropy classifier so the practice problems so you can try it on your own but i will try to give you certain hints so problem says so ah you want to use maximum entropy model for part of speech tagging and you are trying to estimate probability tag given word and in this hypothetical setting you can assume that there are only three possible tags determiner noun and verb and the word can belong to any member of a set v that is your vocabulary and vocabulary contains the words a man sleeps and some additional words so it in this problem it does not matter how many words are there in new vocabulary now the constraint given is as follows so the distribution that you will find probability tag on word should give the following probabilities probability of determiner given the word a is point nine probability of noun given the word man is point nine tag v given sleeps is point nine d given any word other than a man or sleeps in the vocabulary is point six and same for that to to constraints that are given here so remember that's what we did in the case of maximum entropy classifier you are given the data and from the empirical distribution you expert certain facts and now you want to find out a distribution that resembles that empirical distribution ok so now i want to find out probability i given word such that these constraints are followed so now how do i go about building mu maximum entropy classifier this is the problem so now it is said that all other probability that are not given suppose that take some values such that probability i given word is one for all possible tags this is the normalization constraint the probability for all the possible tags for a given words should added two one ok now there are certain questions here so the first question says is that so you are given this problem settings now try to define what will be the features option maximum entropy model given the constraints ok and the problem says that markov features as some f one and f two and each feature should have the same format as we explained in the last class ok so that is each feature your function of x and y x is the context around the word and why is the particular class or tag then the next questions is for each feature f i assume a weight of lambda i now write down the expression of the following probabilities so what is the probability of d given get cat in terms of your model parameters so the parameters are mainly lambda one to lambda six similarly find out probability of n given laughs probability of d given man finally the problems is what should be the values of the parameters in your model lambda one to lambda six such that you have to the distribution as shown in the question ok so this is by problem so let me try to solve it ah in any steps so this is the first problem what should be the features of five model such that they resemble this distribution so ok so these are the actually certain facts from the data so let me write down the first fact probability d given a is point nine ok so now i want to find out some features f x y such that it can try to match this particular constraint so x is on my data so this was my on by word and the context and y is the tag ok so now what am i observing in this in this particular example i am observing that when the word is a then the tag that i i i give it d with the probability of point nine so what kind of feature will will try to model this so my feature should be effects y so i can use a feature word that is my x is equal to a and tag that's my y is equal to d ok that encodes both a and d that are that i am observing in this empirical data ok in this we can call as your f one what is a and tag is d so this is a binary feature so whenever you will see word as a and tagged d you will put it as one so to express it you can say f one is equal to one if this happens is equal to zero if this does not happen ok i am not writing it in this in this paper so now i have to define some other features right and the hint is that six feature will make my task easy so that is like sufficient hint you have six possible constraints and you have to define six feature one for each for each constraint so lets take the second constraint second constraint is probability and given man is point nine ok so for this constraint what will my feature again so f two would be word is equal to man and tag is equal to n noun same way you can do for you third third case so f three will come out to be word is equal to v and sign word is equal to sleeps and tag is equal to v now what would you do for your fourth observation what is that probability d given word is equal to zero point six for word in vocabulary minus a man and sleeps ok for any word other than a minus sleeps so what would your feature again you will have a similar feature f four word is in so let me call this as v prime v minus a man sleeps let me call it as v prime ok this is my v prime so what is in v prime and tag is equal to d ok similarly my f five will become word in v prime and tag is n f six word in v prime and tag is v and these become my six features that will try to model the distribution now then so this is how you will select the features given certain facts about the data so so see here i am not doing anything from matching this these numbers right now so that i will try to do using my lambdas that the feature weights set that i have to choose so the next part of the question says is that now give weights so suppose this is lambda one it has a weight of lambda one lambda two lambda three and so on so each feature f i has a weight of lambda i now try to find out some probabilities in terms of the model parameters so let me take out the first question itself that is probability of d given cat i want to write that in terms of model parameters so what will be this probability d given cat so now in terms of my maximum entropy model how do i write this par [ par / particular ] particular probability it should be e to the power sigma lambda i f i divide by a normalization constant and z should be such that this will add up to one for all the three tags so probably d given cat cat plus probability and given cat plus probability we given cat should be one so let me take this particular y ok let me focus only on this and i will talk about z later so now so this is sigma lambda i f i x y x here is cat and y is d so now f x y so each of the six features are binary features so they will take either one or zero and lambda one once are just given to me and then then um the values are not given so i can just take lambda one lambda two and so on so now what will be this value ok so let me take the first feature so first feature is here word is a and tag is d in this case my word is cat ok so clearly this feature is not one so for f one x five it will be zero so lambda one times zero plus lambda two times what is my second feature second feature see is [ sage / say ] says that word is man and tag is n again this is zero f three word is sleeps tag is v again zero f four so my feature f four is word is in v prime is this word in v prime cat is not either a man and sleep so it is in v prime and tag is d so this is actually one so word is in v prime and tag is d so lambda both four times one is i will not write f four f four is one in this case plus lambda five now you will see in feature five and feature six the tags are different they are n n v so they will again be zero plus lambda six zero this comes out to be lambda four ok so now what is my probability it will be e to the power lambda four divided by z now how do you find out z now to find out z you have to actually compute the other two probabilities also so you have to compute probability n given cat probability v given cat ok and then you can make this plus this plus this is equal to one let me quickly do that so what will happen for when you compute probability and given cat the first three features again use a word a man on sleeps that is not here from fourth features they are using the words in v prime so the word cat and n v prime so the features can be one from f four f five f six but now i have to see the tag for f four the tag was d for f five the tag was n and for f six the tag was v ok this was n and this was v so for n given cat f five will be one and v given get f six will be one so can i write probability and given cat this will be e to the power lambda five given z and this will be e to the power lambda six divided by z ok now you can compute what is p d given cat because this c will add up to one so put in the constraint e to the power lambda four plus e to the power lambda five plus e to the power lambda six divided by z is equal to one so that gives you z is equal to e to the power lambda four plus e to the power lambda five plus e to the power lambda six so this we can write as e to the power lambda four divided by lambda four plus e to the lambda five plus e to the power lambda six that is your probability of d given cat in terms of your model parameters now in a very similar manner i will suggest that you try out the other two cases what is the probability of n given laughs and what is the probability of d given laugh so again we have not use of the of the probabilities point nine and etcetera that were given in the in the in the constraint of the of discussion so that i have to use in the last point what values with the parameters in your model take to give the distribution as described above so that is what values a should takes us that probability d given a is point nine so ok need to you may leave your final answer in terms of equations so let me try to take only the first try to satisfy only the first constraint what should be the constraint on my lambda such that the first constraint of probability d given h point nine satisfied ok and the idea is very very easy you will suggest night like we found out probability d given cat we will now try to find out probability d given a ok and that will be e to the power lambda one yes because first feature is one for this case divided by z ok now what is z so for finding out z i have to find out probability v given a n probability n given a now brought this probability v given a ok so again i will write one divide by z into something e to the power lambda one times f one and so on now the word a is given only in the first feature yes first feature says the word is a and tag is d so the word is a is given it is not given in feature two feature three now what about the other features f four f five f six they all talk about words in v prime whether word is not there so the any of the feature cannot be one four for this case this case so all of them will be zero so it will be simply it small zero ok all the features are zero same happens with n given a ok so they are one by z one by z and this tells me that probability d given a should be e to the power lambda one divided by e to the power lambda one plus two and there should be point nine to satisfy the constraint and that you give me the value of lambda one similarly you will do for all the six cases and write down the questions ok so hope that gives you a good idea on how do we actually start building a maximum entropy classifier in the data and how do we compute the probabilities now i will i will take a very specific case so that is by the paper written in nineteen ninety six by ratnaparakhi on using maximum entropy model for part of speech tagging so what i will show is that what kind of features they use for part of speech tagging ok so now this model is there now what is important to understand is that given a problem what sort of features i should use to get a good performance ok model will work well if few features the features that you have chosen you have chosen make sense for this particular problem so for part of speech tagging problem what are the kind of features that he had used now given a new problem again you have to go back and find out what should be the interesting features you know now the format of features how you should define your features but what are important features will always depend on the particular problem that you are solving so as we had said so in in the maximum entropy model you choose a context around the world from where you can choose your features so in this case what is the context they have chosen they have chosen the current word the next two words the previous to words and the tags given to the previous to words ok and that is interesting how do you use this kind of features in the time in the time at the time of decoding when given a sequence you are trying to find out the tags because you do not know what is the best tag for the previous word ok so we will look at this ah this problem when we talk about the such algorithm the beam such algorithm that how do we make use of previous tag and previous previous tag there are to be assigned that are not yet assigned fully ok but here use always features in in all this ah context for defining his features now what are the form of features so yes just to give you another example features will depend on the history or the context and the current tag so it can it is one if the current word has a suffix of i n g and the tag is v b g that is one kind of feature the suffix of the current word and the tag so what are the other other features ok so now so what is one interesting thing here we divided it features into three parts some features are only for those words that are not rare that a very very common second sort of features are those words that are rare why do why does it define separate features for the words that are rare because these words may not be seen again data but i might see some other real words so can i use some some of the properties of these words instead of using the word directly ok so further words that are not rare they occur many times uses a word directly w i is equal to x and its current tag this is the form of feature but the if the word is rare he does not use the word directly he says x is a prefix of w i length of x is less than equal to four so he uses all the prefixes of the of the word is starting from one two three and four same for suffix now how will that be helpful using the suffix so for example i have e d but overall the word is rare so can i use the fact that the word and sanity ok that's why we are using the suffix here similarly for prefix ok i can have something like i n for making the up opposites for an adjectives and so on ok so this can be captured by using this kind of features whether the word contains a number or in uppercase character or in hyphen ok this for rare words now he just some other features for all the words so what are the features the previous tag is x and the current tag is t this is very very generic feature right the previous two tags are x and y current tag is t previous word is x and current tag is t previous to previous word is x current tag is t then similarly for the next for the next next words ok so now let us see what will be the form these features will take so this is the generic form of the features now let us take a simple context and see what are the forms these features are going to take so this is one context ok i have a sentence the stories about well heeled communities and developers and the tags are d t n n all the the part of speech tags are given to me so this is for my label data now what are the values these features will take so firstly i will differentiate between the rare words and not so rare words ok so what can be the rare words here for example the word well heeled here is a rare word ok and other words like stories committees developers are not rare so for the words that are not rare what feature am i using the current word and the tag so my feature could be the word is a stories and tag is n m s so for the word well heeled i will use the suffix and prefix so w is a prefix of the word and tag is j j w is a prefix and tag is j j similarly e d is a suffix and tag is j j so ok and then there will be some features for all the words like for about and well heeled what would be the feature the previous tag is i a i n and the current tag is j j the previous two tags are nns and i n and the current is j j the previous words or the word stories and well heeled about and well heeled so on so you can use all these features so here are all these features right so this is prefix and suffix for the real word well heeled and there are other things like w i contains hyphen that is in the case of well heeled and tag is j j ok so now thats how you will write down all your features from your data and you will learn your parameters lambdas and all and at run time you will ah use again these features in the parameter to finding the probability of the tags ok so hope this gives you some idea and how can you choose your features for a given task now let me go to the search algorithm for for this maximum model so so this we had discussed earlier also but we did not discuss in full details some given a sentence w one two w n these are the words and i want to find out the probability of tag sequence t i t one to t n ok and we said that we can write it in this form multiplication of probability t i given x i x i is the context for each individual word now the the problem here is so and you can use some tag dictionary that says for each individual word what are the possible tags similar to what we did in the case of viterbi decoding for h m m now what is one particular problem in this case so as such it looks as if you can do it independently for each each word right for the first word i can find out what is the best tag multiplied with best tag for the second word and so on now what is one problem with this approach let me just give you one simple example ok so this is a hypothetical example let us say that we have word one word two word three ok and word two can take two tags i n and j j and word three can take lets to keep it simple that it can take it take like v b z and maybe something else like v b g ok maybe they may not be possible but just a hypothetical case now suppose by using this algorithm or doing it independently we found out that for w two i n give some probability and j j gives some probability now in w three what might what could happen some of the features might depend on the previous assigned tag so how v b z you might want to use what is the tag assigned in the previous word ok so that means choosing the probabilities the best probability at this point may not be ah may not be the best so choosing the tag at this point may not be the best because at at the next step you are using the tags and the probabilities can change this can happen that i n in giving the better probability at this point but when combined with v b z j j gets a better probability this can happen ok that means i can find out the probabilities but i have to use the probabilities and the tags in the next step also ok so this is not fully independent i cannot just choose i n and forget about it i have to remember what is the probability next time i will choose i n v b g j j v b z and try to compute the probability for v b z multiplied by the previous probabilities and that's and how we do that I will show you in the beam search algorithm so what is the bean search algorithm so so your test sentences containing words like w one two w n ok and later say that for the i th word as i denotes the z highest probability tag ok this is top tag sequence up to the word w i now how does the algorithm work so you have the words w one to w n forward w one you have s one one s one two up to s one n top n highest probability tags ok so will come to the probabilities and this is easy again you will use the features here which feature is one which feature is zero then you will do the normalization you will compute all these probabilities for the each of these tags now the the important thing is how do you go to w two ok so one thing to to understand is that you can probably not keep all the possible tag sequences because assume that each word can take on an average k tags so if you have n words the tag would be of the order of n to the power sorry k to the power n yes k to the power so so this will be exponential in the length of the sentence so you cannot keep all the tag sequence so you might have to make the choice of the best sequence is available at this point that's what we do in bean search so at each point i will select what is the top set up capital n sequences at this point and how do we how do we select that so that is the next part of the algorithm so initialize i is equal to two ok so now you have to second word generate tags for w i given s i minus one j as previous tag context and append each tag to s i minus j to make a new sequence and then continue for all the tags now what does that me [ me / mean ] initialize i is equal to two that is your second word now generic tags for w i s given s i minus j as the previous tag context what is i minus j i minus one is s one n j so let me take this as the previous context and suppose this has again some tags like ah i will just write some tags t one t two t three so s one one and t one becomes the first sequence s one one and t two becomes the second sequence and s one one t three becomes the third sequence similarly s one two t one becomes forth sequence and so on ok so is so in this case there will be three times n sequences so this you can take any general case this may not be one tag this will be a sequence up to this point so we all the first sequence at this point now for each of the sequence you know what are the previous tags that have been assigned yes so i know what are tags assigned at this point so i can use all the features so if a feature says t i is equal to something and t i minus one is equal to something else so i can check if t i matches this and the previous tag matches this then only it will do one other wise zero because i'm choosing the context i can compute always feature i remembered it features i can compute the probability for all the sequences and now what i will do i will select at this point what are the s i one to s i n whatever top end sequences at this point ok and again i will go to the third word again its tags combine with all the sequences and choose top end from here so what will happen u [ u / use ] use your space will not do exponentially you will always have have top end sequence is at any given point so finally when you go to the the final word you will get the top end probabilities or all the probabilities and you will select the best sequence and now because the sixth is sequence contains the tag from by starting you will find out the part of speech tag just from the choosing the best sequence at the last word and that is your algorithm so find n highest probability sequences by above loop set this accordingly and repeat if i less than equal to n and return to highest probability sequences x n one for the last word and this is your maximum entropy this we markov model ok instead of maximum entropy model is generate classifier that can do for each word individually but when you are trying to applied for a sequence labeling problem for a sequel you use this maximum entropy markov model ok so this was maximum entropy markov model for you now in the in the next lecture i will take one simple example of this search and i will quickly cover the the model of conditional in the fields that is one one of the state of the arts in sequence labeling task and what we will sure there is some problem with the maximum trophy model that condition of fields try to handle ok so thank you i will see you in the next class