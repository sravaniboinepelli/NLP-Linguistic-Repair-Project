welcome back for the second lecture of this week so we have started talking about entity linking and we we talked about two different approach so one approach where we use the keyphraseness in commonness to find out what are the appropriate mentions and how do you link to them their ah corresponding reference in the ah in the knowledge base and we were considering wikipedia as our knowledge base and we found out one particular problem with using simple keyphraseness and commonness so so what is the problem so so in in commonness what we were doing we were always taking the page that is having the highest ah commonness so what will happen suppose i have word like tree and the commonness and commonness to the sense tree is ninety two point eight two percent but the other concepts because they occur rarely commonness is like two point nine four percent two point five seven percent and so on so what you are seeing wherever the word tree occurs you will by default assign it to the first sense of tree and you will not look at the context at all so in this case so you have it article about depth first search and you have a sentence where the word tree occurs and because of using commonness you will always assign it to the sense tree but the correct sense says here is tree data structure but he it has a very small commonness so only two point five seven percent so what you need to do for ah assigning it to the correct ah data structure correct sense there is a tree data structure for that you need to you you should be able to use the context here that is what are the referent words that i am seeing in the context now question is how best we can use the context now i do not want to use some random words in the context ok i want to use the words in the context that are actually assign that have actual correspondence to a wikipedia page so that i can find out something about the wikipedia page how common this wikipedia page is to one of it's reference now there we we fall into the same problem that how do i use a wikipedia reference to any of it's page when the disambiguation is not yet happened so i am only at the ta at the stage of ah disambiguation so how do i use the actual entities entity page and for that if a nice tree can be used and idea is ok so with this word there are many other words that are that are coming in this article or is is if this piece of text and some of these will be appropriate mentions and they will into one or many ah wikipedia pages now some of out of these at least some will be there that have a unique disambiguation page in wikipedia so there is a unique page in wikipedia where they link to and there i do not need to do any disambiguation so why don't i use only those pages which have a ah unique place in wikipedia to find out what should be a good of a wikipedia page for this entry tree so that's what is done so so this is the hypothesis that if you have a sufficiently long text you can find out terms that do not require disambiguation at all there will be some terms that have only one mention or one referent in wikipedia now use this unable un unambiguous link in the document that context to disambiguate the ambiguous ones so what is the idea here so you are given this article and you want to find out what is the appropriate ah sense for this wordage so in a sense i mean what is the appropriate link in wikipedia is it tree tree graph theory data structures set theory etcetera so what i will do i will see what are the other mentions in this page so what are the other things you have seen algorithm tree structure graph backtracking uninformed search tree backtracks lifo stack and so on now among these there will be some that have only one wikipedia page as the referent so these are ah shown as box search so algorithm tree structure uninformed search and lifo stack have only one wikipedia page so i will take these unambiguous links and try to find out how close these foldings are to my all of these possible senses ok so how close are these folding to this possible sense and the one sense that is having the the is the closest to these four will be called by sense there should be the link to will to which i will link my corresponding ah mention so how do we compute the relatedness score and this can be very simple so so you you can initially start by ripping representing each candidate sense and context term by a single wikipedia article ok so for example what is happening now so a word like tree and tree corresponds to many different senses and call them your article article one article two article three article four ok and if you remember your word sense disambiguation it's like constructing various ah sense backs there are four different senses they are like science science x now you are having a context back kind of context track where you are saying ok in this context of tree i am finding four different words word one word two word three word four now what is the property of each words they link to one page in wikipedia ok so like a w one a w two a w three a w four so now these are wikipedia articles and they are also wikipedia articles now now the problem is select the sense article that is the that is most in common with all of the context articles so that is among the four articles which is most common with all these four articles ok and you will see what is the art max that is having the more similarity with these four articles and this can be captured in many different ways so we will talk about one particular method so and one go to the method is you just take the wikipedia link based method that is two pages are similar if they are having many incoming and outgoing lon links common ok so what is the idea so how do you find out how similar a one is to a w one so i will say ok i have two articles a one a w one and wikipedia i find out what are the incoming links to this article and what are the outgoing links from here same i will do for this article and now once i have found this out i can see ok what are the common links so how many articles in wikipedia article w a prime are linking to both of these similarly what are the articles to which both of these link to and these are very good measures for finding out how similar they are you see we can always do it by seeing how similar they are by measuring their text similarity how much text similarities is there you can capture cosine similarity and what or something else but this is a nice link based measure that says ok how many pages linked to both of these so what are the common in links and how many pages they both linked to that is a common out links and this is again a nice measure in that it says ok this article refers to both of these that means they they need to have something common similar they both mentioned the same article again that means they they need to have something in common and you will find out how many what fraction of incoming links are common what fraction of for being links are common and that you will take it as a measure for computing how similar these two articles are and this is also called by relatedness ok so we talked about keyphraseness commonness and this is relatedness and then you can find out the relatedness of a candidate size sense by taking a weighted average of it's relatedness with all of the context articles so that is to find out the relatedness of this sense a one you will say ok it's relatedness with a w one will a w two a w three a w four ok by using this measure then you take a average or a weighted average compute a weighted average and this will be ok what is the relatedness of this sat sense a one a two a three a four whichever as the highest you take that ok like if you see the previous slide so here you were capturing showing relatedness of various senses and this tree data structure had the highest relatedness sixty three point two six percent that uses the average of relatedness with all these the four different context articles now so there is one term here we are taking a weighted average now what should this weight depend on why should i weight one of this higher than the other ones so again if you think about it it can it can depend on which context term is more important than other another ok so what we have done we have taken the context we have found out to all the words that are that are mentions and from there whichever are so these were ah unambiguous whichever were an unambiguous we are taking them as my context to find out the relatedness but some of these might be more important for this topic or the document than others so so can i give a weight depending on how important they are to the topic now the question again comes how do i know which one are more important to the topic or the theme of this document and then you see you can again use the idea of relatedness ok that means one among these four context art ah senses articles the one that is having the highest relatedness with the others which more which more appropriate to the theme of the theme of the document yes because there is a theme of the document and the words that are appropriate should also be connected to each other that means a word that is having a high relatedness with other words is it should be given a high values ok and that is a nice method to also give a weight here weight to different of these relatedness that is how related these this context article is to the other context articles so in general there are so what are the things that are used to give a weight to the context term so one is called link probability so what is link probability ah so so again in your context you are finding say four or five articles where the link is unambiguous there is only one link now you can use the link probability itself that is among the four which one is like a keyword that is it always links to something some words may not link may not alwa always link some words always link so the words it always link should be given a high weightage because i know there this is a more a specific term if a word is somewhat sometimes links sometimes not linked it may not be a very important keyword so that can be one measure what is the link probability probability that it will be given a link in general wikipedia that is same as my key phraseness measure and certain thing we have already discussed that is relatedness so find out how closely it relates to the central document by computing it's average relatedness to all other context terms ok so i take for each word what is the link probability or keyphraseness and relatedness now i have two different majors so how do i take these together to compute the rela the weight of this word you can simply ah take an average to provide the weight for each context ok so is that clear now so you have some words in the context they are ambiguous sorry unambiguous for each word for each context word you find out the relatedness of this ah mention in your mention sense one of the sense taken weighted average and this weight depends on what is the link probability and what is the relatedness with all the context terms and and by doing doing this method you can find out ah relatedness of each of the four senses now now the so so we have discussed ok we can take some mentions by some method and then once with the mentions we can also give a link by ah finding out which of the candidates are similar in in from with the context ah mentions but here there is an interesting question that by using all this when when we are doing all this approach can i go back and also improve my mention detection part so how i was detecting the mentions so i was gathering all the n grams in the document and retaining only those whose probability exceeds a very low threshold so that is i start with a text document there i take various n grams it can be ok i i take certain pattern they are noun groups or something like there is some patterns ah n grams i can say one two three whatever and i take some n grams ok and then see what is the keyphraseness of each of these and whichever as ah if this is above a threshold when i take it as a mention and then i then it's a mention and then i go to my link disambiguation part but see what are you seeing that when i am finding out the appropriate entities as mentions i am not using the context at all i am just seeing is it a good word for ah is it a good key phrase or not does it have a good keyphraseness or not overall so independent of a context is it a good mention or not so question is can i also use the context to find out what are good mention and what are not so good mentions and that's what we will see so it is the best method so all the remaining phrases are disambiguated using the approach mentioned earlier yes so we have whatever ah mentions we have found or whatever phrases we have found we do disambiguation using an approach that we have so now by doing this approach you get to find a lot of different things like what are the links what are the appropriate ah mentions here what are the wikipedia pages they link to ok so you are getting some new wikipedia pages also so now can you use this additional information to find out are they good ah candidates for mention at all so so that is can you use that to find out which concepts should be linked so here is one example ok so you are having a ah a wikipedia page ah at so it's like a news article so democrats deal is clinton setback and you are having lot of ah so ah various sentences are here now what is your approach in your approach you you take a various mentions like hilary clinton occurs at various locations and try to find out what is the entity in wikipedia it will link to similarly here barack obama nomination vote michigan all these are link to their various wikipedia entities so you get all this by your ah link disambiguation phrase now my question is can i use that together to find out what are good mentions also for my text and for that we have to convert that to some sort of a learning problem learning problem where i run my algorithm on a on a data and see what are the good ma ga what are the mentions i am detecting what are the links i am connecting to now once i have all this information someone gives me gold standard that what are the good links here what are not the book links using that can i learn what are my what are the good candidates to be mentioned what are the good what are not so good candidates to be mention so like coming back to my example so i start with the text data and i find out ok there are some mentions they have driver key phrase above a threshold ok now i also link them to their wikipedia articles some of these might be linked to the same article ok so that i do for this whole document now suppose someone tells me that actually this is a good mention this is a good mention this is a good mention but this is not so good mention ok this is good mention this is not so good mention so once i have all this information can i develop a machine learning method to detect ok given an article given in mention and it's approved wikipedia page all the context is it a good ah mention at all so given a phrase with all these attributes is it a good mention for this document or not so now so you can say that once you given me the text all the all the steps that i have taken are deterministic so i can apply ah keyphraseness i can find out the mentions i can link them to their wikipedia pages so all this i can easily do but how would i get these gold standards that doesn't inappropriate ah mention this is not inappropriate mention and this is one of the bat bottlenecks so how do i get this ah actual links and not so not good links and good mentions and not so good mentions for that so now what is interesting idea here can you use wikipedia again so can use wikipedia again so how would you use wikipedia for this so if you think think a bit so how you can use wikipedia and that's actually very very easy so you take wikipedia and take some wikipedia articles say a one a two a three a four a five so on now each of the article now forget the hyperlink structure here so take it as a plain text and feed it to your algorithm so algorithm takes a one as input plain text and runs this ok so your algorithm will run this it will tell tell you what are the mentions what do they link to and so on now because a one is already wikified so it's already in wikipedia you know what mentions are good what mentions are not good so from there you can automatically construct your gold standard ok and once you have the gold standard you can apply a machine learning method to say ok which given a feature around this fridge is it a good mention in this context or not and that will solve your problem so this is like you are learning to ah link using wikipedia so using the wikipedia data and ah so very nicely you are taking it as a training data and also constructing your gold standard from this without having some manual efforts of labeling because otherwise you will see this labeling will take a huge amount of time and this will help you do that automatically so so what will i do so now once i have taken the wikipedia as in as input i know ok whichever phrases gave me a wikipedia article that was actually there in the original article they are possible examples and whatever was not there becomes a negative example and so yeah so you got the possible negative examples and you feed it to your classifier and then you use various features around these various mentions and articles to detect whether it's a good mention or not so you use various features like like the places where they are mentioned to inform the classifier ab about which topic should and should not be linked so now so now what are what can be these possible features that you can use from a given n gram phrase so so let's see some features some of some of these features are what you have already seen and some other features can depend on how do people actually write ah a wikipedia article so what are the good features so one feature that we can use is link probability ok so so that is for a given mention what is the link probability now if it is occurring at multiple places ok like hilary clinton clinton so they are occurring at different different ah variations so what are they li their link probabilities at each of these places take either the average or the maximum of this link probability and that can be one feature ok so this you are doing jointly so hilary clinton and clinton together should they will linked or not and here you are trying to use ok what is a link probability it at different places taking average or also taking maximum both can be a features then you can use the relatedness so how related these ah phrases are to the central theme of the document so again you will find out what is the relatedness of these mentions with different ah unambiguous links in the entire article so this can be another feature if they are very highly related then only you will take them as your mentions if they are not related to the entire theme of the document that means they are not probably not good ah candidates for mentions then you can also use the disambiguation confidence that is when you are trying to do a disambiguation over this ah mention how confident your classifiers if your classify is not very confident that means you do not have sufficient context in those document and it may not be a good mention at all so this confidence can also be one of the features then you can use the generality that is when you are trying to link some ah phrases in in your text what is the idea you do not want to link something that is very very generic that all people already know about so you want to link the phrases that are very specific so how can you know about the jan is ah how generic or specific a particular phrases further you can use the the the category to your wikipedia and there you can see at what what depth in the tree this particular ah mention comes in so if it comes at a very ah it's very top label itself that means it is a ah it is a very generic term but this coming very low in the tree that means say a specific term so specifi specific terms might be given a high preference so this can also be ah like your feature what is the depth in the wikipedia hierarchy tree and then you can also see how the documents are written that is where all this entities mentioned so for example if it is a good entity it will be mentioned in the introduction similarly it will be mentioned in the con conclusion of section of the of article so if it is mentioned in the initial full lines or the last few lines it might be important so you can simply measure the offset ah from the beginning end and the end then you can also see the spread that is what is the distance between then you shall mention in the last mention so that is how far does the response across the document if the spread is high that means ok it might be a good mention if the spread is low that means very to only cover very small topic of this document this has been used so this can again be a feature for this task and you can think of many of other features combine these features in your classifier and then you are learning whether given this phrase with all these features is it a good ah ah candidate for mention or not and this is like you are learning you are learning to link using the wikipedia structure so as such you you take many different methods but this is the basic conceptual idea about entity linking that how do you detect mentions different methods how once you detect mentions how do you link them to their ah appropriate entries in the wiki in the wikipedia or any other database and can you use this task to also improve your mentions ok and you can take it in different different applications take different databases and you can try out various variations for this task so this so that's where we finish our discussions on entity linking so in the next lecture onwards we will start talking about information extraction that is from a from a document where there is a lot of unstructured data text data how can you identify various entities and the relations between them thank you